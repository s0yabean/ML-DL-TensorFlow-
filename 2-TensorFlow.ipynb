{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Data\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_digit(index):\n",
    "    label = y_train[index].argmax(axis=0)\n",
    "    image = X_train[index].reshape([28,28])\n",
    "    plt.title('Digit : {}'.format(label))\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2db2ac5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_digit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# One Hot Encoding\n",
    "a = [0,1,2,1]\n",
    "num_labels = len(np.unique(a))\n",
    "b = np.eye(num_labels)[a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Decoding\n",
    "a = tf.constant([[0,0,0,0,0,1,0,0,0,0]])\n",
    "b = tf.argmax(a,axis=1)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 454s 3us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sudip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: `toimage` is deprecated!\n",
      "`toimage` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use Pillow's ``Image.fromarray`` directly instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2dbae5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.python.keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import toimage\n",
    "for i in range(0, 9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(toimage(X_train[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import keras\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX1: ML using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate fictitious data\n",
    "N=100\n",
    "X_train = np.linspace(-10,10,N)\n",
    "c1 = np.random.normal(loc=-0.5, scale=0.2, size=N)\n",
    "c2 = np.random.normal(loc=1.0, scale=0.2, size=N)\n",
    "y_train = c1 * X_train + c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Variable\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "W = tf.Variable([0.1],tf.float32)\n",
    "b = tf.Variable([0.1],tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model\n",
    "yhat = tf.multiply(W,X) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 3: Loss Function\n",
    "loss = tf.reduce_mean(tf.square(yhat - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Optimizer\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training data\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 5: Training Loop\n",
    "for i in range(1000):\n",
    "    sess.run(train, {X:X_train, y:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [-0.53416079], b=[ 1.04728782]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluation\n",
    "W = sess.run(W)\n",
    "b = sess.run(b)\n",
    "\n",
    "print('W = {}, b={}'.format(W,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2cd47a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_train,y_train,'o')\n",
    "plt.plot(X_train,W*X_train+b,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2: Linear Regression with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate fictitious data\n",
    "N=100\n",
    "X_train = np.linspace(-10,10,N)\n",
    "c1 = np.random.normal(loc=2, scale=0.2, size=N)\n",
    "c2 = np.random.normal(loc=1.0, scale=0.2, size=N)\n",
    "c3 = np.random.normal(loc=-1.0, scale=0.2, size=N)\n",
    "y_train = c1*X_train*X_train + c2*X_train + c3\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "W1 = tf.Variable([0.1],dtype=tf.float32)\n",
    "W2 = tf.Variable([0.1],dtype=tf.float32)\n",
    "b = tf.Variable([0.1],dtype=tf.float32)\n",
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Model\n",
    "yhat = W1*x*x+W2*x+b\n",
    "\n",
    "# Step 3 Loss Function\n",
    "loss = tf.reduce_mean(tf.square(yhat-y))\n",
    "\n",
    "# Step 4 Optimizer\n",
    "train =tf.train.GradientDescentOptimizer(0.00001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 Training Loop\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(5000):\n",
    "    sess.run(train,feed_dict={x:X_train,y:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "\n",
    "W1 = sess.run(W1)\n",
    "W2 = sess.run(W2)\n",
    "b = sess.run(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2dbaa5278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_train,y_train,'o')\n",
    "plt.plot(X_train,W1*X_train*X_train+W2*X_train+b,'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "tf.set_random_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initial Setup\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "W = tf.Variable(tf.truncated_normal([784, 10],stddev=0.1))\n",
    "b = tf.Variable(tf.truncated_normal([10],stddev=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup Model\n",
    "yhat = tf.matmul(X,W)+b\n",
    "\n",
    "# Step 3: Cross Entropy Loss Functions\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=yhat))\n",
    "\n",
    "# Step 4: Optimizer\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of correct answer found in batches\n",
    "is_correct = tf.equal(tf.argmax(y,1),tf.argmax(yhat,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct,tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Training accuracy = 0.07 Loss = 2.86129\n",
      "2 Training accuracy = 0.14 Loss = 2.58555\n",
      "3 Training accuracy = 0.14 Loss = 2.69366\n",
      "4 Training accuracy = 0.14 Loss = 2.53634\n",
      "5 Training accuracy = 0.18 Loss = 2.52794\n",
      "6 Training accuracy = 0.15 Loss = 2.53728\n",
      "7 Training accuracy = 0.26 Loss = 2.21153\n",
      "8 Training accuracy = 0.18 Loss = 2.28506\n",
      "9 Training accuracy = 0.21 Loss = 2.2578\n",
      "10 Training accuracy = 0.18 Loss = 2.33307\n",
      "11 Training accuracy = 0.22 Loss = 2.19888\n",
      "12 Training accuracy = 0.23 Loss = 2.22489\n",
      "13 Training accuracy = 0.29 Loss = 2.13521\n",
      "14 Training accuracy = 0.24 Loss = 2.20757\n",
      "15 Training accuracy = 0.28 Loss = 2.08752\n",
      "16 Training accuracy = 0.37 Loss = 1.92847\n",
      "17 Training accuracy = 0.3 Loss = 1.95846\n",
      "18 Training accuracy = 0.31 Loss = 2.04716\n",
      "19 Training accuracy = 0.27 Loss = 2.00822\n",
      "20 Training accuracy = 0.38 Loss = 1.87219\n",
      "21 Training accuracy = 0.37 Loss = 1.80545\n",
      "22 Training accuracy = 0.37 Loss = 1.88614\n",
      "23 Training accuracy = 0.39 Loss = 1.8785\n",
      "24 Training accuracy = 0.38 Loss = 1.86552\n",
      "25 Training accuracy = 0.38 Loss = 1.7388\n",
      "26 Training accuracy = 0.48 Loss = 1.65842\n",
      "27 Training accuracy = 0.36 Loss = 1.83864\n",
      "28 Training accuracy = 0.38 Loss = 1.85051\n",
      "29 Training accuracy = 0.49 Loss = 1.67158\n",
      "30 Training accuracy = 0.58 Loss = 1.51516\n",
      "31 Training accuracy = 0.42 Loss = 1.67741\n",
      "32 Training accuracy = 0.4 Loss = 1.70157\n",
      "33 Training accuracy = 0.5 Loss = 1.63275\n",
      "34 Training accuracy = 0.57 Loss = 1.45698\n",
      "35 Training accuracy = 0.55 Loss = 1.54399\n",
      "36 Training accuracy = 0.54 Loss = 1.56143\n",
      "37 Training accuracy = 0.58 Loss = 1.53779\n",
      "38 Training accuracy = 0.66 Loss = 1.44752\n",
      "39 Training accuracy = 0.53 Loss = 1.46895\n",
      "40 Training accuracy = 0.58 Loss = 1.56306\n",
      "41 Training accuracy = 0.57 Loss = 1.45692\n",
      "42 Training accuracy = 0.59 Loss = 1.35331\n",
      "43 Training accuracy = 0.68 Loss = 1.36132\n",
      "44 Training accuracy = 0.66 Loss = 1.42813\n",
      "45 Training accuracy = 0.63 Loss = 1.33384\n",
      "46 Training accuracy = 0.66 Loss = 1.30189\n",
      "47 Training accuracy = 0.6 Loss = 1.46681\n",
      "48 Training accuracy = 0.7 Loss = 1.29551\n",
      "49 Training accuracy = 0.66 Loss = 1.32892\n",
      "50 Training accuracy = 0.7 Loss = 1.32115\n",
      "51 Training accuracy = 0.64 Loss = 1.33669\n",
      "52 Training accuracy = 0.67 Loss = 1.3377\n",
      "53 Training accuracy = 0.73 Loss = 1.25226\n",
      "54 Training accuracy = 0.71 Loss = 1.20456\n",
      "55 Training accuracy = 0.64 Loss = 1.20416\n",
      "56 Training accuracy = 0.68 Loss = 1.20981\n",
      "57 Training accuracy = 0.67 Loss = 1.19815\n",
      "58 Training accuracy = 0.74 Loss = 1.18425\n",
      "59 Training accuracy = 0.68 Loss = 1.28062\n",
      "60 Training accuracy = 0.74 Loss = 1.14118\n",
      "61 Training accuracy = 0.68 Loss = 1.12031\n",
      "62 Training accuracy = 0.71 Loss = 1.14094\n",
      "63 Training accuracy = 0.68 Loss = 1.1495\n",
      "64 Training accuracy = 0.69 Loss = 1.21913\n",
      "65 Training accuracy = 0.69 Loss = 1.22561\n",
      "66 Training accuracy = 0.72 Loss = 1.02192\n",
      "67 Training accuracy = 0.72 Loss = 1.13962\n",
      "68 Training accuracy = 0.72 Loss = 1.08973\n",
      "69 Training accuracy = 0.71 Loss = 1.15486\n",
      "70 Training accuracy = 0.73 Loss = 1.01962\n",
      "71 Training accuracy = 0.7 Loss = 1.07068\n",
      "72 Training accuracy = 0.74 Loss = 1.05576\n",
      "73 Training accuracy = 0.78 Loss = 0.933839\n",
      "74 Training accuracy = 0.75 Loss = 1.06609\n",
      "75 Training accuracy = 0.73 Loss = 1.09003\n",
      "76 Training accuracy = 0.8 Loss = 0.945796\n",
      "77 Training accuracy = 0.78 Loss = 1.04102\n",
      "78 Training accuracy = 0.78 Loss = 0.995966\n",
      "79 Training accuracy = 0.77 Loss = 0.975828\n",
      "80 Training accuracy = 0.67 Loss = 1.14173\n",
      "81 Training accuracy = 0.73 Loss = 1.05246\n",
      "82 Training accuracy = 0.74 Loss = 0.971626\n",
      "83 Training accuracy = 0.82 Loss = 0.964959\n",
      "84 Training accuracy = 0.68 Loss = 1.1156\n",
      "85 Training accuracy = 0.74 Loss = 1.01797\n",
      "86 Training accuracy = 0.81 Loss = 0.947621\n",
      "87 Training accuracy = 0.7 Loss = 1.07781\n",
      "88 Training accuracy = 0.73 Loss = 0.96266\n",
      "89 Training accuracy = 0.85 Loss = 0.877543\n",
      "90 Training accuracy = 0.73 Loss = 0.994979\n",
      "91 Training accuracy = 0.8 Loss = 0.893011\n",
      "92 Training accuracy = 0.8 Loss = 0.776373\n",
      "93 Training accuracy = 0.8 Loss = 0.874422\n",
      "94 Training accuracy = 0.77 Loss = 0.97361\n",
      "95 Training accuracy = 0.79 Loss = 0.836944\n",
      "96 Training accuracy = 0.78 Loss = 0.922769\n",
      "97 Training accuracy = 0.75 Loss = 0.936152\n",
      "98 Training accuracy = 0.8 Loss = 0.858608\n",
      "99 Training accuracy = 0.83 Loss = 0.826018\n",
      "100 Training accuracy = 0.75 Loss = 0.871678\n",
      "101 Training accuracy = 0.8 Loss = 0.88595\n",
      "102 Training accuracy = 0.85 Loss = 0.767941\n",
      "103 Training accuracy = 0.79 Loss = 0.875785\n",
      "104 Training accuracy = 0.86 Loss = 0.752866\n",
      "105 Training accuracy = 0.72 Loss = 0.962512\n",
      "106 Training accuracy = 0.79 Loss = 0.888329\n",
      "107 Training accuracy = 0.73 Loss = 0.990403\n",
      "108 Training accuracy = 0.75 Loss = 0.781233\n",
      "109 Training accuracy = 0.81 Loss = 0.730599\n",
      "110 Training accuracy = 0.77 Loss = 0.911764\n",
      "111 Training accuracy = 0.77 Loss = 0.792954\n",
      "112 Training accuracy = 0.84 Loss = 0.7544\n",
      "113 Training accuracy = 0.77 Loss = 0.941901\n",
      "114 Training accuracy = 0.79 Loss = 0.845574\n",
      "115 Training accuracy = 0.82 Loss = 0.815945\n",
      "116 Training accuracy = 0.87 Loss = 0.72542\n",
      "117 Training accuracy = 0.87 Loss = 0.715026\n",
      "118 Training accuracy = 0.8 Loss = 0.836025\n",
      "119 Training accuracy = 0.79 Loss = 0.817687\n",
      "120 Training accuracy = 0.85 Loss = 0.746476\n",
      "121 Training accuracy = 0.81 Loss = 0.788868\n",
      "122 Training accuracy = 0.77 Loss = 0.84128\n",
      "123 Training accuracy = 0.83 Loss = 0.766804\n",
      "124 Training accuracy = 0.75 Loss = 0.871439\n",
      "125 Training accuracy = 0.85 Loss = 0.722407\n",
      "126 Training accuracy = 0.84 Loss = 0.790388\n",
      "127 Training accuracy = 0.87 Loss = 0.682542\n",
      "128 Training accuracy = 0.77 Loss = 0.844482\n",
      "129 Training accuracy = 0.79 Loss = 0.815863\n",
      "130 Training accuracy = 0.8 Loss = 0.770228\n",
      "131 Training accuracy = 0.83 Loss = 0.729104\n",
      "132 Training accuracy = 0.85 Loss = 0.710262\n",
      "133 Training accuracy = 0.85 Loss = 0.723068\n",
      "134 Training accuracy = 0.77 Loss = 0.831897\n",
      "135 Training accuracy = 0.73 Loss = 0.891856\n",
      "136 Training accuracy = 0.8 Loss = 0.798123\n",
      "137 Training accuracy = 0.87 Loss = 0.703181\n",
      "138 Training accuracy = 0.85 Loss = 0.743203\n",
      "139 Training accuracy = 0.82 Loss = 0.736077\n",
      "140 Training accuracy = 0.79 Loss = 0.780959\n",
      "141 Training accuracy = 0.84 Loss = 0.680222\n",
      "142 Training accuracy = 0.91 Loss = 0.602559\n",
      "143 Training accuracy = 0.87 Loss = 0.662944\n",
      "144 Training accuracy = 0.91 Loss = 0.617741\n",
      "145 Training accuracy = 0.81 Loss = 0.756521\n",
      "146 Training accuracy = 0.76 Loss = 0.778107\n",
      "147 Training accuracy = 0.87 Loss = 0.570673\n",
      "148 Training accuracy = 0.87 Loss = 0.731534\n",
      "149 Training accuracy = 0.84 Loss = 0.680892\n",
      "150 Training accuracy = 0.85 Loss = 0.682475\n",
      "151 Training accuracy = 0.87 Loss = 0.720513\n",
      "152 Training accuracy = 0.84 Loss = 0.668624\n",
      "153 Training accuracy = 0.81 Loss = 0.734546\n",
      "154 Training accuracy = 0.79 Loss = 0.75667\n",
      "155 Training accuracy = 0.86 Loss = 0.636427\n",
      "156 Training accuracy = 0.78 Loss = 0.707888\n",
      "157 Training accuracy = 0.86 Loss = 0.58175\n",
      "158 Training accuracy = 0.77 Loss = 0.766011\n",
      "159 Training accuracy = 0.81 Loss = 0.745285\n",
      "160 Training accuracy = 0.8 Loss = 0.778409\n",
      "161 Training accuracy = 0.79 Loss = 0.766796\n",
      "162 Training accuracy = 0.8 Loss = 0.849173\n",
      "163 Training accuracy = 0.86 Loss = 0.637675\n",
      "164 Training accuracy = 0.83 Loss = 0.634802\n",
      "165 Training accuracy = 0.84 Loss = 0.611553\n",
      "166 Training accuracy = 0.92 Loss = 0.513784\n",
      "167 Training accuracy = 0.79 Loss = 0.768205\n",
      "168 Training accuracy = 0.84 Loss = 0.614236\n",
      "169 Training accuracy = 0.85 Loss = 0.599859\n",
      "170 Training accuracy = 0.79 Loss = 0.729544\n",
      "171 Training accuracy = 0.87 Loss = 0.676181\n",
      "172 Training accuracy = 0.87 Loss = 0.58744\n",
      "173 Training accuracy = 0.82 Loss = 0.792281\n",
      "174 Training accuracy = 0.85 Loss = 0.596108\n",
      "175 Training accuracy = 0.89 Loss = 0.621313\n",
      "176 Training accuracy = 0.87 Loss = 0.625802\n",
      "177 Training accuracy = 0.82 Loss = 0.603628\n",
      "178 Training accuracy = 0.83 Loss = 0.689269\n",
      "179 Training accuracy = 0.78 Loss = 0.63879\n",
      "180 Training accuracy = 0.77 Loss = 0.765457\n",
      "181 Training accuracy = 0.85 Loss = 0.606986\n",
      "182 Training accuracy = 0.83 Loss = 0.710666\n",
      "183 Training accuracy = 0.87 Loss = 0.567168\n",
      "184 Training accuracy = 0.86 Loss = 0.630422\n",
      "185 Training accuracy = 0.87 Loss = 0.618005\n",
      "186 Training accuracy = 0.81 Loss = 0.675045\n",
      "187 Training accuracy = 0.84 Loss = 0.658704\n",
      "188 Training accuracy = 0.82 Loss = 0.69264\n",
      "189 Training accuracy = 0.82 Loss = 0.723252\n",
      "190 Training accuracy = 0.81 Loss = 0.700679\n",
      "191 Training accuracy = 0.81 Loss = 0.675407\n",
      "192 Training accuracy = 0.83 Loss = 0.587583\n",
      "193 Training accuracy = 0.87 Loss = 0.592747\n",
      "194 Training accuracy = 0.84 Loss = 0.639864\n",
      "195 Training accuracy = 0.87 Loss = 0.636972\n",
      "196 Training accuracy = 0.8 Loss = 0.66675\n",
      "197 Training accuracy = 0.87 Loss = 0.561432\n",
      "198 Training accuracy = 0.82 Loss = 0.708949\n",
      "199 Training accuracy = 0.89 Loss = 0.590821\n",
      "200 Training accuracy = 0.8 Loss = 0.729823\n",
      "201 Training accuracy = 0.76 Loss = 0.838809\n",
      "202 Training accuracy = 0.92 Loss = 0.443606\n",
      "203 Training accuracy = 0.84 Loss = 0.768897\n",
      "204 Training accuracy = 0.84 Loss = 0.651474\n",
      "205 Training accuracy = 0.8 Loss = 0.689234\n",
      "206 Training accuracy = 0.83 Loss = 0.598196\n",
      "207 Training accuracy = 0.83 Loss = 0.597941\n",
      "208 Training accuracy = 0.86 Loss = 0.614428\n",
      "209 Training accuracy = 0.91 Loss = 0.453645\n",
      "210 Training accuracy = 0.89 Loss = 0.493013\n",
      "211 Training accuracy = 0.83 Loss = 0.679554\n",
      "212 Training accuracy = 0.87 Loss = 0.532625\n",
      "213 Training accuracy = 0.89 Loss = 0.593594\n",
      "214 Training accuracy = 0.89 Loss = 0.494506\n",
      "215 Training accuracy = 0.91 Loss = 0.47981\n",
      "216 Training accuracy = 0.79 Loss = 0.680528\n",
      "217 Training accuracy = 0.89 Loss = 0.528298\n",
      "218 Training accuracy = 0.91 Loss = 0.506411\n",
      "219 Training accuracy = 0.88 Loss = 0.57937\n",
      "220 Training accuracy = 0.9 Loss = 0.431237\n",
      "221 Training accuracy = 0.84 Loss = 0.57445\n",
      "222 Training accuracy = 0.84 Loss = 0.559174\n",
      "223 Training accuracy = 0.82 Loss = 0.598368\n",
      "224 Training accuracy = 0.88 Loss = 0.549309\n",
      "225 Training accuracy = 0.87 Loss = 0.570521\n",
      "226 Training accuracy = 0.9 Loss = 0.576288\n",
      "227 Training accuracy = 0.83 Loss = 0.66931\n",
      "228 Training accuracy = 0.86 Loss = 0.586959\n",
      "229 Training accuracy = 0.9 Loss = 0.491977\n",
      "230 Training accuracy = 0.87 Loss = 0.529318\n",
      "231 Training accuracy = 0.88 Loss = 0.541658\n",
      "232 Training accuracy = 0.81 Loss = 0.634328\n",
      "233 Training accuracy = 0.87 Loss = 0.527507\n",
      "234 Training accuracy = 0.88 Loss = 0.536802\n",
      "235 Training accuracy = 0.88 Loss = 0.517743\n",
      "236 Training accuracy = 0.84 Loss = 0.561882\n",
      "237 Training accuracy = 0.84 Loss = 0.580861\n",
      "238 Training accuracy = 0.85 Loss = 0.538135\n",
      "239 Training accuracy = 0.88 Loss = 0.603892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 Training accuracy = 0.87 Loss = 0.532099\n",
      "241 Training accuracy = 0.88 Loss = 0.511692\n",
      "242 Training accuracy = 0.89 Loss = 0.44237\n",
      "243 Training accuracy = 0.86 Loss = 0.449127\n",
      "244 Training accuracy = 0.85 Loss = 0.514709\n",
      "245 Training accuracy = 0.79 Loss = 0.623785\n",
      "246 Training accuracy = 0.92 Loss = 0.464282\n",
      "247 Training accuracy = 0.83 Loss = 0.704909\n",
      "248 Training accuracy = 0.89 Loss = 0.49114\n",
      "249 Training accuracy = 0.9 Loss = 0.463554\n",
      "250 Training accuracy = 0.92 Loss = 0.433482\n",
      "251 Training accuracy = 0.83 Loss = 0.589939\n",
      "252 Training accuracy = 0.89 Loss = 0.480248\n",
      "253 Training accuracy = 0.89 Loss = 0.438767\n",
      "254 Training accuracy = 0.83 Loss = 0.564336\n",
      "255 Training accuracy = 0.85 Loss = 0.637945\n",
      "256 Training accuracy = 0.85 Loss = 0.597071\n",
      "257 Training accuracy = 0.81 Loss = 0.620376\n",
      "258 Training accuracy = 0.9 Loss = 0.457534\n",
      "259 Training accuracy = 0.85 Loss = 0.57388\n",
      "260 Training accuracy = 0.86 Loss = 0.607331\n",
      "261 Training accuracy = 0.83 Loss = 0.508923\n",
      "262 Training accuracy = 0.93 Loss = 0.415794\n",
      "263 Training accuracy = 0.83 Loss = 0.550954\n",
      "264 Training accuracy = 0.9 Loss = 0.468795\n",
      "265 Training accuracy = 0.91 Loss = 0.492407\n",
      "266 Training accuracy = 0.87 Loss = 0.542635\n",
      "267 Training accuracy = 0.88 Loss = 0.562311\n",
      "268 Training accuracy = 0.89 Loss = 0.49684\n",
      "269 Training accuracy = 0.87 Loss = 0.510182\n",
      "270 Training accuracy = 0.88 Loss = 0.447805\n",
      "271 Training accuracy = 0.88 Loss = 0.47433\n",
      "272 Training accuracy = 0.86 Loss = 0.4501\n",
      "273 Training accuracy = 0.83 Loss = 0.503651\n",
      "274 Training accuracy = 0.91 Loss = 0.386584\n",
      "275 Training accuracy = 0.85 Loss = 0.518103\n",
      "276 Training accuracy = 0.88 Loss = 0.501206\n",
      "277 Training accuracy = 0.88 Loss = 0.487177\n",
      "278 Training accuracy = 0.91 Loss = 0.440343\n",
      "279 Training accuracy = 0.82 Loss = 0.542951\n",
      "280 Training accuracy = 0.9 Loss = 0.484071\n",
      "281 Training accuracy = 0.91 Loss = 0.536878\n",
      "282 Training accuracy = 0.9 Loss = 0.456129\n",
      "283 Training accuracy = 0.88 Loss = 0.556171\n",
      "284 Training accuracy = 0.83 Loss = 0.635457\n",
      "285 Training accuracy = 0.82 Loss = 0.663674\n",
      "286 Training accuracy = 0.86 Loss = 0.47102\n",
      "287 Training accuracy = 0.89 Loss = 0.481272\n",
      "288 Training accuracy = 0.87 Loss = 0.57835\n",
      "289 Training accuracy = 0.94 Loss = 0.445812\n",
      "290 Training accuracy = 0.87 Loss = 0.517492\n",
      "291 Training accuracy = 0.88 Loss = 0.504232\n",
      "292 Training accuracy = 0.83 Loss = 0.615393\n",
      "293 Training accuracy = 0.86 Loss = 0.490616\n",
      "294 Training accuracy = 0.9 Loss = 0.454138\n",
      "295 Training accuracy = 0.89 Loss = 0.506697\n",
      "296 Training accuracy = 0.85 Loss = 0.575358\n",
      "297 Training accuracy = 0.84 Loss = 0.555762\n",
      "298 Training accuracy = 0.84 Loss = 0.597313\n",
      "299 Training accuracy = 0.84 Loss = 0.523613\n",
      "300 Training accuracy = 0.89 Loss = 0.49084\n",
      "301 Training accuracy = 0.87 Loss = 0.564504\n",
      "302 Training accuracy = 0.9 Loss = 0.481536\n",
      "303 Training accuracy = 0.83 Loss = 0.530592\n",
      "304 Training accuracy = 0.89 Loss = 0.512041\n",
      "305 Training accuracy = 0.88 Loss = 0.522063\n",
      "306 Training accuracy = 0.9 Loss = 0.442794\n",
      "307 Training accuracy = 0.86 Loss = 0.54803\n",
      "308 Training accuracy = 0.92 Loss = 0.461149\n",
      "309 Training accuracy = 0.93 Loss = 0.436207\n",
      "310 Training accuracy = 0.92 Loss = 0.386183\n",
      "311 Training accuracy = 0.88 Loss = 0.517915\n",
      "312 Training accuracy = 0.89 Loss = 0.45604\n",
      "313 Training accuracy = 0.82 Loss = 0.613161\n",
      "314 Training accuracy = 0.87 Loss = 0.486337\n",
      "315 Training accuracy = 0.9 Loss = 0.417177\n",
      "316 Training accuracy = 0.81 Loss = 0.542148\n",
      "317 Training accuracy = 0.84 Loss = 0.534909\n",
      "318 Training accuracy = 0.86 Loss = 0.514846\n",
      "319 Training accuracy = 0.88 Loss = 0.472628\n",
      "320 Training accuracy = 0.93 Loss = 0.402133\n",
      "321 Training accuracy = 0.83 Loss = 0.557353\n",
      "322 Training accuracy = 0.86 Loss = 0.614614\n",
      "323 Training accuracy = 0.78 Loss = 0.659515\n",
      "324 Training accuracy = 0.85 Loss = 0.464816\n",
      "325 Training accuracy = 0.86 Loss = 0.514144\n",
      "326 Training accuracy = 0.88 Loss = 0.484792\n",
      "327 Training accuracy = 0.84 Loss = 0.466397\n",
      "328 Training accuracy = 0.87 Loss = 0.586364\n",
      "329 Training accuracy = 0.86 Loss = 0.541827\n",
      "330 Training accuracy = 0.85 Loss = 0.482054\n",
      "331 Training accuracy = 0.92 Loss = 0.366629\n",
      "332 Training accuracy = 0.9 Loss = 0.361425\n",
      "333 Training accuracy = 0.87 Loss = 0.480314\n",
      "334 Training accuracy = 0.88 Loss = 0.448435\n",
      "335 Training accuracy = 0.87 Loss = 0.494064\n",
      "336 Training accuracy = 0.9 Loss = 0.373504\n",
      "337 Training accuracy = 0.94 Loss = 0.381733\n",
      "338 Training accuracy = 0.81 Loss = 0.581969\n",
      "339 Training accuracy = 0.85 Loss = 0.52901\n",
      "340 Training accuracy = 0.91 Loss = 0.359629\n",
      "341 Training accuracy = 0.91 Loss = 0.392251\n",
      "342 Training accuracy = 0.83 Loss = 0.573156\n",
      "343 Training accuracy = 0.82 Loss = 0.632988\n",
      "344 Training accuracy = 0.83 Loss = 0.596828\n",
      "345 Training accuracy = 0.85 Loss = 0.534997\n",
      "346 Training accuracy = 0.89 Loss = 0.437789\n",
      "347 Training accuracy = 0.9 Loss = 0.446128\n",
      "348 Training accuracy = 0.89 Loss = 0.533858\n",
      "349 Training accuracy = 0.8 Loss = 0.602345\n",
      "350 Training accuracy = 0.93 Loss = 0.378702\n",
      "351 Training accuracy = 0.9 Loss = 0.43814\n",
      "352 Training accuracy = 0.89 Loss = 0.456111\n",
      "353 Training accuracy = 0.88 Loss = 0.469115\n",
      "354 Training accuracy = 0.87 Loss = 0.499376\n",
      "355 Training accuracy = 0.84 Loss = 0.490434\n",
      "356 Training accuracy = 0.92 Loss = 0.440601\n",
      "357 Training accuracy = 0.89 Loss = 0.500344\n",
      "358 Training accuracy = 0.83 Loss = 0.528796\n",
      "359 Training accuracy = 0.89 Loss = 0.410119\n",
      "360 Training accuracy = 0.89 Loss = 0.478859\n",
      "361 Training accuracy = 0.84 Loss = 0.495507\n",
      "362 Training accuracy = 0.92 Loss = 0.385217\n",
      "363 Training accuracy = 0.88 Loss = 0.448692\n",
      "364 Training accuracy = 0.89 Loss = 0.464898\n",
      "365 Training accuracy = 0.87 Loss = 0.48767\n",
      "366 Training accuracy = 0.92 Loss = 0.41805\n",
      "367 Training accuracy = 0.91 Loss = 0.43284\n",
      "368 Training accuracy = 0.86 Loss = 0.445051\n",
      "369 Training accuracy = 0.86 Loss = 0.49621\n",
      "370 Training accuracy = 0.85 Loss = 0.510181\n",
      "371 Training accuracy = 0.85 Loss = 0.649717\n",
      "372 Training accuracy = 0.92 Loss = 0.43331\n",
      "373 Training accuracy = 0.87 Loss = 0.43266\n",
      "374 Training accuracy = 0.91 Loss = 0.355025\n",
      "375 Training accuracy = 0.95 Loss = 0.298415\n",
      "376 Training accuracy = 0.91 Loss = 0.396541\n",
      "377 Training accuracy = 0.92 Loss = 0.413867\n",
      "378 Training accuracy = 0.84 Loss = 0.554174\n",
      "379 Training accuracy = 0.89 Loss = 0.410363\n",
      "380 Training accuracy = 0.89 Loss = 0.37633\n",
      "381 Training accuracy = 0.88 Loss = 0.474309\n",
      "382 Training accuracy = 0.86 Loss = 0.413306\n",
      "383 Training accuracy = 0.89 Loss = 0.470609\n",
      "384 Training accuracy = 0.91 Loss = 0.407708\n",
      "385 Training accuracy = 0.92 Loss = 0.419157\n",
      "386 Training accuracy = 0.88 Loss = 0.479026\n",
      "387 Training accuracy = 0.88 Loss = 0.438298\n",
      "388 Training accuracy = 0.9 Loss = 0.434931\n",
      "389 Training accuracy = 0.89 Loss = 0.528913\n",
      "390 Training accuracy = 0.9 Loss = 0.389213\n",
      "391 Training accuracy = 0.88 Loss = 0.448726\n",
      "392 Training accuracy = 0.9 Loss = 0.431172\n",
      "393 Training accuracy = 0.92 Loss = 0.477686\n",
      "394 Training accuracy = 0.86 Loss = 0.467191\n",
      "395 Training accuracy = 0.87 Loss = 0.40096\n",
      "396 Training accuracy = 0.77 Loss = 0.696622\n",
      "397 Training accuracy = 0.91 Loss = 0.393747\n",
      "398 Training accuracy = 0.87 Loss = 0.537051\n",
      "399 Training accuracy = 0.88 Loss = 0.377399\n",
      "400 Training accuracy = 0.87 Loss = 0.565667\n",
      "401 Training accuracy = 0.92 Loss = 0.383177\n",
      "402 Training accuracy = 0.93 Loss = 0.36231\n",
      "403 Training accuracy = 0.89 Loss = 0.456748\n",
      "404 Training accuracy = 0.93 Loss = 0.361059\n",
      "405 Training accuracy = 0.92 Loss = 0.347442\n",
      "406 Training accuracy = 0.82 Loss = 0.544094\n",
      "407 Training accuracy = 0.94 Loss = 0.331953\n",
      "408 Training accuracy = 0.93 Loss = 0.283741\n",
      "409 Training accuracy = 0.86 Loss = 0.528485\n",
      "410 Training accuracy = 0.94 Loss = 0.397051\n",
      "411 Training accuracy = 0.79 Loss = 0.591546\n",
      "412 Training accuracy = 0.87 Loss = 0.419079\n",
      "413 Training accuracy = 0.91 Loss = 0.370003\n",
      "414 Training accuracy = 0.87 Loss = 0.464431\n",
      "415 Training accuracy = 0.89 Loss = 0.462629\n",
      "416 Training accuracy = 0.92 Loss = 0.433734\n",
      "417 Training accuracy = 0.89 Loss = 0.397813\n",
      "418 Training accuracy = 0.88 Loss = 0.468749\n",
      "419 Training accuracy = 0.83 Loss = 0.577378\n",
      "420 Training accuracy = 0.89 Loss = 0.406893\n",
      "421 Training accuracy = 0.88 Loss = 0.523285\n",
      "422 Training accuracy = 0.9 Loss = 0.411782\n",
      "423 Training accuracy = 0.88 Loss = 0.411566\n",
      "424 Training accuracy = 0.88 Loss = 0.39456\n",
      "425 Training accuracy = 0.88 Loss = 0.363595\n",
      "426 Training accuracy = 0.89 Loss = 0.374094\n",
      "427 Training accuracy = 0.89 Loss = 0.462578\n",
      "428 Training accuracy = 0.86 Loss = 0.445359\n",
      "429 Training accuracy = 0.89 Loss = 0.343903\n",
      "430 Training accuracy = 0.87 Loss = 0.472594\n",
      "431 Training accuracy = 0.86 Loss = 0.563022\n",
      "432 Training accuracy = 0.89 Loss = 0.475082\n",
      "433 Training accuracy = 0.88 Loss = 0.512008\n",
      "434 Training accuracy = 0.95 Loss = 0.249634\n",
      "435 Training accuracy = 0.85 Loss = 0.445996\n",
      "436 Training accuracy = 0.91 Loss = 0.305487\n",
      "437 Training accuracy = 0.86 Loss = 0.482775\n",
      "438 Training accuracy = 0.88 Loss = 0.445829\n",
      "439 Training accuracy = 0.87 Loss = 0.516921\n",
      "440 Training accuracy = 0.92 Loss = 0.384841\n",
      "441 Training accuracy = 0.89 Loss = 0.436805\n",
      "442 Training accuracy = 0.88 Loss = 0.461306\n",
      "443 Training accuracy = 0.9 Loss = 0.399221\n",
      "444 Training accuracy = 0.83 Loss = 0.441207\n",
      "445 Training accuracy = 0.89 Loss = 0.440898\n",
      "446 Training accuracy = 0.86 Loss = 0.578558\n",
      "447 Training accuracy = 0.89 Loss = 0.374871\n",
      "448 Training accuracy = 0.82 Loss = 0.506162\n",
      "449 Training accuracy = 0.82 Loss = 0.558775\n",
      "450 Training accuracy = 0.92 Loss = 0.413647\n",
      "451 Training accuracy = 0.89 Loss = 0.529561\n",
      "452 Training accuracy = 0.85 Loss = 0.514093\n",
      "453 Training accuracy = 0.92 Loss = 0.350152\n",
      "454 Training accuracy = 0.86 Loss = 0.504732\n",
      "455 Training accuracy = 0.88 Loss = 0.499365\n",
      "456 Training accuracy = 0.88 Loss = 0.427125\n",
      "457 Training accuracy = 0.9 Loss = 0.458219\n",
      "458 Training accuracy = 0.83 Loss = 0.608741\n",
      "459 Training accuracy = 0.89 Loss = 0.404622\n",
      "460 Training accuracy = 0.87 Loss = 0.428555\n",
      "461 Training accuracy = 0.85 Loss = 0.501792\n",
      "462 Training accuracy = 0.91 Loss = 0.393592\n",
      "463 Training accuracy = 0.93 Loss = 0.364703\n",
      "464 Training accuracy = 0.9 Loss = 0.41548\n",
      "465 Training accuracy = 0.89 Loss = 0.458008\n",
      "466 Training accuracy = 0.87 Loss = 0.509343\n",
      "467 Training accuracy = 0.84 Loss = 0.556038\n",
      "468 Training accuracy = 0.89 Loss = 0.415584\n",
      "469 Training accuracy = 0.89 Loss = 0.386403\n",
      "470 Training accuracy = 0.89 Loss = 0.549319\n",
      "471 Training accuracy = 0.94 Loss = 0.28948\n",
      "472 Training accuracy = 0.92 Loss = 0.322469\n",
      "473 Training accuracy = 0.85 Loss = 0.470183\n",
      "474 Training accuracy = 0.9 Loss = 0.42313\n",
      "475 Training accuracy = 0.9 Loss = 0.354328\n",
      "476 Training accuracy = 0.87 Loss = 0.416171\n",
      "477 Training accuracy = 0.88 Loss = 0.509592\n",
      "478 Training accuracy = 0.88 Loss = 0.366311\n",
      "479 Training accuracy = 0.88 Loss = 0.526117\n",
      "480 Training accuracy = 0.86 Loss = 0.421711\n",
      "481 Training accuracy = 0.87 Loss = 0.47543\n",
      "482 Training accuracy = 0.89 Loss = 0.383775\n",
      "483 Training accuracy = 0.93 Loss = 0.31395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484 Training accuracy = 0.89 Loss = 0.448851\n",
      "485 Training accuracy = 0.77 Loss = 0.812785\n",
      "486 Training accuracy = 0.86 Loss = 0.457918\n",
      "487 Training accuracy = 0.92 Loss = 0.280249\n",
      "488 Training accuracy = 0.91 Loss = 0.35715\n",
      "489 Training accuracy = 0.9 Loss = 0.434903\n",
      "490 Training accuracy = 0.88 Loss = 0.470763\n",
      "491 Training accuracy = 0.82 Loss = 0.511757\n",
      "492 Training accuracy = 0.86 Loss = 0.420659\n",
      "493 Training accuracy = 0.87 Loss = 0.468644\n",
      "494 Training accuracy = 0.9 Loss = 0.374301\n",
      "495 Training accuracy = 0.88 Loss = 0.436911\n",
      "496 Training accuracy = 0.86 Loss = 0.398345\n",
      "497 Training accuracy = 0.91 Loss = 0.311373\n",
      "498 Training accuracy = 0.88 Loss = 0.391876\n",
      "499 Training accuracy = 0.92 Loss = 0.341773\n",
      "500 Training accuracy = 0.84 Loss = 0.563195\n",
      "501 Training accuracy = 0.92 Loss = 0.356808\n",
      "502 Training accuracy = 0.88 Loss = 0.392763\n",
      "503 Training accuracy = 0.91 Loss = 0.322213\n",
      "504 Training accuracy = 0.87 Loss = 0.440543\n",
      "505 Training accuracy = 0.91 Loss = 0.444531\n",
      "506 Training accuracy = 0.88 Loss = 0.517503\n",
      "507 Training accuracy = 0.89 Loss = 0.546189\n",
      "508 Training accuracy = 0.91 Loss = 0.360789\n",
      "509 Training accuracy = 0.91 Loss = 0.462468\n",
      "510 Training accuracy = 0.91 Loss = 0.424607\n",
      "511 Training accuracy = 0.9 Loss = 0.477241\n",
      "512 Training accuracy = 0.86 Loss = 0.396153\n",
      "513 Training accuracy = 0.87 Loss = 0.434551\n",
      "514 Training accuracy = 0.98 Loss = 0.239582\n",
      "515 Training accuracy = 0.91 Loss = 0.403687\n",
      "516 Training accuracy = 0.92 Loss = 0.260678\n",
      "517 Training accuracy = 0.89 Loss = 0.378344\n",
      "518 Training accuracy = 0.9 Loss = 0.406585\n",
      "519 Training accuracy = 0.88 Loss = 0.503876\n",
      "520 Training accuracy = 0.94 Loss = 0.321514\n",
      "521 Training accuracy = 0.89 Loss = 0.389678\n",
      "522 Training accuracy = 0.91 Loss = 0.405108\n",
      "523 Training accuracy = 0.9 Loss = 0.418969\n",
      "524 Training accuracy = 0.91 Loss = 0.369309\n",
      "525 Training accuracy = 0.88 Loss = 0.369331\n",
      "526 Training accuracy = 0.85 Loss = 0.489541\n",
      "527 Training accuracy = 0.87 Loss = 0.459958\n",
      "528 Training accuracy = 0.91 Loss = 0.339147\n",
      "529 Training accuracy = 0.88 Loss = 0.399112\n",
      "530 Training accuracy = 0.92 Loss = 0.406918\n",
      "531 Training accuracy = 0.82 Loss = 0.445008\n",
      "532 Training accuracy = 0.88 Loss = 0.405885\n",
      "533 Training accuracy = 0.91 Loss = 0.288988\n",
      "534 Training accuracy = 0.9 Loss = 0.363379\n",
      "535 Training accuracy = 0.88 Loss = 0.459459\n",
      "536 Training accuracy = 0.88 Loss = 0.361215\n",
      "537 Training accuracy = 0.91 Loss = 0.401336\n",
      "538 Training accuracy = 0.85 Loss = 0.495767\n",
      "539 Training accuracy = 0.93 Loss = 0.301478\n",
      "540 Training accuracy = 0.89 Loss = 0.377774\n",
      "541 Training accuracy = 0.89 Loss = 0.406288\n",
      "542 Training accuracy = 0.81 Loss = 0.608396\n",
      "543 Training accuracy = 0.84 Loss = 0.4447\n",
      "544 Training accuracy = 0.91 Loss = 0.430366\n",
      "545 Training accuracy = 0.94 Loss = 0.343398\n",
      "546 Training accuracy = 0.9 Loss = 0.375169\n",
      "547 Training accuracy = 0.88 Loss = 0.421263\n",
      "548 Training accuracy = 0.87 Loss = 0.436888\n",
      "549 Training accuracy = 0.9 Loss = 0.506531\n",
      "550 Training accuracy = 0.85 Loss = 0.410155\n",
      "551 Training accuracy = 0.89 Loss = 0.345377\n",
      "552 Training accuracy = 0.89 Loss = 0.383914\n",
      "553 Training accuracy = 0.88 Loss = 0.505374\n",
      "554 Training accuracy = 0.9 Loss = 0.357984\n",
      "555 Training accuracy = 0.9 Loss = 0.408815\n",
      "556 Training accuracy = 0.84 Loss = 0.508529\n",
      "557 Training accuracy = 0.92 Loss = 0.32489\n",
      "558 Training accuracy = 0.92 Loss = 0.369669\n",
      "559 Training accuracy = 0.92 Loss = 0.324783\n",
      "560 Training accuracy = 0.89 Loss = 0.465643\n",
      "561 Training accuracy = 0.93 Loss = 0.340466\n",
      "562 Training accuracy = 0.88 Loss = 0.384742\n",
      "563 Training accuracy = 0.9 Loss = 0.317068\n",
      "564 Training accuracy = 0.87 Loss = 0.602583\n",
      "565 Training accuracy = 0.92 Loss = 0.368411\n",
      "566 Training accuracy = 0.95 Loss = 0.293706\n",
      "567 Training accuracy = 0.92 Loss = 0.332735\n",
      "568 Training accuracy = 0.89 Loss = 0.365505\n",
      "569 Training accuracy = 0.88 Loss = 0.489676\n",
      "570 Training accuracy = 0.88 Loss = 0.425104\n",
      "571 Training accuracy = 0.91 Loss = 0.346214\n",
      "572 Training accuracy = 0.88 Loss = 0.400477\n",
      "573 Training accuracy = 0.84 Loss = 0.516357\n",
      "574 Training accuracy = 0.89 Loss = 0.393104\n",
      "575 Training accuracy = 0.94 Loss = 0.271617\n",
      "576 Training accuracy = 0.85 Loss = 0.456453\n",
      "577 Training accuracy = 0.92 Loss = 0.310046\n",
      "578 Training accuracy = 0.88 Loss = 0.505845\n",
      "579 Training accuracy = 0.91 Loss = 0.398089\n",
      "580 Training accuracy = 0.91 Loss = 0.350104\n",
      "581 Training accuracy = 0.92 Loss = 0.337102\n",
      "582 Training accuracy = 0.94 Loss = 0.299415\n",
      "583 Training accuracy = 0.88 Loss = 0.361442\n",
      "584 Training accuracy = 0.95 Loss = 0.29328\n",
      "585 Training accuracy = 0.86 Loss = 0.424276\n",
      "586 Training accuracy = 0.9 Loss = 0.374053\n",
      "587 Training accuracy = 0.9 Loss = 0.458419\n",
      "588 Training accuracy = 0.88 Loss = 0.501256\n",
      "589 Training accuracy = 0.85 Loss = 0.449276\n",
      "590 Training accuracy = 0.92 Loss = 0.362304\n",
      "591 Training accuracy = 0.9 Loss = 0.404965\n",
      "592 Training accuracy = 0.94 Loss = 0.341519\n",
      "593 Training accuracy = 0.94 Loss = 0.321365\n",
      "594 Training accuracy = 0.84 Loss = 0.491021\n",
      "595 Training accuracy = 0.9 Loss = 0.360983\n",
      "596 Training accuracy = 0.85 Loss = 0.503414\n",
      "597 Training accuracy = 0.91 Loss = 0.306963\n",
      "598 Training accuracy = 0.89 Loss = 0.393095\n",
      "599 Training accuracy = 0.9 Loss = 0.432586\n",
      "600 Training accuracy = 0.85 Loss = 0.401612\n",
      "601 Training accuracy = 0.95 Loss = 0.311322\n",
      "602 Training accuracy = 0.87 Loss = 0.437901\n",
      "603 Training accuracy = 0.86 Loss = 0.362341\n",
      "604 Training accuracy = 0.91 Loss = 0.334625\n",
      "605 Training accuracy = 0.88 Loss = 0.409078\n",
      "606 Training accuracy = 0.89 Loss = 0.435382\n",
      "607 Training accuracy = 0.87 Loss = 0.416769\n",
      "608 Training accuracy = 0.87 Loss = 0.360619\n",
      "609 Training accuracy = 0.86 Loss = 0.386998\n",
      "610 Training accuracy = 0.86 Loss = 0.517312\n",
      "611 Training accuracy = 0.91 Loss = 0.331347\n",
      "612 Training accuracy = 0.94 Loss = 0.387725\n",
      "613 Training accuracy = 0.91 Loss = 0.372389\n",
      "614 Training accuracy = 0.91 Loss = 0.347151\n",
      "615 Training accuracy = 0.91 Loss = 0.301228\n",
      "616 Training accuracy = 0.88 Loss = 0.387829\n",
      "617 Training accuracy = 0.91 Loss = 0.350835\n",
      "618 Training accuracy = 0.87 Loss = 0.383043\n",
      "619 Training accuracy = 0.84 Loss = 0.467553\n",
      "620 Training accuracy = 0.9 Loss = 0.391211\n",
      "621 Training accuracy = 0.92 Loss = 0.323024\n",
      "622 Training accuracy = 0.84 Loss = 0.453079\n",
      "623 Training accuracy = 0.88 Loss = 0.414124\n",
      "624 Training accuracy = 0.87 Loss = 0.403066\n",
      "625 Training accuracy = 0.88 Loss = 0.435223\n",
      "626 Training accuracy = 0.87 Loss = 0.43469\n",
      "627 Training accuracy = 0.84 Loss = 0.536843\n",
      "628 Training accuracy = 0.9 Loss = 0.3924\n",
      "629 Training accuracy = 0.91 Loss = 0.366347\n",
      "630 Training accuracy = 0.9 Loss = 0.4045\n",
      "631 Training accuracy = 0.93 Loss = 0.35348\n",
      "632 Training accuracy = 0.89 Loss = 0.429057\n",
      "633 Training accuracy = 0.97 Loss = 0.271149\n",
      "634 Training accuracy = 0.9 Loss = 0.360226\n",
      "635 Training accuracy = 0.91 Loss = 0.370715\n",
      "636 Training accuracy = 0.94 Loss = 0.304558\n",
      "637 Training accuracy = 0.9 Loss = 0.327092\n",
      "638 Training accuracy = 0.79 Loss = 0.589232\n",
      "639 Training accuracy = 0.94 Loss = 0.284326\n",
      "640 Training accuracy = 0.89 Loss = 0.504606\n",
      "641 Training accuracy = 0.88 Loss = 0.439635\n",
      "642 Training accuracy = 0.88 Loss = 0.396608\n",
      "643 Training accuracy = 0.88 Loss = 0.483219\n",
      "644 Training accuracy = 0.9 Loss = 0.406487\n",
      "645 Training accuracy = 0.89 Loss = 0.381047\n",
      "646 Training accuracy = 0.91 Loss = 0.40642\n",
      "647 Training accuracy = 0.88 Loss = 0.372211\n",
      "648 Training accuracy = 0.88 Loss = 0.400884\n",
      "649 Training accuracy = 0.91 Loss = 0.397871\n",
      "650 Training accuracy = 0.86 Loss = 0.43281\n",
      "651 Training accuracy = 0.87 Loss = 0.472889\n",
      "652 Training accuracy = 0.91 Loss = 0.393324\n",
      "653 Training accuracy = 0.88 Loss = 0.403207\n",
      "654 Training accuracy = 0.94 Loss = 0.343457\n",
      "655 Training accuracy = 0.9 Loss = 0.320142\n",
      "656 Training accuracy = 0.91 Loss = 0.330287\n",
      "657 Training accuracy = 0.9 Loss = 0.434114\n",
      "658 Training accuracy = 0.88 Loss = 0.388658\n",
      "659 Training accuracy = 0.88 Loss = 0.35551\n",
      "660 Training accuracy = 0.92 Loss = 0.300731\n",
      "661 Training accuracy = 0.89 Loss = 0.40099\n",
      "662 Training accuracy = 0.91 Loss = 0.293559\n",
      "663 Training accuracy = 0.94 Loss = 0.329424\n",
      "664 Training accuracy = 0.85 Loss = 0.441027\n",
      "665 Training accuracy = 0.87 Loss = 0.45823\n",
      "666 Training accuracy = 0.92 Loss = 0.334888\n",
      "667 Training accuracy = 0.95 Loss = 0.355445\n",
      "668 Training accuracy = 0.9 Loss = 0.34852\n",
      "669 Training accuracy = 0.86 Loss = 0.391234\n",
      "670 Training accuracy = 0.86 Loss = 0.429444\n",
      "671 Training accuracy = 0.87 Loss = 0.420527\n",
      "672 Training accuracy = 0.93 Loss = 0.330522\n",
      "673 Training accuracy = 0.94 Loss = 0.294663\n",
      "674 Training accuracy = 0.9 Loss = 0.390897\n",
      "675 Training accuracy = 0.92 Loss = 0.399673\n",
      "676 Training accuracy = 0.85 Loss = 0.438312\n",
      "677 Training accuracy = 0.88 Loss = 0.39486\n",
      "678 Training accuracy = 0.93 Loss = 0.378038\n",
      "679 Training accuracy = 0.91 Loss = 0.308879\n",
      "680 Training accuracy = 0.88 Loss = 0.41531\n",
      "681 Training accuracy = 0.92 Loss = 0.383456\n",
      "682 Training accuracy = 0.86 Loss = 0.534826\n",
      "683 Training accuracy = 0.92 Loss = 0.408208\n",
      "684 Training accuracy = 0.9 Loss = 0.371491\n",
      "685 Training accuracy = 0.87 Loss = 0.374018\n",
      "686 Training accuracy = 0.9 Loss = 0.357187\n",
      "687 Training accuracy = 0.91 Loss = 0.343415\n",
      "688 Training accuracy = 0.9 Loss = 0.364268\n",
      "689 Training accuracy = 0.89 Loss = 0.44465\n",
      "690 Training accuracy = 0.89 Loss = 0.341943\n",
      "691 Training accuracy = 0.91 Loss = 0.304252\n",
      "692 Training accuracy = 0.9 Loss = 0.390573\n",
      "693 Training accuracy = 0.88 Loss = 0.449741\n",
      "694 Training accuracy = 0.83 Loss = 0.482891\n",
      "695 Training accuracy = 0.86 Loss = 0.532866\n",
      "696 Training accuracy = 0.9 Loss = 0.372686\n",
      "697 Training accuracy = 0.87 Loss = 0.379407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 Training accuracy = 0.91 Loss = 0.346828\n",
      "699 Training accuracy = 0.91 Loss = 0.274204\n",
      "700 Training accuracy = 0.92 Loss = 0.358798\n",
      "701 Training accuracy = 0.89 Loss = 0.336991\n",
      "702 Training accuracy = 0.89 Loss = 0.418651\n",
      "703 Training accuracy = 0.87 Loss = 0.434193\n",
      "704 Training accuracy = 0.93 Loss = 0.309254\n",
      "705 Training accuracy = 0.88 Loss = 0.341787\n",
      "706 Training accuracy = 0.9 Loss = 0.391291\n",
      "707 Training accuracy = 0.88 Loss = 0.367133\n",
      "708 Training accuracy = 0.89 Loss = 0.398692\n",
      "709 Training accuracy = 0.93 Loss = 0.301537\n",
      "710 Training accuracy = 0.89 Loss = 0.348018\n",
      "711 Training accuracy = 0.9 Loss = 0.455913\n",
      "712 Training accuracy = 0.89 Loss = 0.372745\n",
      "713 Training accuracy = 0.93 Loss = 0.35731\n",
      "714 Training accuracy = 0.88 Loss = 0.412573\n",
      "715 Training accuracy = 0.89 Loss = 0.401788\n",
      "716 Training accuracy = 0.94 Loss = 0.257154\n",
      "717 Training accuracy = 0.87 Loss = 0.398052\n",
      "718 Training accuracy = 0.92 Loss = 0.338166\n",
      "719 Training accuracy = 0.88 Loss = 0.582334\n",
      "720 Training accuracy = 0.86 Loss = 0.534816\n",
      "721 Training accuracy = 0.91 Loss = 0.340511\n",
      "722 Training accuracy = 0.87 Loss = 0.377471\n",
      "723 Training accuracy = 0.89 Loss = 0.362739\n",
      "724 Training accuracy = 0.91 Loss = 0.360985\n",
      "725 Training accuracy = 0.88 Loss = 0.456716\n",
      "726 Training accuracy = 0.88 Loss = 0.411795\n",
      "727 Training accuracy = 0.92 Loss = 0.282323\n",
      "728 Training accuracy = 0.84 Loss = 0.446123\n",
      "729 Training accuracy = 0.96 Loss = 0.183407\n",
      "730 Training accuracy = 0.92 Loss = 0.249526\n",
      "731 Training accuracy = 0.94 Loss = 0.287826\n",
      "732 Training accuracy = 0.94 Loss = 0.274347\n",
      "733 Training accuracy = 0.9 Loss = 0.338585\n",
      "734 Training accuracy = 0.89 Loss = 0.439749\n",
      "735 Training accuracy = 0.9 Loss = 0.326733\n",
      "736 Training accuracy = 0.9 Loss = 0.288085\n",
      "737 Training accuracy = 0.91 Loss = 0.353803\n",
      "738 Training accuracy = 0.87 Loss = 0.36536\n",
      "739 Training accuracy = 0.93 Loss = 0.316313\n",
      "740 Training accuracy = 0.88 Loss = 0.441523\n",
      "741 Training accuracy = 0.86 Loss = 0.494533\n",
      "742 Training accuracy = 0.93 Loss = 0.26301\n",
      "743 Training accuracy = 0.86 Loss = 0.488928\n",
      "744 Training accuracy = 0.87 Loss = 0.481624\n",
      "745 Training accuracy = 0.94 Loss = 0.298908\n",
      "746 Training accuracy = 0.88 Loss = 0.32599\n",
      "747 Training accuracy = 0.93 Loss = 0.303543\n",
      "748 Training accuracy = 0.89 Loss = 0.327139\n",
      "749 Training accuracy = 0.9 Loss = 0.349335\n",
      "750 Training accuracy = 0.92 Loss = 0.310797\n",
      "751 Training accuracy = 0.93 Loss = 0.300882\n",
      "752 Training accuracy = 0.88 Loss = 0.407865\n",
      "753 Training accuracy = 0.9 Loss = 0.404583\n",
      "754 Training accuracy = 0.89 Loss = 0.390856\n",
      "755 Training accuracy = 0.9 Loss = 0.338817\n",
      "756 Training accuracy = 0.9 Loss = 0.465574\n",
      "757 Training accuracy = 0.86 Loss = 0.46486\n",
      "758 Training accuracy = 0.87 Loss = 0.356319\n",
      "759 Training accuracy = 0.91 Loss = 0.400434\n",
      "760 Training accuracy = 0.94 Loss = 0.276381\n",
      "761 Training accuracy = 0.92 Loss = 0.282128\n",
      "762 Training accuracy = 0.9 Loss = 0.304873\n",
      "763 Training accuracy = 0.87 Loss = 0.39083\n",
      "764 Training accuracy = 0.87 Loss = 0.367512\n",
      "765 Training accuracy = 0.94 Loss = 0.247774\n",
      "766 Training accuracy = 0.97 Loss = 0.222927\n",
      "767 Training accuracy = 0.89 Loss = 0.319281\n",
      "768 Training accuracy = 0.9 Loss = 0.345482\n",
      "769 Training accuracy = 0.95 Loss = 0.328482\n",
      "770 Training accuracy = 0.87 Loss = 0.319851\n",
      "771 Training accuracy = 0.92 Loss = 0.301215\n",
      "772 Training accuracy = 0.91 Loss = 0.397821\n",
      "773 Training accuracy = 0.92 Loss = 0.334227\n",
      "774 Training accuracy = 0.94 Loss = 0.303483\n",
      "775 Training accuracy = 0.89 Loss = 0.35717\n",
      "776 Training accuracy = 0.89 Loss = 0.524616\n",
      "777 Training accuracy = 0.93 Loss = 0.331292\n",
      "778 Training accuracy = 0.88 Loss = 0.40346\n",
      "779 Training accuracy = 0.91 Loss = 0.364837\n",
      "780 Training accuracy = 0.92 Loss = 0.299944\n",
      "781 Training accuracy = 0.91 Loss = 0.338178\n",
      "782 Training accuracy = 0.89 Loss = 0.372664\n",
      "783 Training accuracy = 0.95 Loss = 0.247709\n",
      "784 Training accuracy = 0.92 Loss = 0.362338\n",
      "785 Training accuracy = 0.89 Loss = 0.35839\n",
      "786 Training accuracy = 0.87 Loss = 0.390983\n",
      "787 Training accuracy = 0.86 Loss = 0.37555\n",
      "788 Training accuracy = 0.91 Loss = 0.278657\n",
      "789 Training accuracy = 0.87 Loss = 0.36608\n",
      "790 Training accuracy = 0.93 Loss = 0.401157\n",
      "791 Training accuracy = 0.86 Loss = 0.473075\n",
      "792 Training accuracy = 0.92 Loss = 0.35421\n",
      "793 Training accuracy = 0.87 Loss = 0.482073\n",
      "794 Training accuracy = 0.93 Loss = 0.292781\n",
      "795 Training accuracy = 0.92 Loss = 0.343297\n",
      "796 Training accuracy = 0.91 Loss = 0.36484\n",
      "797 Training accuracy = 0.89 Loss = 0.384684\n",
      "798 Training accuracy = 0.9 Loss = 0.373016\n",
      "799 Training accuracy = 0.86 Loss = 0.551666\n",
      "800 Training accuracy = 0.83 Loss = 0.4676\n",
      "801 Training accuracy = 0.9 Loss = 0.323034\n",
      "802 Training accuracy = 0.93 Loss = 0.322952\n",
      "803 Training accuracy = 0.91 Loss = 0.333284\n",
      "804 Training accuracy = 0.86 Loss = 0.447668\n",
      "805 Training accuracy = 0.91 Loss = 0.285154\n",
      "806 Training accuracy = 0.91 Loss = 0.320276\n",
      "807 Training accuracy = 0.87 Loss = 0.424329\n",
      "808 Training accuracy = 0.93 Loss = 0.39728\n",
      "809 Training accuracy = 0.84 Loss = 0.392308\n",
      "810 Training accuracy = 0.91 Loss = 0.334466\n",
      "811 Training accuracy = 0.93 Loss = 0.421948\n",
      "812 Training accuracy = 0.88 Loss = 0.35307\n",
      "813 Training accuracy = 0.95 Loss = 0.259158\n",
      "814 Training accuracy = 0.87 Loss = 0.395152\n",
      "815 Training accuracy = 0.91 Loss = 0.404339\n",
      "816 Training accuracy = 0.82 Loss = 0.525881\n",
      "817 Training accuracy = 0.86 Loss = 0.463828\n",
      "818 Training accuracy = 0.92 Loss = 0.330197\n",
      "819 Training accuracy = 0.92 Loss = 0.287441\n",
      "820 Training accuracy = 0.89 Loss = 0.35982\n",
      "821 Training accuracy = 0.94 Loss = 0.231078\n",
      "822 Training accuracy = 0.95 Loss = 0.333078\n",
      "823 Training accuracy = 0.93 Loss = 0.252144\n",
      "824 Training accuracy = 0.89 Loss = 0.309109\n",
      "825 Training accuracy = 0.91 Loss = 0.366625\n",
      "826 Training accuracy = 0.91 Loss = 0.374258\n",
      "827 Training accuracy = 0.94 Loss = 0.302508\n",
      "828 Training accuracy = 0.86 Loss = 0.385578\n",
      "829 Training accuracy = 0.93 Loss = 0.321258\n",
      "830 Training accuracy = 0.92 Loss = 0.381199\n",
      "831 Training accuracy = 0.87 Loss = 0.391505\n",
      "832 Training accuracy = 0.91 Loss = 0.311048\n",
      "833 Training accuracy = 0.95 Loss = 0.277806\n",
      "834 Training accuracy = 0.99 Loss = 0.148136\n",
      "835 Training accuracy = 0.87 Loss = 0.36262\n",
      "836 Training accuracy = 0.9 Loss = 0.353705\n",
      "837 Training accuracy = 0.92 Loss = 0.377581\n",
      "838 Training accuracy = 0.9 Loss = 0.322057\n",
      "839 Training accuracy = 0.95 Loss = 0.287995\n",
      "840 Training accuracy = 0.88 Loss = 0.390925\n",
      "841 Training accuracy = 0.88 Loss = 0.39039\n",
      "842 Training accuracy = 0.9 Loss = 0.361702\n",
      "843 Training accuracy = 0.86 Loss = 0.616618\n",
      "844 Training accuracy = 0.93 Loss = 0.265292\n",
      "845 Training accuracy = 0.87 Loss = 0.413805\n",
      "846 Training accuracy = 0.94 Loss = 0.279841\n",
      "847 Training accuracy = 0.89 Loss = 0.321013\n",
      "848 Training accuracy = 0.88 Loss = 0.403432\n",
      "849 Training accuracy = 0.89 Loss = 0.395339\n",
      "850 Training accuracy = 0.89 Loss = 0.404213\n",
      "851 Training accuracy = 0.89 Loss = 0.306584\n",
      "852 Training accuracy = 0.95 Loss = 0.284122\n",
      "853 Training accuracy = 0.91 Loss = 0.360273\n",
      "854 Training accuracy = 0.92 Loss = 0.361712\n",
      "855 Training accuracy = 0.89 Loss = 0.42555\n",
      "856 Training accuracy = 0.89 Loss = 0.430958\n",
      "857 Training accuracy = 0.91 Loss = 0.300741\n",
      "858 Training accuracy = 0.88 Loss = 0.492628\n",
      "859 Training accuracy = 0.96 Loss = 0.233839\n",
      "860 Training accuracy = 0.91 Loss = 0.416863\n",
      "861 Training accuracy = 0.9 Loss = 0.331583\n",
      "862 Training accuracy = 0.88 Loss = 0.312193\n",
      "863 Training accuracy = 0.91 Loss = 0.291982\n",
      "864 Training accuracy = 0.91 Loss = 0.349138\n",
      "865 Training accuracy = 0.88 Loss = 0.399063\n",
      "866 Training accuracy = 0.87 Loss = 0.318828\n",
      "867 Training accuracy = 0.89 Loss = 0.39548\n",
      "868 Training accuracy = 0.86 Loss = 0.389957\n",
      "869 Training accuracy = 0.82 Loss = 0.469932\n",
      "870 Training accuracy = 0.93 Loss = 0.23379\n",
      "871 Training accuracy = 0.88 Loss = 0.317726\n",
      "872 Training accuracy = 0.9 Loss = 0.411162\n",
      "873 Training accuracy = 0.89 Loss = 0.391842\n",
      "874 Training accuracy = 0.86 Loss = 0.483837\n",
      "875 Training accuracy = 0.91 Loss = 0.326934\n",
      "876 Training accuracy = 0.87 Loss = 0.387196\n",
      "877 Training accuracy = 0.95 Loss = 0.37628\n",
      "878 Training accuracy = 0.91 Loss = 0.345474\n",
      "879 Training accuracy = 0.96 Loss = 0.260016\n",
      "880 Training accuracy = 0.87 Loss = 0.385075\n",
      "881 Training accuracy = 0.92 Loss = 0.320724\n",
      "882 Training accuracy = 0.93 Loss = 0.380195\n",
      "883 Training accuracy = 0.83 Loss = 0.523966\n",
      "884 Training accuracy = 0.82 Loss = 0.457537\n",
      "885 Training accuracy = 0.95 Loss = 0.32442\n",
      "886 Training accuracy = 0.94 Loss = 0.250735\n",
      "887 Training accuracy = 0.92 Loss = 0.30117\n",
      "888 Training accuracy = 0.94 Loss = 0.311433\n",
      "889 Training accuracy = 0.91 Loss = 0.393353\n",
      "890 Training accuracy = 0.95 Loss = 0.238364\n",
      "891 Training accuracy = 0.85 Loss = 0.46106\n",
      "892 Training accuracy = 0.93 Loss = 0.267823\n",
      "893 Training accuracy = 0.91 Loss = 0.349827\n",
      "894 Training accuracy = 0.85 Loss = 0.460002\n",
      "895 Training accuracy = 0.93 Loss = 0.307991\n",
      "896 Training accuracy = 0.94 Loss = 0.25193\n",
      "897 Training accuracy = 0.93 Loss = 0.378352\n",
      "898 Training accuracy = 0.95 Loss = 0.187675\n",
      "899 Training accuracy = 0.94 Loss = 0.247959\n",
      "900 Training accuracy = 0.91 Loss = 0.497031\n",
      "901 Training accuracy = 0.87 Loss = 0.438278\n",
      "902 Training accuracy = 0.92 Loss = 0.291952\n",
      "903 Training accuracy = 0.91 Loss = 0.379096\n",
      "904 Training accuracy = 0.93 Loss = 0.321871\n",
      "905 Training accuracy = 0.97 Loss = 0.245099\n",
      "906 Training accuracy = 0.89 Loss = 0.493302\n",
      "907 Training accuracy = 0.9 Loss = 0.295907\n",
      "908 Training accuracy = 0.9 Loss = 0.48228\n",
      "909 Training accuracy = 0.89 Loss = 0.378223\n",
      "910 Training accuracy = 0.9 Loss = 0.334484\n",
      "911 Training accuracy = 0.88 Loss = 0.484002\n",
      "912 Training accuracy = 0.93 Loss = 0.360559\n",
      "913 Training accuracy = 0.93 Loss = 0.214294\n",
      "914 Training accuracy = 0.92 Loss = 0.365054\n",
      "915 Training accuracy = 0.84 Loss = 0.529885\n",
      "916 Training accuracy = 0.84 Loss = 0.571816\n",
      "917 Training accuracy = 0.94 Loss = 0.349914\n",
      "918 Training accuracy = 0.95 Loss = 0.284896\n",
      "919 Training accuracy = 0.88 Loss = 0.366831\n",
      "920 Training accuracy = 0.91 Loss = 0.269644\n",
      "921 Training accuracy = 0.91 Loss = 0.37334\n",
      "922 Training accuracy = 0.88 Loss = 0.414554\n",
      "923 Training accuracy = 0.88 Loss = 0.394212\n",
      "924 Training accuracy = 0.87 Loss = 0.473884\n",
      "925 Training accuracy = 0.95 Loss = 0.218806\n",
      "926 Training accuracy = 0.92 Loss = 0.370041\n",
      "927 Training accuracy = 0.96 Loss = 0.223544\n",
      "928 Training accuracy = 0.94 Loss = 0.369912\n",
      "929 Training accuracy = 0.94 Loss = 0.328055\n",
      "930 Training accuracy = 0.88 Loss = 0.367993\n",
      "931 Training accuracy = 0.9 Loss = 0.336629\n",
      "932 Training accuracy = 0.96 Loss = 0.216232\n",
      "933 Training accuracy = 0.88 Loss = 0.406973\n",
      "934 Training accuracy = 0.95 Loss = 0.287205\n",
      "935 Training accuracy = 0.89 Loss = 0.335733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936 Training accuracy = 0.9 Loss = 0.354312\n",
      "937 Training accuracy = 0.92 Loss = 0.308548\n",
      "938 Training accuracy = 0.91 Loss = 0.295736\n",
      "939 Training accuracy = 0.94 Loss = 0.350266\n",
      "940 Training accuracy = 0.92 Loss = 0.304334\n",
      "941 Training accuracy = 0.95 Loss = 0.225079\n",
      "942 Training accuracy = 0.88 Loss = 0.347614\n",
      "943 Training accuracy = 0.91 Loss = 0.328262\n",
      "944 Training accuracy = 0.93 Loss = 0.258846\n",
      "945 Training accuracy = 0.88 Loss = 0.355656\n",
      "946 Training accuracy = 0.9 Loss = 0.369626\n",
      "947 Training accuracy = 0.85 Loss = 0.338572\n",
      "948 Training accuracy = 0.93 Loss = 0.26601\n",
      "949 Training accuracy = 0.87 Loss = 0.420484\n",
      "950 Training accuracy = 0.92 Loss = 0.414815\n",
      "951 Training accuracy = 0.86 Loss = 0.576923\n",
      "952 Training accuracy = 0.91 Loss = 0.363856\n",
      "953 Training accuracy = 0.9 Loss = 0.315659\n",
      "954 Training accuracy = 0.91 Loss = 0.325032\n",
      "955 Training accuracy = 0.89 Loss = 0.371791\n",
      "956 Training accuracy = 0.91 Loss = 0.338141\n",
      "957 Training accuracy = 0.92 Loss = 0.298264\n",
      "958 Training accuracy = 0.92 Loss = 0.293909\n",
      "959 Training accuracy = 0.92 Loss = 0.262767\n",
      "960 Training accuracy = 0.91 Loss = 0.4582\n",
      "961 Training accuracy = 0.93 Loss = 0.34149\n",
      "962 Training accuracy = 0.88 Loss = 0.351751\n",
      "963 Training accuracy = 0.88 Loss = 0.41628\n",
      "964 Training accuracy = 0.92 Loss = 0.319619\n",
      "965 Training accuracy = 0.95 Loss = 0.248501\n",
      "966 Training accuracy = 0.92 Loss = 0.382834\n",
      "967 Training accuracy = 0.94 Loss = 0.271411\n",
      "968 Training accuracy = 0.89 Loss = 0.335987\n",
      "969 Training accuracy = 0.93 Loss = 0.227252\n",
      "970 Training accuracy = 0.93 Loss = 0.345846\n",
      "971 Training accuracy = 0.89 Loss = 0.400611\n",
      "972 Training accuracy = 0.86 Loss = 0.491172\n",
      "973 Training accuracy = 0.88 Loss = 0.327216\n",
      "974 Training accuracy = 0.92 Loss = 0.278641\n",
      "975 Training accuracy = 0.9 Loss = 0.308904\n",
      "976 Training accuracy = 0.91 Loss = 0.350193\n",
      "977 Training accuracy = 0.88 Loss = 0.400439\n",
      "978 Training accuracy = 0.89 Loss = 0.406224\n",
      "979 Training accuracy = 0.87 Loss = 0.412341\n",
      "980 Training accuracy = 0.88 Loss = 0.357058\n",
      "981 Training accuracy = 0.91 Loss = 0.339269\n",
      "982 Training accuracy = 0.93 Loss = 0.253287\n",
      "983 Training accuracy = 0.91 Loss = 0.407441\n",
      "984 Training accuracy = 0.9 Loss = 0.370812\n",
      "985 Training accuracy = 0.91 Loss = 0.342924\n",
      "986 Training accuracy = 0.94 Loss = 0.384599\n",
      "987 Training accuracy = 0.91 Loss = 0.33365\n",
      "988 Training accuracy = 0.89 Loss = 0.310492\n",
      "989 Training accuracy = 0.9 Loss = 0.390005\n",
      "990 Training accuracy = 0.86 Loss = 0.376157\n",
      "991 Training accuracy = 0.89 Loss = 0.311077\n",
      "992 Training accuracy = 0.94 Loss = 0.241648\n",
      "993 Training accuracy = 0.93 Loss = 0.235995\n",
      "994 Training accuracy = 0.87 Loss = 0.369336\n",
      "995 Training accuracy = 0.92 Loss = 0.400172\n",
      "996 Training accuracy = 0.92 Loss = 0.270774\n",
      "997 Training accuracy = 0.89 Loss = 0.42006\n",
      "998 Training accuracy = 0.92 Loss = 0.304563\n",
      "999 Training accuracy = 0.82 Loss = 0.452287\n",
      "1000 Training accuracy = 0.93 Loss = 0.37183\n",
      "1001 Training accuracy = 0.9 Loss = 0.314763\n",
      "1002 Training accuracy = 0.95 Loss = 0.264704\n",
      "1003 Training accuracy = 0.9 Loss = 0.30531\n",
      "1004 Training accuracy = 0.93 Loss = 0.429284\n",
      "1005 Training accuracy = 0.91 Loss = 0.310937\n",
      "1006 Training accuracy = 0.89 Loss = 0.360288\n",
      "1007 Training accuracy = 0.93 Loss = 0.310772\n",
      "1008 Training accuracy = 0.89 Loss = 0.446152\n",
      "1009 Training accuracy = 0.85 Loss = 0.549695\n",
      "1010 Training accuracy = 0.91 Loss = 0.290019\n",
      "1011 Training accuracy = 0.92 Loss = 0.270613\n",
      "1012 Training accuracy = 0.92 Loss = 0.357946\n",
      "1013 Training accuracy = 0.96 Loss = 0.274385\n",
      "1014 Training accuracy = 0.87 Loss = 0.464373\n",
      "1015 Training accuracy = 0.83 Loss = 0.449412\n",
      "1016 Training accuracy = 0.87 Loss = 0.395331\n",
      "1017 Training accuracy = 0.88 Loss = 0.368555\n",
      "1018 Training accuracy = 0.87 Loss = 0.442413\n",
      "1019 Training accuracy = 0.89 Loss = 0.40387\n",
      "1020 Training accuracy = 0.9 Loss = 0.412416\n",
      "1021 Training accuracy = 0.89 Loss = 0.429269\n",
      "1022 Training accuracy = 0.92 Loss = 0.281072\n",
      "1023 Training accuracy = 0.92 Loss = 0.277559\n",
      "1024 Training accuracy = 0.88 Loss = 0.436664\n",
      "1025 Training accuracy = 0.9 Loss = 0.286293\n",
      "1026 Training accuracy = 0.89 Loss = 0.498467\n",
      "1027 Training accuracy = 0.95 Loss = 0.259545\n",
      "1028 Training accuracy = 0.91 Loss = 0.370327\n",
      "1029 Training accuracy = 0.93 Loss = 0.31492\n",
      "1030 Training accuracy = 0.87 Loss = 0.389549\n",
      "1031 Training accuracy = 0.86 Loss = 0.539861\n",
      "1032 Training accuracy = 0.89 Loss = 0.466005\n",
      "1033 Training accuracy = 0.95 Loss = 0.252894\n",
      "1034 Training accuracy = 0.94 Loss = 0.413265\n",
      "1035 Training accuracy = 0.9 Loss = 0.438916\n",
      "1036 Training accuracy = 0.91 Loss = 0.322202\n",
      "1037 Training accuracy = 0.9 Loss = 0.301321\n",
      "1038 Training accuracy = 0.92 Loss = 0.335467\n",
      "1039 Training accuracy = 0.88 Loss = 0.538486\n",
      "1040 Training accuracy = 0.86 Loss = 0.506599\n",
      "1041 Training accuracy = 0.93 Loss = 0.224017\n",
      "1042 Training accuracy = 0.94 Loss = 0.25097\n",
      "1043 Training accuracy = 0.91 Loss = 0.305798\n",
      "1044 Training accuracy = 0.92 Loss = 0.236903\n",
      "1045 Training accuracy = 0.94 Loss = 0.267464\n",
      "1046 Training accuracy = 0.91 Loss = 0.344064\n",
      "1047 Training accuracy = 0.95 Loss = 0.227813\n",
      "1048 Training accuracy = 0.94 Loss = 0.263806\n",
      "1049 Training accuracy = 0.88 Loss = 0.371949\n",
      "1050 Training accuracy = 0.95 Loss = 0.284526\n",
      "1051 Training accuracy = 0.89 Loss = 0.314551\n",
      "1052 Training accuracy = 0.93 Loss = 0.308392\n",
      "1053 Training accuracy = 0.9 Loss = 0.372324\n",
      "1054 Training accuracy = 0.93 Loss = 0.352221\n",
      "1055 Training accuracy = 0.92 Loss = 0.253061\n",
      "1056 Training accuracy = 0.85 Loss = 0.455237\n",
      "1057 Training accuracy = 0.92 Loss = 0.269531\n",
      "1058 Training accuracy = 0.9 Loss = 0.326908\n",
      "1059 Training accuracy = 0.91 Loss = 0.29424\n",
      "1060 Training accuracy = 0.9 Loss = 0.293554\n",
      "1061 Training accuracy = 0.94 Loss = 0.210685\n",
      "1062 Training accuracy = 0.88 Loss = 0.454542\n",
      "1063 Training accuracy = 0.93 Loss = 0.253058\n",
      "1064 Training accuracy = 0.89 Loss = 0.296684\n",
      "1065 Training accuracy = 0.92 Loss = 0.283603\n",
      "1066 Training accuracy = 0.93 Loss = 0.293187\n",
      "1067 Training accuracy = 0.9 Loss = 0.349034\n",
      "1068 Training accuracy = 0.94 Loss = 0.255588\n",
      "1069 Training accuracy = 0.89 Loss = 0.37945\n",
      "1070 Training accuracy = 0.86 Loss = 0.456498\n",
      "1071 Training accuracy = 0.89 Loss = 0.356002\n",
      "1072 Training accuracy = 0.89 Loss = 0.291315\n",
      "1073 Training accuracy = 0.86 Loss = 0.437061\n",
      "1074 Training accuracy = 0.96 Loss = 0.251979\n",
      "1075 Training accuracy = 0.93 Loss = 0.315608\n",
      "1076 Training accuracy = 0.92 Loss = 0.258704\n",
      "1077 Training accuracy = 0.88 Loss = 0.411747\n",
      "1078 Training accuracy = 0.95 Loss = 0.291965\n",
      "1079 Training accuracy = 0.94 Loss = 0.258222\n",
      "1080 Training accuracy = 0.89 Loss = 0.378138\n",
      "1081 Training accuracy = 0.91 Loss = 0.361546\n",
      "1082 Training accuracy = 0.88 Loss = 0.399941\n",
      "1083 Training accuracy = 0.91 Loss = 0.272996\n",
      "1084 Training accuracy = 0.89 Loss = 0.283519\n",
      "1085 Training accuracy = 0.88 Loss = 0.338455\n",
      "1086 Training accuracy = 0.88 Loss = 0.339732\n",
      "1087 Training accuracy = 0.88 Loss = 0.395119\n",
      "1088 Training accuracy = 0.93 Loss = 0.316266\n",
      "1089 Training accuracy = 0.88 Loss = 0.352615\n",
      "1090 Training accuracy = 0.87 Loss = 0.394994\n",
      "1091 Training accuracy = 0.95 Loss = 0.200374\n",
      "1092 Training accuracy = 0.9 Loss = 0.313172\n",
      "1093 Training accuracy = 0.87 Loss = 0.360972\n",
      "1094 Training accuracy = 0.89 Loss = 0.337585\n",
      "1095 Training accuracy = 0.93 Loss = 0.259739\n",
      "1096 Training accuracy = 0.89 Loss = 0.40293\n",
      "1097 Training accuracy = 0.93 Loss = 0.271325\n",
      "1098 Training accuracy = 0.91 Loss = 0.282381\n",
      "1099 Training accuracy = 0.88 Loss = 0.423738\n",
      "1100 Training accuracy = 0.96 Loss = 0.246257\n",
      "1101 Training accuracy = 0.91 Loss = 0.273437\n",
      "1102 Training accuracy = 0.91 Loss = 0.360654\n",
      "1103 Training accuracy = 0.86 Loss = 0.403611\n",
      "1104 Training accuracy = 0.93 Loss = 0.224404\n",
      "1105 Training accuracy = 0.91 Loss = 0.301154\n",
      "1106 Training accuracy = 0.9 Loss = 0.290869\n",
      "1107 Training accuracy = 0.89 Loss = 0.336248\n",
      "1108 Training accuracy = 0.91 Loss = 0.286512\n",
      "1109 Training accuracy = 0.92 Loss = 0.310186\n",
      "1110 Training accuracy = 0.92 Loss = 0.273059\n",
      "1111 Training accuracy = 0.93 Loss = 0.232496\n",
      "1112 Training accuracy = 0.93 Loss = 0.304008\n",
      "1113 Training accuracy = 0.9 Loss = 0.296462\n",
      "1114 Training accuracy = 0.9 Loss = 0.300579\n",
      "1115 Training accuracy = 0.91 Loss = 0.382922\n",
      "1116 Training accuracy = 0.89 Loss = 0.436497\n",
      "1117 Training accuracy = 0.89 Loss = 0.375701\n",
      "1118 Training accuracy = 0.91 Loss = 0.38072\n",
      "1119 Training accuracy = 0.9 Loss = 0.364467\n",
      "1120 Training accuracy = 0.91 Loss = 0.329617\n",
      "1121 Training accuracy = 0.92 Loss = 0.353729\n",
      "1122 Training accuracy = 0.91 Loss = 0.294811\n",
      "1123 Training accuracy = 0.88 Loss = 0.368887\n",
      "1124 Training accuracy = 0.92 Loss = 0.323658\n",
      "1125 Training accuracy = 0.86 Loss = 0.479391\n",
      "1126 Training accuracy = 0.89 Loss = 0.36358\n",
      "1127 Training accuracy = 0.94 Loss = 0.207425\n",
      "1128 Training accuracy = 0.93 Loss = 0.334607\n",
      "1129 Training accuracy = 0.9 Loss = 0.487534\n",
      "1130 Training accuracy = 0.91 Loss = 0.318661\n",
      "1131 Training accuracy = 0.92 Loss = 0.31673\n",
      "1132 Training accuracy = 0.9 Loss = 0.250809\n",
      "1133 Training accuracy = 0.89 Loss = 0.301056\n",
      "1134 Training accuracy = 0.86 Loss = 0.429076\n",
      "1135 Training accuracy = 0.93 Loss = 0.276394\n",
      "1136 Training accuracy = 0.87 Loss = 0.374887\n",
      "1137 Training accuracy = 0.89 Loss = 0.431625\n",
      "1138 Training accuracy = 0.89 Loss = 0.443385\n",
      "1139 Training accuracy = 0.9 Loss = 0.261187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 Training accuracy = 0.93 Loss = 0.248103\n",
      "1141 Training accuracy = 0.91 Loss = 0.43971\n",
      "1142 Training accuracy = 0.91 Loss = 0.374253\n",
      "1143 Training accuracy = 0.89 Loss = 0.314697\n",
      "1144 Training accuracy = 0.92 Loss = 0.237554\n",
      "1145 Training accuracy = 0.89 Loss = 0.364946\n",
      "1146 Training accuracy = 0.92 Loss = 0.3172\n",
      "1147 Training accuracy = 0.92 Loss = 0.348573\n",
      "1148 Training accuracy = 0.93 Loss = 0.245525\n",
      "1149 Training accuracy = 0.93 Loss = 0.306034\n",
      "1150 Training accuracy = 0.92 Loss = 0.30902\n",
      "1151 Training accuracy = 0.95 Loss = 0.245805\n",
      "1152 Training accuracy = 0.89 Loss = 0.351967\n",
      "1153 Training accuracy = 0.96 Loss = 0.184889\n",
      "1154 Training accuracy = 0.89 Loss = 0.387163\n",
      "1155 Training accuracy = 0.82 Loss = 0.442991\n",
      "1156 Training accuracy = 0.89 Loss = 0.293025\n",
      "1157 Training accuracy = 0.94 Loss = 0.274869\n",
      "1158 Training accuracy = 0.86 Loss = 0.415793\n",
      "1159 Training accuracy = 0.93 Loss = 0.313676\n",
      "1160 Training accuracy = 0.95 Loss = 0.261728\n",
      "1161 Training accuracy = 0.93 Loss = 0.370568\n",
      "1162 Training accuracy = 0.95 Loss = 0.213177\n",
      "1163 Training accuracy = 0.92 Loss = 0.332412\n",
      "1164 Training accuracy = 0.89 Loss = 0.461258\n",
      "1165 Training accuracy = 0.93 Loss = 0.257591\n",
      "1166 Training accuracy = 0.93 Loss = 0.251617\n",
      "1167 Training accuracy = 0.97 Loss = 0.234752\n",
      "1168 Training accuracy = 0.94 Loss = 0.231404\n",
      "1169 Training accuracy = 0.94 Loss = 0.254374\n",
      "1170 Training accuracy = 0.88 Loss = 0.401249\n",
      "1171 Training accuracy = 0.85 Loss = 0.499595\n",
      "1172 Training accuracy = 0.96 Loss = 0.211908\n",
      "1173 Training accuracy = 0.95 Loss = 0.304347\n",
      "1174 Training accuracy = 0.92 Loss = 0.271722\n",
      "1175 Training accuracy = 0.9 Loss = 0.295935\n",
      "1176 Training accuracy = 0.96 Loss = 0.228752\n",
      "1177 Training accuracy = 0.87 Loss = 0.316175\n",
      "1178 Training accuracy = 0.95 Loss = 0.220085\n",
      "1179 Training accuracy = 0.93 Loss = 0.322387\n",
      "1180 Training accuracy = 0.94 Loss = 0.225868\n",
      "1181 Training accuracy = 0.89 Loss = 0.38565\n",
      "1182 Training accuracy = 0.92 Loss = 0.293074\n",
      "1183 Training accuracy = 0.91 Loss = 0.346367\n",
      "1184 Training accuracy = 0.9 Loss = 0.367349\n",
      "1185 Training accuracy = 0.91 Loss = 0.333792\n",
      "1186 Training accuracy = 0.89 Loss = 0.460614\n",
      "1187 Training accuracy = 0.95 Loss = 0.297861\n",
      "1188 Training accuracy = 0.91 Loss = 0.468462\n",
      "1189 Training accuracy = 0.93 Loss = 0.317436\n",
      "1190 Training accuracy = 0.9 Loss = 0.329922\n",
      "1191 Training accuracy = 0.91 Loss = 0.270351\n",
      "1192 Training accuracy = 0.92 Loss = 0.349412\n",
      "1193 Training accuracy = 0.93 Loss = 0.273923\n",
      "1194 Training accuracy = 0.9 Loss = 0.318477\n",
      "1195 Training accuracy = 0.89 Loss = 0.341185\n",
      "1196 Training accuracy = 0.93 Loss = 0.276439\n",
      "1197 Training accuracy = 0.9 Loss = 0.280115\n",
      "1198 Training accuracy = 0.91 Loss = 0.239298\n",
      "1199 Training accuracy = 0.89 Loss = 0.335365\n",
      "1200 Training accuracy = 0.89 Loss = 0.310807\n",
      "1201 Training accuracy = 0.93 Loss = 0.22153\n",
      "1202 Training accuracy = 0.92 Loss = 0.215782\n",
      "1203 Training accuracy = 0.91 Loss = 0.299044\n",
      "1204 Training accuracy = 0.91 Loss = 0.287208\n",
      "1205 Training accuracy = 0.83 Loss = 0.475759\n",
      "1206 Training accuracy = 0.9 Loss = 0.342595\n",
      "1207 Training accuracy = 0.96 Loss = 0.254692\n",
      "1208 Training accuracy = 0.91 Loss = 0.367151\n",
      "1209 Training accuracy = 0.86 Loss = 0.488892\n",
      "1210 Training accuracy = 0.89 Loss = 0.313078\n",
      "1211 Training accuracy = 0.89 Loss = 0.360415\n",
      "1212 Training accuracy = 0.9 Loss = 0.308315\n",
      "1213 Training accuracy = 0.92 Loss = 0.291965\n",
      "1214 Training accuracy = 0.89 Loss = 0.364991\n",
      "1215 Training accuracy = 0.9 Loss = 0.304275\n",
      "1216 Training accuracy = 0.95 Loss = 0.244968\n",
      "1217 Training accuracy = 0.91 Loss = 0.368326\n",
      "1218 Training accuracy = 0.93 Loss = 0.280723\n",
      "1219 Training accuracy = 0.92 Loss = 0.356028\n",
      "1220 Training accuracy = 0.87 Loss = 0.368168\n",
      "1221 Training accuracy = 0.9 Loss = 0.415145\n",
      "1222 Training accuracy = 0.92 Loss = 0.32568\n",
      "1223 Training accuracy = 0.91 Loss = 0.354826\n",
      "1224 Training accuracy = 0.91 Loss = 0.337044\n",
      "1225 Training accuracy = 0.88 Loss = 0.359808\n",
      "1226 Training accuracy = 0.94 Loss = 0.197951\n",
      "1227 Training accuracy = 0.93 Loss = 0.340147\n",
      "1228 Training accuracy = 0.95 Loss = 0.289533\n",
      "1229 Training accuracy = 0.91 Loss = 0.339492\n",
      "1230 Training accuracy = 0.86 Loss = 0.447554\n",
      "1231 Training accuracy = 0.88 Loss = 0.603606\n",
      "1232 Training accuracy = 0.9 Loss = 0.277188\n",
      "1233 Training accuracy = 0.89 Loss = 0.339234\n",
      "1234 Training accuracy = 0.93 Loss = 0.270818\n",
      "1235 Training accuracy = 0.83 Loss = 0.513104\n",
      "1236 Training accuracy = 0.95 Loss = 0.265934\n",
      "1237 Training accuracy = 0.92 Loss = 0.350117\n",
      "1238 Training accuracy = 0.89 Loss = 0.298529\n",
      "1239 Training accuracy = 0.93 Loss = 0.385788\n",
      "1240 Training accuracy = 0.9 Loss = 0.2662\n",
      "1241 Training accuracy = 0.92 Loss = 0.290109\n",
      "1242 Training accuracy = 0.9 Loss = 0.329628\n",
      "1243 Training accuracy = 0.87 Loss = 0.385744\n",
      "1244 Training accuracy = 0.92 Loss = 0.423661\n",
      "1245 Training accuracy = 0.86 Loss = 0.468699\n",
      "1246 Training accuracy = 0.91 Loss = 0.375106\n",
      "1247 Training accuracy = 0.85 Loss = 0.34676\n",
      "1248 Training accuracy = 0.87 Loss = 0.328243\n",
      "1249 Training accuracy = 0.93 Loss = 0.313219\n",
      "1250 Training accuracy = 0.9 Loss = 0.301742\n",
      "1251 Training accuracy = 0.92 Loss = 0.276821\n",
      "1252 Training accuracy = 0.96 Loss = 0.19214\n",
      "1253 Training accuracy = 0.9 Loss = 0.357449\n",
      "1254 Training accuracy = 0.9 Loss = 0.290421\n",
      "1255 Training accuracy = 0.92 Loss = 0.293405\n",
      "1256 Training accuracy = 0.94 Loss = 0.25432\n",
      "1257 Training accuracy = 0.91 Loss = 0.265603\n",
      "1258 Training accuracy = 0.9 Loss = 0.462082\n",
      "1259 Training accuracy = 0.9 Loss = 0.375712\n",
      "1260 Training accuracy = 0.91 Loss = 0.354893\n",
      "1261 Training accuracy = 0.91 Loss = 0.27652\n",
      "1262 Training accuracy = 0.94 Loss = 0.27149\n",
      "1263 Training accuracy = 0.95 Loss = 0.167083\n",
      "1264 Training accuracy = 0.96 Loss = 0.255319\n",
      "1265 Training accuracy = 0.94 Loss = 0.203659\n",
      "1266 Training accuracy = 0.93 Loss = 0.265297\n",
      "1267 Training accuracy = 0.92 Loss = 0.238958\n",
      "1268 Training accuracy = 0.9 Loss = 0.386626\n",
      "1269 Training accuracy = 0.88 Loss = 0.334164\n",
      "1270 Training accuracy = 0.89 Loss = 0.397058\n",
      "1271 Training accuracy = 0.95 Loss = 0.221805\n",
      "1272 Training accuracy = 0.9 Loss = 0.344937\n",
      "1273 Training accuracy = 0.88 Loss = 0.322341\n",
      "1274 Training accuracy = 0.94 Loss = 0.308411\n",
      "1275 Training accuracy = 0.93 Loss = 0.294755\n",
      "1276 Training accuracy = 0.9 Loss = 0.275131\n",
      "1277 Training accuracy = 0.94 Loss = 0.198213\n",
      "1278 Training accuracy = 0.95 Loss = 0.239448\n",
      "1279 Training accuracy = 0.9 Loss = 0.357965\n",
      "1280 Training accuracy = 0.92 Loss = 0.231395\n",
      "1281 Training accuracy = 0.91 Loss = 0.266449\n",
      "1282 Training accuracy = 0.86 Loss = 0.375327\n",
      "1283 Training accuracy = 0.92 Loss = 0.341812\n",
      "1284 Training accuracy = 0.93 Loss = 0.317214\n",
      "1285 Training accuracy = 0.93 Loss = 0.324449\n",
      "1286 Training accuracy = 0.87 Loss = 0.37261\n",
      "1287 Training accuracy = 0.96 Loss = 0.240448\n",
      "1288 Training accuracy = 0.89 Loss = 0.325632\n",
      "1289 Training accuracy = 0.91 Loss = 0.383909\n",
      "1290 Training accuracy = 0.83 Loss = 0.583999\n",
      "1291 Training accuracy = 0.91 Loss = 0.210846\n",
      "1292 Training accuracy = 0.9 Loss = 0.284259\n",
      "1293 Training accuracy = 0.89 Loss = 0.37572\n",
      "1294 Training accuracy = 0.89 Loss = 0.362106\n",
      "1295 Training accuracy = 0.9 Loss = 0.386836\n",
      "1296 Training accuracy = 0.96 Loss = 0.193637\n",
      "1297 Training accuracy = 0.92 Loss = 0.269343\n",
      "1298 Training accuracy = 0.93 Loss = 0.286091\n",
      "1299 Training accuracy = 0.94 Loss = 0.24738\n",
      "1300 Training accuracy = 0.94 Loss = 0.272166\n",
      "1301 Training accuracy = 0.9 Loss = 0.364321\n",
      "1302 Training accuracy = 0.89 Loss = 0.440313\n",
      "1303 Training accuracy = 0.92 Loss = 0.296798\n",
      "1304 Training accuracy = 0.96 Loss = 0.175254\n",
      "1305 Training accuracy = 0.87 Loss = 0.306113\n",
      "1306 Training accuracy = 0.95 Loss = 0.216892\n",
      "1307 Training accuracy = 0.89 Loss = 0.388871\n",
      "1308 Training accuracy = 0.94 Loss = 0.25972\n",
      "1309 Training accuracy = 0.91 Loss = 0.342774\n",
      "1310 Training accuracy = 0.92 Loss = 0.29928\n",
      "1311 Training accuracy = 0.91 Loss = 0.43708\n",
      "1312 Training accuracy = 0.86 Loss = 0.366115\n",
      "1313 Training accuracy = 0.9 Loss = 0.372878\n",
      "1314 Training accuracy = 0.87 Loss = 0.527198\n",
      "1315 Training accuracy = 0.85 Loss = 0.413361\n",
      "1316 Training accuracy = 0.93 Loss = 0.219947\n",
      "1317 Training accuracy = 0.89 Loss = 0.329595\n",
      "1318 Training accuracy = 0.91 Loss = 0.331804\n",
      "1319 Training accuracy = 0.94 Loss = 0.294675\n",
      "1320 Training accuracy = 0.92 Loss = 0.330003\n",
      "1321 Training accuracy = 0.91 Loss = 0.288848\n",
      "1322 Training accuracy = 0.92 Loss = 0.28672\n",
      "1323 Training accuracy = 0.92 Loss = 0.313212\n",
      "1324 Training accuracy = 0.88 Loss = 0.44728\n",
      "1325 Training accuracy = 0.93 Loss = 0.353281\n",
      "1326 Training accuracy = 0.9 Loss = 0.315191\n",
      "1327 Training accuracy = 0.9 Loss = 0.324237\n",
      "1328 Training accuracy = 0.92 Loss = 0.361876\n",
      "1329 Training accuracy = 0.89 Loss = 0.356238\n",
      "1330 Training accuracy = 0.88 Loss = 0.440777\n",
      "1331 Training accuracy = 0.89 Loss = 0.395138\n",
      "1332 Training accuracy = 0.9 Loss = 0.333522\n",
      "1333 Training accuracy = 0.92 Loss = 0.36299\n",
      "1334 Training accuracy = 0.9 Loss = 0.291037\n",
      "1335 Training accuracy = 0.96 Loss = 0.279752\n",
      "1336 Training accuracy = 0.92 Loss = 0.259994\n",
      "1337 Training accuracy = 0.9 Loss = 0.410627\n",
      "1338 Training accuracy = 0.95 Loss = 0.223291\n",
      "1339 Training accuracy = 0.98 Loss = 0.158055\n",
      "1340 Training accuracy = 0.91 Loss = 0.312876\n",
      "1341 Training accuracy = 0.91 Loss = 0.311222\n",
      "1342 Training accuracy = 0.89 Loss = 0.490206\n",
      "1343 Training accuracy = 0.95 Loss = 0.277097\n",
      "1344 Training accuracy = 0.89 Loss = 0.392525\n",
      "1345 Training accuracy = 0.97 Loss = 0.175552\n",
      "1346 Training accuracy = 0.92 Loss = 0.264785\n",
      "1347 Training accuracy = 0.92 Loss = 0.301429\n",
      "1348 Training accuracy = 0.86 Loss = 0.407779\n",
      "1349 Training accuracy = 0.96 Loss = 0.218469\n",
      "1350 Training accuracy = 0.93 Loss = 0.271399\n",
      "1351 Training accuracy = 0.95 Loss = 0.232505\n",
      "1352 Training accuracy = 0.9 Loss = 0.344522\n",
      "1353 Training accuracy = 0.93 Loss = 0.259654\n",
      "1354 Training accuracy = 0.92 Loss = 0.235252\n",
      "1355 Training accuracy = 0.91 Loss = 0.320721\n",
      "1356 Training accuracy = 0.89 Loss = 0.348633\n",
      "1357 Training accuracy = 0.88 Loss = 0.467931\n",
      "1358 Training accuracy = 0.94 Loss = 0.273758\n",
      "1359 Training accuracy = 0.9 Loss = 0.366578\n",
      "1360 Training accuracy = 0.89 Loss = 0.383703\n",
      "1361 Training accuracy = 0.9 Loss = 0.401577\n",
      "1362 Training accuracy = 0.91 Loss = 0.280805\n",
      "1363 Training accuracy = 0.94 Loss = 0.269697\n",
      "1364 Training accuracy = 0.92 Loss = 0.293391\n",
      "1365 Training accuracy = 0.92 Loss = 0.296412\n",
      "1366 Training accuracy = 0.87 Loss = 0.456339\n",
      "1367 Training accuracy = 0.87 Loss = 0.369164\n",
      "1368 Training accuracy = 0.88 Loss = 0.349559\n",
      "1369 Training accuracy = 0.91 Loss = 0.286122\n",
      "1370 Training accuracy = 0.92 Loss = 0.315725\n",
      "1371 Training accuracy = 0.92 Loss = 0.253428\n",
      "1372 Training accuracy = 0.9 Loss = 0.482474\n",
      "1373 Training accuracy = 0.92 Loss = 0.362447\n",
      "1374 Training accuracy = 0.9 Loss = 0.279042\n",
      "1375 Training accuracy = 0.88 Loss = 0.484324\n",
      "1376 Training accuracy = 0.94 Loss = 0.35157\n",
      "1377 Training accuracy = 0.93 Loss = 0.266586\n",
      "1378 Training accuracy = 0.91 Loss = 0.316312\n",
      "1379 Training accuracy = 0.93 Loss = 0.204152\n",
      "1380 Training accuracy = 0.97 Loss = 0.141888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1381 Training accuracy = 0.89 Loss = 0.343195\n",
      "1382 Training accuracy = 0.92 Loss = 0.316847\n",
      "1383 Training accuracy = 0.97 Loss = 0.190778\n",
      "1384 Training accuracy = 0.91 Loss = 0.39438\n",
      "1385 Training accuracy = 0.96 Loss = 0.235893\n",
      "1386 Training accuracy = 0.94 Loss = 0.363945\n",
      "1387 Training accuracy = 0.85 Loss = 0.369044\n",
      "1388 Training accuracy = 0.89 Loss = 0.41907\n",
      "1389 Training accuracy = 0.91 Loss = 0.333746\n",
      "1390 Training accuracy = 0.88 Loss = 0.324805\n",
      "1391 Training accuracy = 0.97 Loss = 0.158316\n",
      "1392 Training accuracy = 0.95 Loss = 0.281829\n",
      "1393 Training accuracy = 0.97 Loss = 0.164248\n",
      "1394 Training accuracy = 0.87 Loss = 0.403209\n",
      "1395 Training accuracy = 0.92 Loss = 0.352933\n",
      "1396 Training accuracy = 0.9 Loss = 0.545357\n",
      "1397 Training accuracy = 0.93 Loss = 0.331095\n",
      "1398 Training accuracy = 0.91 Loss = 0.270654\n",
      "1399 Training accuracy = 0.97 Loss = 0.207758\n",
      "1400 Training accuracy = 0.92 Loss = 0.353853\n",
      "1401 Training accuracy = 0.93 Loss = 0.307703\n",
      "1402 Training accuracy = 0.91 Loss = 0.256022\n",
      "1403 Training accuracy = 0.94 Loss = 0.240354\n",
      "1404 Training accuracy = 0.91 Loss = 0.391427\n",
      "1405 Training accuracy = 0.94 Loss = 0.214113\n",
      "1406 Training accuracy = 0.9 Loss = 0.355066\n",
      "1407 Training accuracy = 0.86 Loss = 0.478056\n",
      "1408 Training accuracy = 0.95 Loss = 0.211037\n",
      "1409 Training accuracy = 0.9 Loss = 0.416322\n",
      "1410 Training accuracy = 0.94 Loss = 0.22744\n",
      "1411 Training accuracy = 0.9 Loss = 0.338259\n",
      "1412 Training accuracy = 0.91 Loss = 0.315001\n",
      "1413 Training accuracy = 0.93 Loss = 0.373343\n",
      "1414 Training accuracy = 0.87 Loss = 0.374126\n",
      "1415 Training accuracy = 0.95 Loss = 0.225001\n",
      "1416 Training accuracy = 0.92 Loss = 0.306411\n",
      "1417 Training accuracy = 0.93 Loss = 0.280997\n",
      "1418 Training accuracy = 0.9 Loss = 0.327124\n",
      "1419 Training accuracy = 0.94 Loss = 0.211608\n",
      "1420 Training accuracy = 0.94 Loss = 0.27879\n",
      "1421 Training accuracy = 0.95 Loss = 0.295506\n",
      "1422 Training accuracy = 0.93 Loss = 0.209378\n",
      "1423 Training accuracy = 0.93 Loss = 0.231638\n",
      "1424 Training accuracy = 0.88 Loss = 0.368887\n",
      "1425 Training accuracy = 0.95 Loss = 0.205789\n",
      "1426 Training accuracy = 0.97 Loss = 0.157395\n",
      "1427 Training accuracy = 0.9 Loss = 0.417957\n",
      "1428 Training accuracy = 0.91 Loss = 0.271074\n",
      "1429 Training accuracy = 0.91 Loss = 0.381844\n",
      "1430 Training accuracy = 0.92 Loss = 0.241269\n",
      "1431 Training accuracy = 0.9 Loss = 0.361276\n",
      "1432 Training accuracy = 0.9 Loss = 0.299681\n",
      "1433 Training accuracy = 0.9 Loss = 0.353175\n",
      "1434 Training accuracy = 0.93 Loss = 0.263825\n",
      "1435 Training accuracy = 0.88 Loss = 0.326361\n",
      "1436 Training accuracy = 0.92 Loss = 0.301373\n",
      "1437 Training accuracy = 0.92 Loss = 0.265957\n",
      "1438 Training accuracy = 0.87 Loss = 0.345629\n",
      "1439 Training accuracy = 0.92 Loss = 0.219493\n",
      "1440 Training accuracy = 0.91 Loss = 0.321437\n",
      "1441 Training accuracy = 0.95 Loss = 0.214583\n",
      "1442 Training accuracy = 0.9 Loss = 0.360357\n",
      "1443 Training accuracy = 0.92 Loss = 0.289086\n",
      "1444 Training accuracy = 0.93 Loss = 0.256031\n",
      "1445 Training accuracy = 0.87 Loss = 0.451346\n",
      "1446 Training accuracy = 0.93 Loss = 0.250926\n",
      "1447 Training accuracy = 0.93 Loss = 0.221975\n",
      "1448 Training accuracy = 0.86 Loss = 0.402511\n",
      "1449 Training accuracy = 0.97 Loss = 0.172972\n",
      "1450 Training accuracy = 0.87 Loss = 0.489569\n",
      "1451 Training accuracy = 0.89 Loss = 0.391127\n",
      "1452 Training accuracy = 0.95 Loss = 0.276425\n",
      "1453 Training accuracy = 0.9 Loss = 0.381734\n",
      "1454 Training accuracy = 0.89 Loss = 0.385662\n",
      "1455 Training accuracy = 0.9 Loss = 0.248524\n",
      "1456 Training accuracy = 0.87 Loss = 0.453983\n",
      "1457 Training accuracy = 0.89 Loss = 0.296303\n",
      "1458 Training accuracy = 0.91 Loss = 0.317338\n",
      "1459 Training accuracy = 0.87 Loss = 0.324773\n",
      "1460 Training accuracy = 0.92 Loss = 0.220243\n",
      "1461 Training accuracy = 0.97 Loss = 0.230651\n",
      "1462 Training accuracy = 0.87 Loss = 0.394623\n",
      "1463 Training accuracy = 0.92 Loss = 0.299523\n",
      "1464 Training accuracy = 0.96 Loss = 0.172147\n",
      "1465 Training accuracy = 0.9 Loss = 0.344317\n",
      "1466 Training accuracy = 0.92 Loss = 0.290869\n",
      "1467 Training accuracy = 0.96 Loss = 0.329086\n",
      "1468 Training accuracy = 0.87 Loss = 0.357355\n",
      "1469 Training accuracy = 0.93 Loss = 0.195733\n",
      "1470 Training accuracy = 0.94 Loss = 0.319273\n",
      "1471 Training accuracy = 0.9 Loss = 0.28169\n",
      "1472 Training accuracy = 0.95 Loss = 0.236748\n",
      "1473 Training accuracy = 0.9 Loss = 0.275704\n",
      "1474 Training accuracy = 0.9 Loss = 0.389101\n",
      "1475 Training accuracy = 0.94 Loss = 0.33514\n",
      "1476 Training accuracy = 0.91 Loss = 0.284809\n",
      "1477 Training accuracy = 0.91 Loss = 0.314339\n",
      "1478 Training accuracy = 0.87 Loss = 0.331016\n",
      "1479 Training accuracy = 0.95 Loss = 0.216514\n",
      "1480 Training accuracy = 0.86 Loss = 0.346993\n",
      "1481 Training accuracy = 0.88 Loss = 0.437049\n",
      "1482 Training accuracy = 0.94 Loss = 0.233144\n",
      "1483 Training accuracy = 0.92 Loss = 0.402461\n",
      "1484 Training accuracy = 0.9 Loss = 0.361682\n",
      "1485 Training accuracy = 0.93 Loss = 0.191993\n",
      "1486 Training accuracy = 0.87 Loss = 0.37402\n",
      "1487 Training accuracy = 0.9 Loss = 0.487492\n",
      "1488 Training accuracy = 0.84 Loss = 0.593004\n",
      "1489 Training accuracy = 0.93 Loss = 0.258135\n",
      "1490 Training accuracy = 0.92 Loss = 0.228009\n",
      "1491 Training accuracy = 0.9 Loss = 0.336068\n",
      "1492 Training accuracy = 0.94 Loss = 0.238505\n",
      "1493 Training accuracy = 0.91 Loss = 0.352648\n",
      "1494 Training accuracy = 0.91 Loss = 0.289348\n",
      "1495 Training accuracy = 0.96 Loss = 0.198735\n",
      "1496 Training accuracy = 0.84 Loss = 0.507789\n",
      "1497 Training accuracy = 0.96 Loss = 0.200977\n",
      "1498 Training accuracy = 0.9 Loss = 0.459059\n",
      "1499 Training accuracy = 0.92 Loss = 0.222028\n",
      "1500 Training accuracy = 0.92 Loss = 0.269745\n",
      "1501 Training accuracy = 0.91 Loss = 0.347194\n",
      "1502 Training accuracy = 0.86 Loss = 0.554316\n",
      "1503 Training accuracy = 0.95 Loss = 0.216083\n",
      "1504 Training accuracy = 0.9 Loss = 0.310115\n",
      "1505 Training accuracy = 0.9 Loss = 0.452674\n",
      "1506 Training accuracy = 0.92 Loss = 0.337045\n",
      "1507 Training accuracy = 0.9 Loss = 0.374227\n",
      "1508 Training accuracy = 0.9 Loss = 0.261375\n",
      "1509 Training accuracy = 0.92 Loss = 0.276862\n",
      "1510 Training accuracy = 0.91 Loss = 0.33387\n",
      "1511 Training accuracy = 0.95 Loss = 0.269087\n",
      "1512 Training accuracy = 0.86 Loss = 0.464038\n",
      "1513 Training accuracy = 0.89 Loss = 0.400231\n",
      "1514 Training accuracy = 0.92 Loss = 0.275737\n",
      "1515 Training accuracy = 0.93 Loss = 0.240022\n",
      "1516 Training accuracy = 0.92 Loss = 0.299042\n",
      "1517 Training accuracy = 0.9 Loss = 0.477473\n",
      "1518 Training accuracy = 0.88 Loss = 0.360419\n",
      "1519 Training accuracy = 0.93 Loss = 0.326558\n",
      "1520 Training accuracy = 0.9 Loss = 0.304646\n",
      "1521 Training accuracy = 0.89 Loss = 0.398739\n",
      "1522 Training accuracy = 0.91 Loss = 0.279489\n",
      "1523 Training accuracy = 0.89 Loss = 0.464427\n",
      "1524 Training accuracy = 0.9 Loss = 0.29514\n",
      "1525 Training accuracy = 0.94 Loss = 0.232144\n",
      "1526 Training accuracy = 0.94 Loss = 0.214872\n",
      "1527 Training accuracy = 0.87 Loss = 0.425447\n",
      "1528 Training accuracy = 0.94 Loss = 0.264815\n",
      "1529 Training accuracy = 0.92 Loss = 0.26812\n",
      "1530 Training accuracy = 0.95 Loss = 0.162176\n",
      "1531 Training accuracy = 0.89 Loss = 0.395699\n",
      "1532 Training accuracy = 0.89 Loss = 0.418624\n",
      "1533 Training accuracy = 0.93 Loss = 0.282305\n",
      "1534 Training accuracy = 0.93 Loss = 0.279635\n",
      "1535 Training accuracy = 0.92 Loss = 0.312875\n",
      "1536 Training accuracy = 0.91 Loss = 0.344735\n",
      "1537 Training accuracy = 0.88 Loss = 0.411667\n",
      "1538 Training accuracy = 0.92 Loss = 0.319833\n",
      "1539 Training accuracy = 0.94 Loss = 0.268723\n",
      "1540 Training accuracy = 0.93 Loss = 0.323947\n",
      "1541 Training accuracy = 0.93 Loss = 0.293503\n",
      "1542 Training accuracy = 0.89 Loss = 0.331913\n",
      "1543 Training accuracy = 0.92 Loss = 0.294177\n",
      "1544 Training accuracy = 0.9 Loss = 0.322549\n",
      "1545 Training accuracy = 0.86 Loss = 0.431696\n",
      "1546 Training accuracy = 0.92 Loss = 0.324721\n",
      "1547 Training accuracy = 0.89 Loss = 0.30578\n",
      "1548 Training accuracy = 0.86 Loss = 0.411982\n",
      "1549 Training accuracy = 0.95 Loss = 0.208568\n",
      "1550 Training accuracy = 0.91 Loss = 0.409551\n",
      "1551 Training accuracy = 0.89 Loss = 0.296712\n",
      "1552 Training accuracy = 0.9 Loss = 0.462723\n",
      "1553 Training accuracy = 0.9 Loss = 0.316394\n",
      "1554 Training accuracy = 0.92 Loss = 0.269209\n",
      "1555 Training accuracy = 0.9 Loss = 0.476013\n",
      "1556 Training accuracy = 0.9 Loss = 0.402585\n",
      "1557 Training accuracy = 0.92 Loss = 0.392698\n",
      "1558 Training accuracy = 0.89 Loss = 0.441027\n",
      "1559 Training accuracy = 0.93 Loss = 0.269441\n",
      "1560 Training accuracy = 0.94 Loss = 0.289437\n",
      "1561 Training accuracy = 0.92 Loss = 0.302141\n",
      "1562 Training accuracy = 0.88 Loss = 0.364987\n",
      "1563 Training accuracy = 0.97 Loss = 0.152428\n",
      "1564 Training accuracy = 0.96 Loss = 0.261693\n",
      "1565 Training accuracy = 0.95 Loss = 0.206663\n",
      "1566 Training accuracy = 0.95 Loss = 0.182963\n",
      "1567 Training accuracy = 0.95 Loss = 0.238315\n",
      "1568 Training accuracy = 0.89 Loss = 0.278137\n",
      "1569 Training accuracy = 0.95 Loss = 0.23771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1570 Training accuracy = 0.95 Loss = 0.172679\n",
      "1571 Training accuracy = 0.84 Loss = 0.41395\n",
      "1572 Training accuracy = 0.89 Loss = 0.507378\n",
      "1573 Training accuracy = 0.91 Loss = 0.339165\n",
      "1574 Training accuracy = 0.92 Loss = 0.283889\n",
      "1575 Training accuracy = 0.9 Loss = 0.346066\n",
      "1576 Training accuracy = 0.94 Loss = 0.288707\n",
      "1577 Training accuracy = 0.9 Loss = 0.387205\n",
      "1578 Training accuracy = 0.92 Loss = 0.329877\n",
      "1579 Training accuracy = 0.92 Loss = 0.245166\n",
      "1580 Training accuracy = 0.91 Loss = 0.271168\n",
      "1581 Training accuracy = 0.9 Loss = 0.28711\n",
      "1582 Training accuracy = 0.94 Loss = 0.243272\n",
      "1583 Training accuracy = 0.92 Loss = 0.393307\n",
      "1584 Training accuracy = 0.94 Loss = 0.342635\n",
      "1585 Training accuracy = 0.9 Loss = 0.442997\n",
      "1586 Training accuracy = 0.85 Loss = 0.405295\n",
      "1587 Training accuracy = 0.92 Loss = 0.27735\n",
      "1588 Training accuracy = 0.92 Loss = 0.283224\n",
      "1589 Training accuracy = 0.94 Loss = 0.206124\n",
      "1590 Training accuracy = 0.97 Loss = 0.172656\n",
      "1591 Training accuracy = 0.95 Loss = 0.246308\n",
      "1592 Training accuracy = 0.9 Loss = 0.342152\n",
      "1593 Training accuracy = 0.92 Loss = 0.284139\n",
      "1594 Training accuracy = 0.89 Loss = 0.355028\n",
      "1595 Training accuracy = 0.92 Loss = 0.286289\n",
      "1596 Training accuracy = 0.91 Loss = 0.28024\n",
      "1597 Training accuracy = 0.88 Loss = 0.38975\n",
      "1598 Training accuracy = 0.89 Loss = 0.339385\n",
      "1599 Training accuracy = 0.94 Loss = 0.311891\n",
      "1600 Training accuracy = 0.87 Loss = 0.392179\n",
      "1601 Training accuracy = 0.93 Loss = 0.323344\n",
      "1602 Training accuracy = 0.87 Loss = 0.427883\n",
      "1603 Training accuracy = 0.95 Loss = 0.231626\n",
      "1604 Training accuracy = 0.86 Loss = 0.427941\n",
      "1605 Training accuracy = 0.93 Loss = 0.239435\n",
      "1606 Training accuracy = 0.91 Loss = 0.206892\n",
      "1607 Training accuracy = 0.93 Loss = 0.225051\n",
      "1608 Training accuracy = 0.92 Loss = 0.253978\n",
      "1609 Training accuracy = 0.86 Loss = 0.353697\n",
      "1610 Training accuracy = 0.86 Loss = 0.392759\n",
      "1611 Training accuracy = 0.92 Loss = 0.339532\n",
      "1612 Training accuracy = 0.92 Loss = 0.307543\n",
      "1613 Training accuracy = 0.9 Loss = 0.385568\n",
      "1614 Training accuracy = 0.9 Loss = 0.430388\n",
      "1615 Training accuracy = 0.9 Loss = 0.371188\n",
      "1616 Training accuracy = 0.92 Loss = 0.237744\n",
      "1617 Training accuracy = 0.9 Loss = 0.306543\n",
      "1618 Training accuracy = 0.9 Loss = 0.333269\n",
      "1619 Training accuracy = 0.91 Loss = 0.317863\n",
      "1620 Training accuracy = 0.92 Loss = 0.320256\n",
      "1621 Training accuracy = 0.9 Loss = 0.36566\n",
      "1622 Training accuracy = 0.91 Loss = 0.302553\n",
      "1623 Training accuracy = 0.91 Loss = 0.304562\n",
      "1624 Training accuracy = 0.92 Loss = 0.298826\n",
      "1625 Training accuracy = 0.85 Loss = 0.599716\n",
      "1626 Training accuracy = 0.94 Loss = 0.236304\n",
      "1627 Training accuracy = 0.94 Loss = 0.187603\n",
      "1628 Training accuracy = 0.87 Loss = 0.2831\n",
      "1629 Training accuracy = 0.9 Loss = 0.398065\n",
      "1630 Training accuracy = 0.92 Loss = 0.199855\n",
      "1631 Training accuracy = 0.9 Loss = 0.456487\n",
      "1632 Training accuracy = 0.89 Loss = 0.431457\n",
      "1633 Training accuracy = 0.96 Loss = 0.191576\n",
      "1634 Training accuracy = 0.93 Loss = 0.253059\n",
      "1635 Training accuracy = 0.91 Loss = 0.290364\n",
      "1636 Training accuracy = 0.93 Loss = 0.277892\n",
      "1637 Training accuracy = 0.93 Loss = 0.258543\n",
      "1638 Training accuracy = 0.78 Loss = 0.542774\n",
      "1639 Training accuracy = 0.86 Loss = 0.279356\n",
      "1640 Training accuracy = 0.92 Loss = 0.232679\n",
      "1641 Training accuracy = 0.95 Loss = 0.222111\n",
      "1642 Training accuracy = 0.92 Loss = 0.283084\n",
      "1643 Training accuracy = 0.92 Loss = 0.277254\n",
      "1644 Training accuracy = 0.87 Loss = 0.419825\n",
      "1645 Training accuracy = 0.92 Loss = 0.330239\n",
      "1646 Training accuracy = 0.93 Loss = 0.268641\n",
      "1647 Training accuracy = 0.88 Loss = 0.352883\n",
      "1648 Training accuracy = 0.89 Loss = 0.371017\n",
      "1649 Training accuracy = 0.92 Loss = 0.287031\n",
      "1650 Training accuracy = 0.94 Loss = 0.185602\n",
      "1651 Training accuracy = 0.9 Loss = 0.349837\n",
      "1652 Training accuracy = 0.91 Loss = 0.233411\n",
      "1653 Training accuracy = 0.9 Loss = 0.291961\n",
      "1654 Training accuracy = 0.92 Loss = 0.218133\n",
      "1655 Training accuracy = 0.9 Loss = 0.473968\n",
      "1656 Training accuracy = 0.89 Loss = 0.361313\n",
      "1657 Training accuracy = 0.92 Loss = 0.45712\n",
      "1658 Training accuracy = 0.91 Loss = 0.322957\n",
      "1659 Training accuracy = 0.92 Loss = 0.320261\n",
      "1660 Training accuracy = 0.95 Loss = 0.241376\n",
      "1661 Training accuracy = 0.95 Loss = 0.221026\n",
      "1662 Training accuracy = 0.91 Loss = 0.243156\n",
      "1663 Training accuracy = 0.91 Loss = 0.259473\n",
      "1664 Training accuracy = 0.93 Loss = 0.286385\n",
      "1665 Training accuracy = 0.92 Loss = 0.210517\n",
      "1666 Training accuracy = 0.91 Loss = 0.423085\n",
      "1667 Training accuracy = 0.94 Loss = 0.217883\n",
      "1668 Training accuracy = 0.89 Loss = 0.353145\n",
      "1669 Training accuracy = 0.9 Loss = 0.312658\n",
      "1670 Training accuracy = 0.93 Loss = 0.305898\n",
      "1671 Training accuracy = 0.94 Loss = 0.263722\n",
      "1672 Training accuracy = 0.9 Loss = 0.376444\n",
      "1673 Training accuracy = 0.88 Loss = 0.386557\n",
      "1674 Training accuracy = 0.9 Loss = 0.296234\n",
      "1675 Training accuracy = 0.89 Loss = 0.427201\n",
      "1676 Training accuracy = 0.92 Loss = 0.282791\n",
      "1677 Training accuracy = 0.86 Loss = 0.384181\n",
      "1678 Training accuracy = 0.9 Loss = 0.363428\n",
      "1679 Training accuracy = 0.95 Loss = 0.213813\n",
      "1680 Training accuracy = 0.95 Loss = 0.228566\n",
      "1681 Training accuracy = 0.88 Loss = 0.323848\n",
      "1682 Training accuracy = 0.92 Loss = 0.192356\n",
      "1683 Training accuracy = 0.87 Loss = 0.392462\n",
      "1684 Training accuracy = 0.97 Loss = 0.21527\n",
      "1685 Training accuracy = 0.92 Loss = 0.328212\n",
      "1686 Training accuracy = 0.88 Loss = 0.316186\n",
      "1687 Training accuracy = 0.9 Loss = 0.325443\n",
      "1688 Training accuracy = 0.9 Loss = 0.321488\n",
      "1689 Training accuracy = 0.92 Loss = 0.418989\n",
      "1690 Training accuracy = 0.89 Loss = 0.419959\n",
      "1691 Training accuracy = 0.89 Loss = 0.419796\n",
      "1692 Training accuracy = 0.92 Loss = 0.298259\n",
      "1693 Training accuracy = 0.99 Loss = 0.119729\n",
      "1694 Training accuracy = 0.87 Loss = 0.366611\n",
      "1695 Training accuracy = 0.93 Loss = 0.284814\n",
      "1696 Training accuracy = 0.88 Loss = 0.346554\n",
      "1697 Training accuracy = 0.94 Loss = 0.185806\n",
      "1698 Training accuracy = 0.94 Loss = 0.272642\n",
      "1699 Training accuracy = 0.9 Loss = 0.264691\n",
      "1700 Training accuracy = 0.98 Loss = 0.15857\n",
      "1701 Training accuracy = 0.91 Loss = 0.370851\n",
      "1702 Training accuracy = 0.9 Loss = 0.401115\n",
      "1703 Training accuracy = 0.92 Loss = 0.305046\n",
      "1704 Training accuracy = 0.92 Loss = 0.256151\n",
      "1705 Training accuracy = 0.85 Loss = 0.433985\n",
      "1706 Training accuracy = 0.88 Loss = 0.302758\n",
      "1707 Training accuracy = 0.94 Loss = 0.210525\n",
      "1708 Training accuracy = 0.91 Loss = 0.270092\n",
      "1709 Training accuracy = 0.9 Loss = 0.295275\n",
      "1710 Training accuracy = 0.95 Loss = 0.230053\n",
      "1711 Training accuracy = 0.93 Loss = 0.251415\n",
      "1712 Training accuracy = 0.91 Loss = 0.254416\n",
      "1713 Training accuracy = 0.92 Loss = 0.311947\n",
      "1714 Training accuracy = 0.92 Loss = 0.309138\n",
      "1715 Training accuracy = 0.92 Loss = 0.248726\n",
      "1716 Training accuracy = 0.93 Loss = 0.21761\n",
      "1717 Training accuracy = 0.91 Loss = 0.260252\n",
      "1718 Training accuracy = 0.95 Loss = 0.24802\n",
      "1719 Training accuracy = 0.96 Loss = 0.186179\n",
      "1720 Training accuracy = 0.9 Loss = 0.274425\n",
      "1721 Training accuracy = 0.94 Loss = 0.264203\n",
      "1722 Training accuracy = 0.88 Loss = 0.345509\n",
      "1723 Training accuracy = 0.93 Loss = 0.247475\n",
      "1724 Training accuracy = 0.95 Loss = 0.234984\n",
      "1725 Training accuracy = 0.91 Loss = 0.311338\n",
      "1726 Training accuracy = 0.97 Loss = 0.143451\n",
      "1727 Training accuracy = 0.94 Loss = 0.322598\n",
      "1728 Training accuracy = 0.97 Loss = 0.180679\n",
      "1729 Training accuracy = 0.97 Loss = 0.163223\n",
      "1730 Training accuracy = 0.95 Loss = 0.192157\n",
      "1731 Training accuracy = 0.92 Loss = 0.249776\n",
      "1732 Training accuracy = 0.91 Loss = 0.319623\n",
      "1733 Training accuracy = 0.9 Loss = 0.313603\n",
      "1734 Training accuracy = 0.92 Loss = 0.312719\n",
      "1735 Training accuracy = 0.93 Loss = 0.268255\n",
      "1736 Training accuracy = 0.95 Loss = 0.223451\n",
      "1737 Training accuracy = 0.91 Loss = 0.432324\n",
      "1738 Training accuracy = 0.93 Loss = 0.360338\n",
      "1739 Training accuracy = 0.95 Loss = 0.208764\n",
      "1740 Training accuracy = 0.93 Loss = 0.311104\n",
      "1741 Training accuracy = 0.92 Loss = 0.384365\n",
      "1742 Training accuracy = 0.94 Loss = 0.198642\n",
      "1743 Training accuracy = 0.91 Loss = 0.334701\n",
      "1744 Training accuracy = 0.88 Loss = 0.35225\n",
      "1745 Training accuracy = 0.9 Loss = 0.370329\n",
      "1746 Training accuracy = 0.92 Loss = 0.266296\n",
      "1747 Training accuracy = 0.92 Loss = 0.229949\n",
      "1748 Training accuracy = 0.91 Loss = 0.311252\n",
      "1749 Training accuracy = 0.93 Loss = 0.271325\n",
      "1750 Training accuracy = 0.89 Loss = 0.364295\n",
      "1751 Training accuracy = 0.87 Loss = 0.374989\n",
      "1752 Training accuracy = 0.94 Loss = 0.203225\n",
      "1753 Training accuracy = 0.94 Loss = 0.217571\n",
      "1754 Training accuracy = 0.91 Loss = 0.373516\n",
      "1755 Training accuracy = 0.9 Loss = 0.462314\n",
      "1756 Training accuracy = 0.9 Loss = 0.295966\n",
      "1757 Training accuracy = 0.91 Loss = 0.407877\n",
      "1758 Training accuracy = 0.92 Loss = 0.3081\n",
      "1759 Training accuracy = 0.96 Loss = 0.25027\n",
      "1760 Training accuracy = 0.93 Loss = 0.259235\n",
      "1761 Training accuracy = 0.9 Loss = 0.307922\n",
      "1762 Training accuracy = 0.87 Loss = 0.473979\n",
      "1763 Training accuracy = 0.96 Loss = 0.197383\n",
      "1764 Training accuracy = 0.93 Loss = 0.215683\n",
      "1765 Training accuracy = 0.93 Loss = 0.238064\n",
      "1766 Training accuracy = 0.91 Loss = 0.325626\n",
      "1767 Training accuracy = 0.89 Loss = 0.420543\n",
      "1768 Training accuracy = 0.95 Loss = 0.176454\n",
      "1769 Training accuracy = 0.95 Loss = 0.199565\n",
      "1770 Training accuracy = 0.91 Loss = 0.417115\n",
      "1771 Training accuracy = 0.89 Loss = 0.47455\n",
      "1772 Training accuracy = 0.91 Loss = 0.262843\n",
      "1773 Training accuracy = 0.95 Loss = 0.243468\n",
      "1774 Training accuracy = 0.92 Loss = 0.356256\n",
      "1775 Training accuracy = 0.96 Loss = 0.27403\n",
      "1776 Training accuracy = 0.95 Loss = 0.244299\n",
      "1777 Training accuracy = 0.92 Loss = 0.324313\n",
      "1778 Training accuracy = 0.93 Loss = 0.25152\n",
      "1779 Training accuracy = 0.95 Loss = 0.238197\n",
      "1780 Training accuracy = 0.95 Loss = 0.197784\n",
      "1781 Training accuracy = 0.91 Loss = 0.316891\n",
      "1782 Training accuracy = 0.92 Loss = 0.388576\n",
      "1783 Training accuracy = 0.94 Loss = 0.332013\n",
      "1784 Training accuracy = 0.91 Loss = 0.301519\n",
      "1785 Training accuracy = 0.94 Loss = 0.251734\n",
      "1786 Training accuracy = 0.94 Loss = 0.244126\n",
      "1787 Training accuracy = 0.93 Loss = 0.246883\n",
      "1788 Training accuracy = 0.95 Loss = 0.189675\n",
      "1789 Training accuracy = 0.9 Loss = 0.280125\n",
      "1790 Training accuracy = 0.9 Loss = 0.388935\n",
      "1791 Training accuracy = 0.83 Loss = 0.496536\n",
      "1792 Training accuracy = 0.94 Loss = 0.203649\n",
      "1793 Training accuracy = 0.95 Loss = 0.216763\n",
      "1794 Training accuracy = 0.9 Loss = 0.276476\n",
      "1795 Training accuracy = 0.89 Loss = 0.329366\n",
      "1796 Training accuracy = 0.89 Loss = 0.340316\n",
      "1797 Training accuracy = 0.94 Loss = 0.236397\n",
      "1798 Training accuracy = 0.91 Loss = 0.221753\n",
      "1799 Training accuracy = 0.93 Loss = 0.209299\n",
      "1800 Training accuracy = 0.91 Loss = 0.318351\n",
      "1801 Training accuracy = 0.89 Loss = 0.283838\n",
      "1802 Training accuracy = 0.92 Loss = 0.35291\n",
      "1803 Training accuracy = 0.86 Loss = 0.429015\n",
      "1804 Training accuracy = 0.91 Loss = 0.361439\n",
      "1805 Training accuracy = 0.91 Loss = 0.339078\n",
      "1806 Training accuracy = 0.9 Loss = 0.320001\n",
      "1807 Training accuracy = 0.92 Loss = 0.29728\n",
      "1808 Training accuracy = 0.94 Loss = 0.253107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809 Training accuracy = 0.85 Loss = 0.383849\n",
      "1810 Training accuracy = 0.91 Loss = 0.276826\n",
      "1811 Training accuracy = 0.9 Loss = 0.35959\n",
      "1812 Training accuracy = 0.94 Loss = 0.229377\n",
      "1813 Training accuracy = 0.96 Loss = 0.194022\n",
      "1814 Training accuracy = 0.88 Loss = 0.393453\n",
      "1815 Training accuracy = 0.95 Loss = 0.190398\n",
      "1816 Training accuracy = 0.92 Loss = 0.392725\n",
      "1817 Training accuracy = 0.95 Loss = 0.229491\n",
      "1818 Training accuracy = 0.95 Loss = 0.196938\n",
      "1819 Training accuracy = 0.86 Loss = 0.323323\n",
      "1820 Training accuracy = 0.87 Loss = 0.378761\n",
      "1821 Training accuracy = 0.92 Loss = 0.418439\n",
      "1822 Training accuracy = 0.92 Loss = 0.252555\n",
      "1823 Training accuracy = 0.91 Loss = 0.361697\n",
      "1824 Training accuracy = 0.93 Loss = 0.397344\n",
      "1825 Training accuracy = 0.92 Loss = 0.300384\n",
      "1826 Training accuracy = 0.93 Loss = 0.291464\n",
      "1827 Training accuracy = 0.91 Loss = 0.302342\n",
      "1828 Training accuracy = 0.92 Loss = 0.290916\n",
      "1829 Training accuracy = 0.89 Loss = 0.291953\n",
      "1830 Training accuracy = 0.93 Loss = 0.280412\n",
      "1831 Training accuracy = 0.88 Loss = 0.380006\n",
      "1832 Training accuracy = 0.9 Loss = 0.317906\n",
      "1833 Training accuracy = 0.94 Loss = 0.207817\n",
      "1834 Training accuracy = 0.9 Loss = 0.357419\n",
      "1835 Training accuracy = 0.88 Loss = 0.39394\n",
      "1836 Training accuracy = 0.93 Loss = 0.235727\n",
      "1837 Training accuracy = 0.91 Loss = 0.359566\n",
      "1838 Training accuracy = 0.93 Loss = 0.278763\n",
      "1839 Training accuracy = 0.85 Loss = 0.343811\n",
      "1840 Training accuracy = 0.93 Loss = 0.282846\n",
      "1841 Training accuracy = 0.95 Loss = 0.289784\n",
      "1842 Training accuracy = 0.92 Loss = 0.310291\n",
      "1843 Training accuracy = 0.9 Loss = 0.383954\n",
      "1844 Training accuracy = 0.92 Loss = 0.255754\n",
      "1845 Training accuracy = 0.94 Loss = 0.302341\n",
      "1846 Training accuracy = 0.95 Loss = 0.234902\n",
      "1847 Training accuracy = 0.94 Loss = 0.18185\n",
      "1848 Training accuracy = 0.88 Loss = 0.42444\n",
      "1849 Training accuracy = 0.86 Loss = 0.374551\n",
      "1850 Training accuracy = 0.95 Loss = 0.208861\n",
      "1851 Training accuracy = 0.89 Loss = 0.435762\n",
      "1852 Training accuracy = 0.89 Loss = 0.349697\n",
      "1853 Training accuracy = 0.96 Loss = 0.216817\n",
      "1854 Training accuracy = 0.89 Loss = 0.332263\n",
      "1855 Training accuracy = 0.97 Loss = 0.156241\n",
      "1856 Training accuracy = 0.89 Loss = 0.275729\n",
      "1857 Training accuracy = 0.92 Loss = 0.278815\n",
      "1858 Training accuracy = 0.9 Loss = 0.340009\n",
      "1859 Training accuracy = 0.9 Loss = 0.298918\n",
      "1860 Training accuracy = 0.96 Loss = 0.220225\n",
      "1861 Training accuracy = 0.95 Loss = 0.286689\n",
      "1862 Training accuracy = 0.91 Loss = 0.265719\n",
      "1863 Training accuracy = 0.95 Loss = 0.251334\n",
      "1864 Training accuracy = 0.91 Loss = 0.357512\n",
      "1865 Training accuracy = 0.88 Loss = 0.284783\n",
      "1866 Training accuracy = 0.9 Loss = 0.423296\n",
      "1867 Training accuracy = 0.9 Loss = 0.330612\n",
      "1868 Training accuracy = 0.89 Loss = 0.341515\n",
      "1869 Training accuracy = 0.93 Loss = 0.217235\n",
      "1870 Training accuracy = 0.9 Loss = 0.389482\n",
      "1871 Training accuracy = 0.94 Loss = 0.195103\n",
      "1872 Training accuracy = 0.92 Loss = 0.277108\n",
      "1873 Training accuracy = 0.95 Loss = 0.168261\n",
      "1874 Training accuracy = 0.91 Loss = 0.235869\n",
      "1875 Training accuracy = 0.9 Loss = 0.336331\n",
      "1876 Training accuracy = 0.93 Loss = 0.315057\n",
      "1877 Training accuracy = 0.88 Loss = 0.370062\n",
      "1878 Training accuracy = 0.9 Loss = 0.317506\n",
      "1879 Training accuracy = 0.92 Loss = 0.317257\n",
      "1880 Training accuracy = 0.92 Loss = 0.296964\n",
      "1881 Training accuracy = 0.87 Loss = 0.340281\n",
      "1882 Training accuracy = 0.93 Loss = 0.22404\n",
      "1883 Training accuracy = 0.92 Loss = 0.24649\n",
      "1884 Training accuracy = 0.89 Loss = 0.423876\n",
      "1885 Training accuracy = 0.92 Loss = 0.360556\n",
      "1886 Training accuracy = 0.93 Loss = 0.403896\n",
      "1887 Training accuracy = 0.96 Loss = 0.202261\n",
      "1888 Training accuracy = 0.96 Loss = 0.176413\n",
      "1889 Training accuracy = 0.95 Loss = 0.18125\n",
      "1890 Training accuracy = 0.91 Loss = 0.271121\n",
      "1891 Training accuracy = 0.92 Loss = 0.272542\n",
      "1892 Training accuracy = 0.93 Loss = 0.245703\n",
      "1893 Training accuracy = 0.92 Loss = 0.319516\n",
      "1894 Training accuracy = 0.93 Loss = 0.261927\n",
      "1895 Training accuracy = 0.87 Loss = 0.453648\n",
      "1896 Training accuracy = 0.89 Loss = 0.356266\n",
      "1897 Training accuracy = 0.87 Loss = 0.29598\n",
      "1898 Training accuracy = 0.88 Loss = 0.286848\n",
      "1899 Training accuracy = 0.9 Loss = 0.435594\n",
      "1900 Training accuracy = 0.94 Loss = 0.209615\n",
      "1901 Training accuracy = 0.86 Loss = 0.322164\n",
      "1902 Training accuracy = 0.89 Loss = 0.27936\n",
      "1903 Training accuracy = 0.88 Loss = 0.322577\n",
      "1904 Training accuracy = 0.94 Loss = 0.277873\n",
      "1905 Training accuracy = 0.88 Loss = 0.357495\n",
      "1906 Training accuracy = 0.93 Loss = 0.278043\n",
      "1907 Training accuracy = 0.96 Loss = 0.228022\n",
      "1908 Training accuracy = 0.93 Loss = 0.305575\n",
      "1909 Training accuracy = 0.9 Loss = 0.310524\n",
      "1910 Training accuracy = 0.91 Loss = 0.323725\n",
      "1911 Training accuracy = 0.92 Loss = 0.311079\n",
      "1912 Training accuracy = 0.88 Loss = 0.427586\n",
      "1913 Training accuracy = 0.91 Loss = 0.279586\n",
      "1914 Training accuracy = 0.89 Loss = 0.335708\n",
      "1915 Training accuracy = 0.93 Loss = 0.22733\n",
      "1916 Training accuracy = 0.96 Loss = 0.152567\n",
      "1917 Training accuracy = 0.9 Loss = 0.34277\n",
      "1918 Training accuracy = 0.93 Loss = 0.225241\n",
      "1919 Training accuracy = 0.88 Loss = 0.304515\n",
      "1920 Training accuracy = 0.92 Loss = 0.259034\n",
      "1921 Training accuracy = 0.95 Loss = 0.240316\n",
      "1922 Training accuracy = 0.94 Loss = 0.198908\n",
      "1923 Training accuracy = 0.94 Loss = 0.207005\n",
      "1924 Training accuracy = 0.92 Loss = 0.254646\n",
      "1925 Training accuracy = 0.92 Loss = 0.299518\n",
      "1926 Training accuracy = 0.88 Loss = 0.473527\n",
      "1927 Training accuracy = 0.89 Loss = 0.385497\n",
      "1928 Training accuracy = 0.93 Loss = 0.288382\n",
      "1929 Training accuracy = 0.9 Loss = 0.296342\n",
      "1930 Training accuracy = 0.92 Loss = 0.236182\n",
      "1931 Training accuracy = 0.97 Loss = 0.125974\n",
      "1932 Training accuracy = 0.93 Loss = 0.275699\n",
      "1933 Training accuracy = 0.93 Loss = 0.246137\n",
      "1934 Training accuracy = 0.9 Loss = 0.33219\n",
      "1935 Training accuracy = 0.92 Loss = 0.256626\n",
      "1936 Training accuracy = 0.95 Loss = 0.218419\n",
      "1937 Training accuracy = 0.89 Loss = 0.324747\n",
      "1938 Training accuracy = 0.93 Loss = 0.351643\n",
      "1939 Training accuracy = 0.92 Loss = 0.296126\n",
      "1940 Training accuracy = 0.92 Loss = 0.26517\n",
      "1941 Training accuracy = 0.93 Loss = 0.206729\n",
      "1942 Training accuracy = 0.98 Loss = 0.151315\n",
      "1943 Training accuracy = 0.88 Loss = 0.357306\n",
      "1944 Training accuracy = 0.94 Loss = 0.315312\n",
      "1945 Training accuracy = 0.89 Loss = 0.418218\n",
      "1946 Training accuracy = 0.91 Loss = 0.299773\n",
      "1947 Training accuracy = 0.92 Loss = 0.310307\n",
      "1948 Training accuracy = 0.96 Loss = 0.176409\n",
      "1949 Training accuracy = 0.87 Loss = 0.512112\n",
      "1950 Training accuracy = 0.89 Loss = 0.314309\n",
      "1951 Training accuracy = 0.9 Loss = 0.384667\n",
      "1952 Training accuracy = 0.88 Loss = 0.434594\n",
      "1953 Training accuracy = 0.93 Loss = 0.231237\n",
      "1954 Training accuracy = 0.9 Loss = 0.344138\n",
      "1955 Training accuracy = 0.91 Loss = 0.410794\n",
      "1956 Training accuracy = 0.93 Loss = 0.384523\n",
      "1957 Training accuracy = 0.89 Loss = 0.42916\n",
      "1958 Training accuracy = 0.9 Loss = 0.400608\n",
      "1959 Training accuracy = 0.89 Loss = 0.461959\n",
      "1960 Training accuracy = 0.94 Loss = 0.37304\n",
      "1961 Training accuracy = 0.9 Loss = 0.350095\n",
      "1962 Training accuracy = 0.91 Loss = 0.212829\n",
      "1963 Training accuracy = 0.95 Loss = 0.340909\n",
      "1964 Training accuracy = 0.87 Loss = 0.452025\n",
      "1965 Training accuracy = 0.9 Loss = 0.297892\n",
      "1966 Training accuracy = 0.93 Loss = 0.270411\n",
      "1967 Training accuracy = 0.96 Loss = 0.271636\n",
      "1968 Training accuracy = 0.91 Loss = 0.388477\n",
      "1969 Training accuracy = 0.95 Loss = 0.224332\n",
      "1970 Training accuracy = 0.9 Loss = 0.309309\n",
      "1971 Training accuracy = 0.93 Loss = 0.292501\n",
      "1972 Training accuracy = 0.92 Loss = 0.266437\n",
      "1973 Training accuracy = 0.93 Loss = 0.253963\n",
      "1974 Training accuracy = 0.95 Loss = 0.158192\n",
      "1975 Training accuracy = 0.86 Loss = 0.602095\n",
      "1976 Training accuracy = 0.88 Loss = 0.396537\n",
      "1977 Training accuracy = 0.93 Loss = 0.213135\n",
      "1978 Training accuracy = 0.95 Loss = 0.244657\n",
      "1979 Training accuracy = 0.89 Loss = 0.249476\n",
      "1980 Training accuracy = 0.92 Loss = 0.294203\n",
      "1981 Training accuracy = 0.92 Loss = 0.416288\n",
      "1982 Training accuracy = 0.93 Loss = 0.266555\n",
      "1983 Training accuracy = 0.97 Loss = 0.250887\n",
      "1984 Training accuracy = 0.88 Loss = 0.490798\n",
      "1985 Training accuracy = 0.88 Loss = 0.381444\n",
      "1986 Training accuracy = 0.91 Loss = 0.366696\n",
      "1987 Training accuracy = 0.92 Loss = 0.378208\n",
      "1988 Training accuracy = 0.91 Loss = 0.323849\n",
      "1989 Training accuracy = 0.92 Loss = 0.185475\n",
      "1990 Training accuracy = 0.95 Loss = 0.228051\n",
      "1991 Training accuracy = 0.89 Loss = 0.401273\n",
      "1992 Training accuracy = 0.93 Loss = 0.22574\n",
      "1993 Training accuracy = 0.92 Loss = 0.342208\n",
      "1994 Training accuracy = 0.96 Loss = 0.230049\n",
      "1995 Training accuracy = 0.89 Loss = 0.33298\n",
      "1996 Training accuracy = 0.91 Loss = 0.320484\n",
      "1997 Training accuracy = 0.98 Loss = 0.203418\n",
      "1998 Training accuracy = 0.93 Loss = 0.240381\n",
      "1999 Training accuracy = 0.92 Loss = 0.289232\n",
      "2000 Training accuracy = 0.95 Loss = 0.221831\n",
      "2001 Training accuracy = 0.9 Loss = 0.342156\n",
      "2002 Training accuracy = 0.92 Loss = 0.451747\n",
      "2003 Training accuracy = 0.94 Loss = 0.414298\n",
      "2004 Training accuracy = 0.87 Loss = 0.473295\n",
      "2005 Training accuracy = 0.91 Loss = 0.243162\n",
      "2006 Training accuracy = 0.87 Loss = 0.346804\n",
      "2007 Training accuracy = 0.91 Loss = 0.298604\n",
      "2008 Training accuracy = 0.94 Loss = 0.25625\n",
      "2009 Training accuracy = 0.9 Loss = 0.30468\n",
      "2010 Training accuracy = 0.97 Loss = 0.147497\n",
      "2011 Training accuracy = 0.91 Loss = 0.287137\n",
      "2012 Training accuracy = 0.9 Loss = 0.270288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013 Training accuracy = 0.94 Loss = 0.217523\n",
      "2014 Training accuracy = 0.9 Loss = 0.381548\n",
      "2015 Training accuracy = 0.94 Loss = 0.217234\n",
      "2016 Training accuracy = 0.9 Loss = 0.338311\n",
      "2017 Training accuracy = 0.89 Loss = 0.41053\n",
      "2018 Training accuracy = 0.9 Loss = 0.413226\n",
      "2019 Training accuracy = 0.9 Loss = 0.376381\n",
      "2020 Training accuracy = 0.91 Loss = 0.360843\n",
      "2021 Training accuracy = 0.91 Loss = 0.407254\n",
      "2022 Training accuracy = 0.9 Loss = 0.340132\n",
      "2023 Training accuracy = 0.93 Loss = 0.316852\n",
      "2024 Training accuracy = 0.94 Loss = 0.257786\n",
      "2025 Training accuracy = 0.92 Loss = 0.305487\n",
      "2026 Training accuracy = 0.92 Loss = 0.235255\n",
      "2027 Training accuracy = 0.89 Loss = 0.312681\n",
      "2028 Training accuracy = 0.91 Loss = 0.354795\n",
      "2029 Training accuracy = 0.94 Loss = 0.228544\n",
      "2030 Training accuracy = 0.89 Loss = 0.356792\n",
      "2031 Training accuracy = 0.92 Loss = 0.325866\n",
      "2032 Training accuracy = 0.94 Loss = 0.171667\n",
      "2033 Training accuracy = 0.91 Loss = 0.281137\n",
      "2034 Training accuracy = 0.94 Loss = 0.219371\n",
      "2035 Training accuracy = 0.92 Loss = 0.339958\n",
      "2036 Training accuracy = 0.96 Loss = 0.175238\n",
      "2037 Training accuracy = 0.91 Loss = 0.324214\n",
      "2038 Training accuracy = 0.91 Loss = 0.394045\n",
      "2039 Training accuracy = 0.93 Loss = 0.247656\n",
      "2040 Training accuracy = 0.94 Loss = 0.288356\n",
      "2041 Training accuracy = 0.93 Loss = 0.242728\n",
      "2042 Training accuracy = 0.87 Loss = 0.31084\n",
      "2043 Training accuracy = 0.99 Loss = 0.100223\n",
      "2044 Training accuracy = 0.95 Loss = 0.259166\n",
      "2045 Training accuracy = 0.88 Loss = 0.305196\n",
      "2046 Training accuracy = 0.95 Loss = 0.204219\n",
      "2047 Training accuracy = 0.88 Loss = 0.423505\n",
      "2048 Training accuracy = 0.92 Loss = 0.25526\n",
      "2049 Training accuracy = 0.91 Loss = 0.273856\n",
      "2050 Training accuracy = 0.95 Loss = 0.20199\n",
      "2051 Training accuracy = 0.93 Loss = 0.322148\n",
      "2052 Training accuracy = 0.94 Loss = 0.260165\n",
      "2053 Training accuracy = 0.94 Loss = 0.195681\n",
      "2054 Training accuracy = 0.92 Loss = 0.26917\n",
      "2055 Training accuracy = 0.96 Loss = 0.276124\n",
      "2056 Training accuracy = 0.89 Loss = 0.305964\n",
      "2057 Training accuracy = 0.95 Loss = 0.174217\n",
      "2058 Training accuracy = 0.92 Loss = 0.266275\n",
      "2059 Training accuracy = 0.93 Loss = 0.217545\n",
      "2060 Training accuracy = 0.94 Loss = 0.221387\n",
      "2061 Training accuracy = 0.93 Loss = 0.318819\n",
      "2062 Training accuracy = 0.86 Loss = 0.408837\n",
      "2063 Training accuracy = 0.94 Loss = 0.275775\n",
      "2064 Training accuracy = 0.91 Loss = 0.225277\n",
      "2065 Training accuracy = 0.89 Loss = 0.39881\n",
      "2066 Training accuracy = 0.9 Loss = 0.357744\n",
      "2067 Training accuracy = 0.94 Loss = 0.212796\n",
      "2068 Training accuracy = 0.92 Loss = 0.292743\n",
      "2069 Training accuracy = 0.96 Loss = 0.142971\n",
      "2070 Training accuracy = 0.87 Loss = 0.442816\n",
      "2071 Training accuracy = 0.91 Loss = 0.253153\n",
      "2072 Training accuracy = 0.92 Loss = 0.275637\n",
      "2073 Training accuracy = 0.89 Loss = 0.316319\n",
      "2074 Training accuracy = 0.97 Loss = 0.155449\n",
      "2075 Training accuracy = 0.93 Loss = 0.313736\n",
      "2076 Training accuracy = 0.93 Loss = 0.234026\n",
      "2077 Training accuracy = 0.89 Loss = 0.402617\n",
      "2078 Training accuracy = 0.96 Loss = 0.190629\n",
      "2079 Training accuracy = 0.89 Loss = 0.439496\n",
      "2080 Training accuracy = 0.87 Loss = 0.42902\n",
      "2081 Training accuracy = 0.9 Loss = 0.390229\n",
      "2082 Training accuracy = 0.93 Loss = 0.231375\n",
      "2083 Training accuracy = 0.94 Loss = 0.204527\n",
      "2084 Training accuracy = 0.94 Loss = 0.213136\n",
      "2085 Training accuracy = 0.95 Loss = 0.289678\n",
      "2086 Training accuracy = 0.87 Loss = 0.375502\n",
      "2087 Training accuracy = 0.94 Loss = 0.256484\n",
      "2088 Training accuracy = 0.88 Loss = 0.434278\n",
      "2089 Training accuracy = 0.91 Loss = 0.295314\n",
      "2090 Training accuracy = 0.91 Loss = 0.350146\n",
      "2091 Training accuracy = 0.95 Loss = 0.302348\n",
      "2092 Training accuracy = 0.91 Loss = 0.29331\n",
      "2093 Training accuracy = 0.92 Loss = 0.249267\n",
      "2094 Training accuracy = 0.89 Loss = 0.328426\n",
      "2095 Training accuracy = 0.91 Loss = 0.342086\n",
      "2096 Training accuracy = 0.88 Loss = 0.39737\n",
      "2097 Training accuracy = 0.94 Loss = 0.305237\n",
      "2098 Training accuracy = 0.9 Loss = 0.263599\n",
      "2099 Training accuracy = 0.88 Loss = 0.308916\n",
      "2100 Training accuracy = 0.92 Loss = 0.251838\n",
      "2101 Training accuracy = 0.9 Loss = 0.29469\n",
      "2102 Training accuracy = 0.9 Loss = 0.349106\n",
      "2103 Training accuracy = 0.91 Loss = 0.339974\n",
      "2104 Training accuracy = 0.96 Loss = 0.157065\n",
      "2105 Training accuracy = 0.9 Loss = 0.285595\n",
      "2106 Training accuracy = 0.9 Loss = 0.276132\n",
      "2107 Training accuracy = 0.88 Loss = 0.419166\n",
      "2108 Training accuracy = 0.96 Loss = 0.207005\n",
      "2109 Training accuracy = 0.94 Loss = 0.205409\n",
      "2110 Training accuracy = 0.9 Loss = 0.346746\n",
      "2111 Training accuracy = 0.92 Loss = 0.364546\n",
      "2112 Training accuracy = 0.88 Loss = 0.388711\n",
      "2113 Training accuracy = 0.83 Loss = 0.527707\n",
      "2114 Training accuracy = 0.92 Loss = 0.211662\n",
      "2115 Training accuracy = 0.88 Loss = 0.393642\n",
      "2116 Training accuracy = 0.9 Loss = 0.498448\n",
      "2117 Training accuracy = 0.93 Loss = 0.198953\n",
      "2118 Training accuracy = 0.89 Loss = 0.385462\n",
      "2119 Training accuracy = 0.91 Loss = 0.319461\n",
      "2120 Training accuracy = 0.88 Loss = 0.371818\n",
      "2121 Training accuracy = 0.9 Loss = 0.378995\n",
      "2122 Training accuracy = 0.85 Loss = 0.440207\n",
      "2123 Training accuracy = 0.93 Loss = 0.269182\n",
      "2124 Training accuracy = 0.83 Loss = 0.428226\n",
      "2125 Training accuracy = 0.92 Loss = 0.255354\n",
      "2126 Training accuracy = 0.91 Loss = 0.317012\n",
      "2127 Training accuracy = 0.95 Loss = 0.198905\n",
      "2128 Training accuracy = 0.88 Loss = 0.346455\n",
      "2129 Training accuracy = 0.88 Loss = 0.444008\n",
      "2130 Training accuracy = 0.93 Loss = 0.283474\n",
      "2131 Training accuracy = 0.94 Loss = 0.186568\n",
      "2132 Training accuracy = 0.94 Loss = 0.259726\n",
      "2133 Training accuracy = 0.94 Loss = 0.292784\n",
      "2134 Training accuracy = 0.91 Loss = 0.343278\n",
      "2135 Training accuracy = 0.92 Loss = 0.279672\n",
      "2136 Training accuracy = 0.93 Loss = 0.280955\n",
      "2137 Training accuracy = 0.93 Loss = 0.32782\n",
      "2138 Training accuracy = 0.91 Loss = 0.328499\n",
      "2139 Training accuracy = 0.91 Loss = 0.250626\n",
      "2140 Training accuracy = 0.93 Loss = 0.219676\n",
      "2141 Training accuracy = 0.94 Loss = 0.179823\n",
      "2142 Training accuracy = 0.91 Loss = 0.425934\n",
      "2143 Training accuracy = 0.91 Loss = 0.329447\n",
      "2144 Training accuracy = 0.89 Loss = 0.392864\n",
      "2145 Training accuracy = 0.86 Loss = 0.444319\n",
      "2146 Training accuracy = 0.93 Loss = 0.211507\n",
      "2147 Training accuracy = 0.91 Loss = 0.30064\n",
      "2148 Training accuracy = 0.96 Loss = 0.145702\n",
      "2149 Training accuracy = 0.92 Loss = 0.249138\n",
      "2150 Training accuracy = 0.89 Loss = 0.228831\n",
      "2151 Training accuracy = 0.87 Loss = 0.388075\n",
      "2152 Training accuracy = 0.9 Loss = 0.335059\n",
      "2153 Training accuracy = 0.92 Loss = 0.220109\n",
      "2154 Training accuracy = 0.94 Loss = 0.187537\n",
      "2155 Training accuracy = 0.95 Loss = 0.200446\n",
      "2156 Training accuracy = 0.92 Loss = 0.285658\n",
      "2157 Training accuracy = 0.95 Loss = 0.211483\n",
      "2158 Training accuracy = 0.92 Loss = 0.221657\n",
      "2159 Training accuracy = 0.95 Loss = 0.240853\n",
      "2160 Training accuracy = 0.88 Loss = 0.391222\n",
      "2161 Training accuracy = 0.93 Loss = 0.309814\n",
      "2162 Training accuracy = 0.87 Loss = 0.413025\n",
      "2163 Training accuracy = 0.94 Loss = 0.35331\n",
      "2164 Training accuracy = 0.95 Loss = 0.192105\n",
      "2165 Training accuracy = 0.97 Loss = 0.118448\n",
      "2166 Training accuracy = 0.95 Loss = 0.272595\n",
      "2167 Training accuracy = 0.91 Loss = 0.318171\n",
      "2168 Training accuracy = 0.9 Loss = 0.345674\n",
      "2169 Training accuracy = 0.94 Loss = 0.260039\n",
      "2170 Training accuracy = 0.93 Loss = 0.260526\n",
      "2171 Training accuracy = 0.89 Loss = 0.289349\n",
      "2172 Training accuracy = 0.91 Loss = 0.298193\n",
      "2173 Training accuracy = 0.95 Loss = 0.204928\n",
      "2174 Training accuracy = 0.95 Loss = 0.243257\n",
      "2175 Training accuracy = 0.86 Loss = 0.45057\n",
      "2176 Training accuracy = 0.92 Loss = 0.310715\n",
      "2177 Training accuracy = 0.9 Loss = 0.306404\n",
      "2178 Training accuracy = 0.96 Loss = 0.176709\n",
      "2179 Training accuracy = 0.87 Loss = 0.463046\n",
      "2180 Training accuracy = 0.9 Loss = 0.320208\n",
      "2181 Training accuracy = 0.9 Loss = 0.269797\n",
      "2182 Training accuracy = 0.92 Loss = 0.219882\n",
      "2183 Training accuracy = 0.92 Loss = 0.364133\n",
      "2184 Training accuracy = 0.94 Loss = 0.275833\n",
      "2185 Training accuracy = 0.95 Loss = 0.195425\n",
      "2186 Training accuracy = 0.9 Loss = 0.349655\n",
      "2187 Training accuracy = 0.91 Loss = 0.277754\n",
      "2188 Training accuracy = 0.9 Loss = 0.357413\n",
      "2189 Training accuracy = 0.95 Loss = 0.186048\n",
      "2190 Training accuracy = 0.92 Loss = 0.291669\n",
      "2191 Training accuracy = 0.93 Loss = 0.26173\n",
      "2192 Training accuracy = 0.89 Loss = 0.419068\n",
      "2193 Training accuracy = 0.89 Loss = 0.274511\n",
      "2194 Training accuracy = 0.87 Loss = 0.309969\n",
      "2195 Training accuracy = 0.92 Loss = 0.316293\n",
      "2196 Training accuracy = 0.93 Loss = 0.256296\n",
      "2197 Training accuracy = 0.9 Loss = 0.326615\n",
      "2198 Training accuracy = 0.89 Loss = 0.389586\n",
      "2199 Training accuracy = 0.92 Loss = 0.288588\n",
      "2200 Training accuracy = 0.89 Loss = 0.306231\n",
      "2201 Training accuracy = 0.9 Loss = 0.290563\n",
      "2202 Training accuracy = 0.94 Loss = 0.228831\n",
      "2203 Training accuracy = 0.91 Loss = 0.2803\n",
      "2204 Training accuracy = 0.9 Loss = 0.364555\n",
      "2205 Training accuracy = 0.92 Loss = 0.223464\n",
      "2206 Training accuracy = 0.96 Loss = 0.191647\n",
      "2207 Training accuracy = 0.94 Loss = 0.231395\n",
      "2208 Training accuracy = 0.93 Loss = 0.304959\n",
      "2209 Training accuracy = 0.94 Loss = 0.263077\n",
      "2210 Training accuracy = 0.9 Loss = 0.363032\n",
      "2211 Training accuracy = 0.93 Loss = 0.229731\n",
      "2212 Training accuracy = 0.95 Loss = 0.37713\n",
      "2213 Training accuracy = 0.92 Loss = 0.298575\n",
      "2214 Training accuracy = 0.91 Loss = 0.362386\n",
      "2215 Training accuracy = 0.88 Loss = 0.428073\n",
      "2216 Training accuracy = 0.88 Loss = 0.389807\n",
      "2217 Training accuracy = 0.9 Loss = 0.482407\n",
      "2218 Training accuracy = 0.96 Loss = 0.223132\n",
      "2219 Training accuracy = 0.94 Loss = 0.224337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220 Training accuracy = 0.94 Loss = 0.216095\n",
      "2221 Training accuracy = 0.92 Loss = 0.337534\n",
      "2222 Training accuracy = 0.9 Loss = 0.375944\n",
      "2223 Training accuracy = 0.9 Loss = 0.286949\n",
      "2224 Training accuracy = 0.93 Loss = 0.222339\n",
      "2225 Training accuracy = 0.94 Loss = 0.210977\n",
      "2226 Training accuracy = 0.91 Loss = 0.310795\n",
      "2227 Training accuracy = 0.95 Loss = 0.217153\n",
      "2228 Training accuracy = 0.93 Loss = 0.268278\n",
      "2229 Training accuracy = 0.96 Loss = 0.248618\n",
      "2230 Training accuracy = 0.92 Loss = 0.301286\n",
      "2231 Training accuracy = 0.93 Loss = 0.22795\n",
      "2232 Training accuracy = 0.94 Loss = 0.211118\n",
      "2233 Training accuracy = 0.89 Loss = 0.362181\n",
      "2234 Training accuracy = 0.92 Loss = 0.279724\n",
      "2235 Training accuracy = 0.92 Loss = 0.338487\n",
      "2236 Training accuracy = 0.95 Loss = 0.241709\n",
      "2237 Training accuracy = 0.92 Loss = 0.308902\n",
      "2238 Training accuracy = 0.92 Loss = 0.381265\n",
      "2239 Training accuracy = 0.92 Loss = 0.294514\n",
      "2240 Training accuracy = 0.87 Loss = 0.349505\n",
      "2241 Training accuracy = 0.88 Loss = 0.520084\n",
      "2242 Training accuracy = 0.91 Loss = 0.294706\n",
      "2243 Training accuracy = 0.94 Loss = 0.29262\n",
      "2244 Training accuracy = 0.93 Loss = 0.231301\n",
      "2245 Training accuracy = 0.88 Loss = 0.32208\n",
      "2246 Training accuracy = 0.9 Loss = 0.350233\n",
      "2247 Training accuracy = 0.94 Loss = 0.238914\n",
      "2248 Training accuracy = 0.95 Loss = 0.2289\n",
      "2249 Training accuracy = 0.86 Loss = 0.364143\n",
      "2250 Training accuracy = 0.88 Loss = 0.416971\n",
      "2251 Training accuracy = 0.95 Loss = 0.200463\n",
      "2252 Training accuracy = 0.9 Loss = 0.311321\n",
      "2253 Training accuracy = 0.91 Loss = 0.264559\n",
      "2254 Training accuracy = 0.91 Loss = 0.34822\n",
      "2255 Training accuracy = 0.89 Loss = 0.342723\n",
      "2256 Training accuracy = 0.95 Loss = 0.18086\n",
      "2257 Training accuracy = 0.91 Loss = 0.324406\n",
      "2258 Training accuracy = 0.98 Loss = 0.149054\n",
      "2259 Training accuracy = 0.91 Loss = 0.295978\n",
      "2260 Training accuracy = 0.97 Loss = 0.157372\n",
      "2261 Training accuracy = 0.93 Loss = 0.17667\n",
      "2262 Training accuracy = 0.92 Loss = 0.241964\n",
      "2263 Training accuracy = 0.91 Loss = 0.312923\n",
      "2264 Training accuracy = 0.94 Loss = 0.239885\n",
      "2265 Training accuracy = 0.93 Loss = 0.214828\n",
      "2266 Training accuracy = 0.94 Loss = 0.238881\n",
      "2267 Training accuracy = 0.94 Loss = 0.253489\n",
      "2268 Training accuracy = 0.96 Loss = 0.144852\n",
      "2269 Training accuracy = 0.9 Loss = 0.304341\n",
      "2270 Training accuracy = 0.93 Loss = 0.265706\n",
      "2271 Training accuracy = 0.96 Loss = 0.195407\n",
      "2272 Training accuracy = 0.93 Loss = 0.181696\n",
      "2273 Training accuracy = 0.94 Loss = 0.198224\n",
      "2274 Training accuracy = 0.89 Loss = 0.29369\n",
      "2275 Training accuracy = 0.94 Loss = 0.340556\n",
      "2276 Training accuracy = 0.92 Loss = 0.28373\n",
      "2277 Training accuracy = 0.91 Loss = 0.328947\n",
      "2278 Training accuracy = 0.91 Loss = 0.287208\n",
      "2279 Training accuracy = 0.94 Loss = 0.203083\n",
      "2280 Training accuracy = 0.93 Loss = 0.392691\n",
      "2281 Training accuracy = 0.93 Loss = 0.286032\n",
      "2282 Training accuracy = 0.89 Loss = 0.291806\n",
      "2283 Training accuracy = 0.94 Loss = 0.225621\n",
      "2284 Training accuracy = 0.88 Loss = 0.3317\n",
      "2285 Training accuracy = 0.94 Loss = 0.204233\n",
      "2286 Training accuracy = 0.97 Loss = 0.17401\n",
      "2287 Training accuracy = 0.95 Loss = 0.16691\n",
      "2288 Training accuracy = 0.93 Loss = 0.221257\n",
      "2289 Training accuracy = 0.95 Loss = 0.219236\n",
      "2290 Training accuracy = 0.91 Loss = 0.27571\n",
      "2291 Training accuracy = 0.93 Loss = 0.18241\n",
      "2292 Training accuracy = 0.93 Loss = 0.260428\n",
      "2293 Training accuracy = 0.91 Loss = 0.323006\n",
      "2294 Training accuracy = 0.92 Loss = 0.295741\n",
      "2295 Training accuracy = 0.94 Loss = 0.235317\n",
      "2296 Training accuracy = 0.96 Loss = 0.205683\n",
      "2297 Training accuracy = 0.91 Loss = 0.314272\n",
      "2298 Training accuracy = 0.95 Loss = 0.221236\n",
      "2299 Training accuracy = 0.92 Loss = 0.283381\n",
      "2300 Training accuracy = 0.92 Loss = 0.258924\n",
      "2301 Training accuracy = 0.96 Loss = 0.2477\n",
      "2302 Training accuracy = 0.93 Loss = 0.210754\n",
      "2303 Training accuracy = 0.92 Loss = 0.271318\n",
      "2304 Training accuracy = 0.95 Loss = 0.215177\n",
      "2305 Training accuracy = 0.92 Loss = 0.338355\n",
      "2306 Training accuracy = 0.91 Loss = 0.379942\n",
      "2307 Training accuracy = 0.93 Loss = 0.268873\n",
      "2308 Training accuracy = 0.95 Loss = 0.273455\n",
      "2309 Training accuracy = 0.89 Loss = 0.272898\n",
      "2310 Training accuracy = 0.92 Loss = 0.322168\n",
      "2311 Training accuracy = 0.94 Loss = 0.286821\n",
      "2312 Training accuracy = 0.97 Loss = 0.281836\n",
      "2313 Training accuracy = 0.94 Loss = 0.207231\n",
      "2314 Training accuracy = 0.89 Loss = 0.421388\n",
      "2315 Training accuracy = 0.93 Loss = 0.195772\n",
      "2316 Training accuracy = 0.89 Loss = 0.370347\n",
      "2317 Training accuracy = 0.95 Loss = 0.211709\n",
      "2318 Training accuracy = 0.92 Loss = 0.285312\n",
      "2319 Training accuracy = 0.93 Loss = 0.309219\n",
      "2320 Training accuracy = 0.93 Loss = 0.304681\n",
      "2321 Training accuracy = 0.94 Loss = 0.204015\n",
      "2322 Training accuracy = 0.89 Loss = 0.248248\n",
      "2323 Training accuracy = 0.91 Loss = 0.323121\n",
      "2324 Training accuracy = 0.9 Loss = 0.337975\n",
      "2325 Training accuracy = 0.91 Loss = 0.346962\n",
      "2326 Training accuracy = 0.89 Loss = 0.230238\n",
      "2327 Training accuracy = 0.85 Loss = 0.385193\n",
      "2328 Training accuracy = 0.95 Loss = 0.28152\n",
      "2329 Training accuracy = 0.92 Loss = 0.299177\n",
      "2330 Training accuracy = 0.95 Loss = 0.299725\n",
      "2331 Training accuracy = 0.9 Loss = 0.346701\n",
      "2332 Training accuracy = 0.93 Loss = 0.266556\n",
      "2333 Training accuracy = 0.93 Loss = 0.272848\n",
      "2334 Training accuracy = 0.93 Loss = 0.177718\n",
      "2335 Training accuracy = 0.92 Loss = 0.254997\n",
      "2336 Training accuracy = 0.95 Loss = 0.264002\n",
      "2337 Training accuracy = 0.87 Loss = 0.311465\n",
      "2338 Training accuracy = 0.89 Loss = 0.303724\n",
      "2339 Training accuracy = 0.95 Loss = 0.209215\n",
      "2340 Training accuracy = 0.93 Loss = 0.227814\n",
      "2341 Training accuracy = 0.93 Loss = 0.2642\n",
      "2342 Training accuracy = 0.95 Loss = 0.193927\n",
      "2343 Training accuracy = 0.89 Loss = 0.267389\n",
      "2344 Training accuracy = 0.85 Loss = 0.590683\n",
      "2345 Training accuracy = 0.89 Loss = 0.315734\n",
      "2346 Training accuracy = 0.93 Loss = 0.230886\n",
      "2347 Training accuracy = 0.9 Loss = 0.314118\n",
      "2348 Training accuracy = 0.94 Loss = 0.350984\n",
      "2349 Training accuracy = 0.91 Loss = 0.255195\n",
      "2350 Training accuracy = 0.99 Loss = 0.110255\n",
      "2351 Training accuracy = 0.92 Loss = 0.33135\n",
      "2352 Training accuracy = 0.88 Loss = 0.375073\n",
      "2353 Training accuracy = 0.91 Loss = 0.257462\n",
      "2354 Training accuracy = 0.95 Loss = 0.197964\n",
      "2355 Training accuracy = 0.95 Loss = 0.182472\n",
      "2356 Training accuracy = 0.94 Loss = 0.336591\n",
      "2357 Training accuracy = 0.91 Loss = 0.298143\n",
      "2358 Training accuracy = 0.96 Loss = 0.205018\n",
      "2359 Training accuracy = 0.96 Loss = 0.17482\n",
      "2360 Training accuracy = 0.91 Loss = 0.242676\n",
      "2361 Training accuracy = 0.91 Loss = 0.341734\n",
      "2362 Training accuracy = 0.88 Loss = 0.319602\n",
      "2363 Training accuracy = 0.89 Loss = 0.281847\n",
      "2364 Training accuracy = 0.94 Loss = 0.225697\n",
      "2365 Training accuracy = 0.93 Loss = 0.304552\n",
      "2366 Training accuracy = 0.95 Loss = 0.216789\n",
      "2367 Training accuracy = 0.93 Loss = 0.243244\n",
      "2368 Training accuracy = 0.92 Loss = 0.264059\n",
      "2369 Training accuracy = 0.94 Loss = 0.201707\n",
      "2370 Training accuracy = 0.92 Loss = 0.273984\n",
      "2371 Training accuracy = 0.95 Loss = 0.209455\n",
      "2372 Training accuracy = 0.92 Loss = 0.321545\n",
      "2373 Training accuracy = 0.92 Loss = 0.211348\n",
      "2374 Training accuracy = 0.92 Loss = 0.216809\n",
      "2375 Training accuracy = 0.93 Loss = 0.268432\n",
      "2376 Training accuracy = 0.94 Loss = 0.210131\n",
      "2377 Training accuracy = 0.93 Loss = 0.327645\n",
      "2378 Training accuracy = 0.9 Loss = 0.387527\n",
      "2379 Training accuracy = 0.92 Loss = 0.313363\n",
      "2380 Training accuracy = 0.92 Loss = 0.257636\n",
      "2381 Training accuracy = 0.93 Loss = 0.247083\n",
      "2382 Training accuracy = 0.95 Loss = 0.264628\n",
      "2383 Training accuracy = 0.92 Loss = 0.268691\n",
      "2384 Training accuracy = 0.93 Loss = 0.19877\n",
      "2385 Training accuracy = 0.92 Loss = 0.252921\n",
      "2386 Training accuracy = 0.91 Loss = 0.33657\n",
      "2387 Training accuracy = 0.95 Loss = 0.323478\n",
      "2388 Training accuracy = 0.96 Loss = 0.178785\n",
      "2389 Training accuracy = 0.88 Loss = 0.379066\n",
      "2390 Training accuracy = 0.91 Loss = 0.39029\n",
      "2391 Training accuracy = 0.98 Loss = 0.123281\n",
      "2392 Training accuracy = 0.91 Loss = 0.300758\n",
      "2393 Training accuracy = 0.93 Loss = 0.261463\n",
      "2394 Training accuracy = 0.96 Loss = 0.201885\n",
      "2395 Training accuracy = 0.91 Loss = 0.37285\n",
      "2396 Training accuracy = 0.92 Loss = 0.262422\n",
      "2397 Training accuracy = 0.9 Loss = 0.478476\n",
      "2398 Training accuracy = 0.9 Loss = 0.277512\n",
      "2399 Training accuracy = 0.93 Loss = 0.296432\n",
      "2400 Training accuracy = 0.86 Loss = 0.315633\n",
      "2401 Training accuracy = 0.91 Loss = 0.243223\n",
      "2402 Training accuracy = 0.95 Loss = 0.1803\n",
      "2403 Training accuracy = 0.92 Loss = 0.282905\n",
      "2404 Training accuracy = 0.89 Loss = 0.269706\n",
      "2405 Training accuracy = 0.92 Loss = 0.383796\n",
      "2406 Training accuracy = 0.88 Loss = 0.367617\n",
      "2407 Training accuracy = 0.96 Loss = 0.166544\n",
      "2408 Training accuracy = 0.94 Loss = 0.261818\n",
      "2409 Training accuracy = 0.86 Loss = 0.420622\n",
      "2410 Training accuracy = 0.83 Loss = 0.547796\n",
      "2411 Training accuracy = 0.94 Loss = 0.267968\n",
      "2412 Training accuracy = 0.91 Loss = 0.29454\n",
      "2413 Training accuracy = 0.86 Loss = 0.490673\n",
      "2414 Training accuracy = 0.94 Loss = 0.260813\n",
      "2415 Training accuracy = 0.94 Loss = 0.291743\n",
      "2416 Training accuracy = 0.89 Loss = 0.30921\n",
      "2417 Training accuracy = 0.94 Loss = 0.273447\n",
      "2418 Training accuracy = 0.89 Loss = 0.422555\n",
      "2419 Training accuracy = 0.93 Loss = 0.263786\n",
      "2420 Training accuracy = 0.9 Loss = 0.342449\n",
      "2421 Training accuracy = 0.92 Loss = 0.281604\n",
      "2422 Training accuracy = 0.95 Loss = 0.218745\n",
      "2423 Training accuracy = 0.92 Loss = 0.298955\n",
      "2424 Training accuracy = 0.92 Loss = 0.283294\n",
      "2425 Training accuracy = 0.91 Loss = 0.320621\n",
      "2426 Training accuracy = 0.92 Loss = 0.325454\n",
      "2427 Training accuracy = 0.93 Loss = 0.251496\n",
      "2428 Training accuracy = 0.95 Loss = 0.239942\n",
      "2429 Training accuracy = 0.93 Loss = 0.229959\n",
      "2430 Training accuracy = 0.88 Loss = 0.508466\n",
      "2431 Training accuracy = 0.95 Loss = 0.174549\n",
      "2432 Training accuracy = 0.9 Loss = 0.318765\n",
      "2433 Training accuracy = 0.9 Loss = 0.373706\n",
      "2434 Training accuracy = 0.94 Loss = 0.27922\n",
      "2435 Training accuracy = 0.87 Loss = 0.299836\n",
      "2436 Training accuracy = 0.91 Loss = 0.409866\n",
      "2437 Training accuracy = 0.93 Loss = 0.276143\n",
      "2438 Training accuracy = 0.91 Loss = 0.32329\n",
      "2439 Training accuracy = 0.88 Loss = 0.319939\n",
      "2440 Training accuracy = 0.92 Loss = 0.221559\n",
      "2441 Training accuracy = 0.92 Loss = 0.262187\n",
      "2442 Training accuracy = 0.96 Loss = 0.178173\n",
      "2443 Training accuracy = 0.87 Loss = 0.448627\n",
      "2444 Training accuracy = 0.9 Loss = 0.33913\n",
      "2445 Training accuracy = 0.9 Loss = 0.313194\n",
      "2446 Training accuracy = 0.91 Loss = 0.266142\n",
      "2447 Training accuracy = 0.88 Loss = 0.376942\n",
      "2448 Training accuracy = 0.95 Loss = 0.231434\n",
      "2449 Training accuracy = 0.93 Loss = 0.236464\n",
      "2450 Training accuracy = 0.91 Loss = 0.373678\n",
      "2451 Training accuracy = 0.89 Loss = 0.320414\n",
      "2452 Training accuracy = 0.94 Loss = 0.316432\n",
      "2453 Training accuracy = 0.92 Loss = 0.393865\n",
      "2454 Training accuracy = 0.91 Loss = 0.259558\n",
      "2455 Training accuracy = 0.87 Loss = 0.338876\n",
      "2456 Training accuracy = 0.93 Loss = 0.235212\n",
      "2457 Training accuracy = 0.88 Loss = 0.39942\n",
      "2458 Training accuracy = 0.87 Loss = 0.665935\n",
      "2459 Training accuracy = 0.94 Loss = 0.193911\n",
      "2460 Training accuracy = 0.93 Loss = 0.283151\n",
      "2461 Training accuracy = 0.92 Loss = 0.29884\n",
      "2462 Training accuracy = 0.96 Loss = 0.170857\n",
      "2463 Training accuracy = 0.95 Loss = 0.156381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2464 Training accuracy = 0.86 Loss = 0.573807\n",
      "2465 Training accuracy = 0.98 Loss = 0.146857\n",
      "2466 Training accuracy = 0.95 Loss = 0.18011\n",
      "2467 Training accuracy = 0.92 Loss = 0.225817\n",
      "2468 Training accuracy = 0.91 Loss = 0.319754\n",
      "2469 Training accuracy = 0.93 Loss = 0.296507\n",
      "2470 Training accuracy = 0.9 Loss = 0.316687\n",
      "2471 Training accuracy = 0.92 Loss = 0.335294\n",
      "2472 Training accuracy = 0.92 Loss = 0.257339\n",
      "2473 Training accuracy = 0.82 Loss = 0.389523\n",
      "2474 Training accuracy = 0.92 Loss = 0.245559\n",
      "2475 Training accuracy = 0.91 Loss = 0.315709\n",
      "2476 Training accuracy = 0.91 Loss = 0.326495\n",
      "2477 Training accuracy = 0.9 Loss = 0.254696\n",
      "2478 Training accuracy = 0.89 Loss = 0.319176\n",
      "2479 Training accuracy = 0.91 Loss = 0.243844\n",
      "2480 Training accuracy = 0.86 Loss = 0.335098\n",
      "2481 Training accuracy = 0.96 Loss = 0.157041\n",
      "2482 Training accuracy = 0.92 Loss = 0.218516\n",
      "2483 Training accuracy = 0.89 Loss = 0.289517\n",
      "2484 Training accuracy = 0.93 Loss = 0.302033\n",
      "2485 Training accuracy = 0.97 Loss = 0.143209\n",
      "2486 Training accuracy = 0.9 Loss = 0.359011\n",
      "2487 Training accuracy = 0.94 Loss = 0.252261\n",
      "2488 Training accuracy = 0.91 Loss = 0.308792\n",
      "2489 Training accuracy = 0.93 Loss = 0.354581\n",
      "2490 Training accuracy = 0.92 Loss = 0.190887\n",
      "2491 Training accuracy = 0.89 Loss = 0.392202\n",
      "2492 Training accuracy = 0.95 Loss = 0.148535\n",
      "2493 Training accuracy = 0.91 Loss = 0.344707\n",
      "2494 Training accuracy = 0.88 Loss = 0.304944\n",
      "2495 Training accuracy = 0.93 Loss = 0.318571\n",
      "2496 Training accuracy = 0.91 Loss = 0.305519\n",
      "2497 Training accuracy = 0.91 Loss = 0.281217\n",
      "2498 Training accuracy = 0.9 Loss = 0.374704\n",
      "2499 Training accuracy = 0.91 Loss = 0.348765\n",
      "2500 Training accuracy = 0.89 Loss = 0.324673\n",
      "2501 Training accuracy = 0.93 Loss = 0.23164\n",
      "2502 Training accuracy = 0.92 Loss = 0.230776\n",
      "2503 Training accuracy = 0.93 Loss = 0.224085\n",
      "2504 Training accuracy = 0.93 Loss = 0.263162\n",
      "2505 Training accuracy = 0.92 Loss = 0.331334\n",
      "2506 Training accuracy = 0.95 Loss = 0.238482\n",
      "2507 Training accuracy = 0.93 Loss = 0.194489\n",
      "2508 Training accuracy = 0.93 Loss = 0.252052\n",
      "2509 Training accuracy = 0.95 Loss = 0.192928\n",
      "2510 Training accuracy = 0.9 Loss = 0.38916\n",
      "2511 Training accuracy = 0.94 Loss = 0.22998\n",
      "2512 Training accuracy = 0.91 Loss = 0.357459\n",
      "2513 Training accuracy = 0.91 Loss = 0.389095\n",
      "2514 Training accuracy = 0.89 Loss = 0.304675\n",
      "2515 Training accuracy = 0.94 Loss = 0.170492\n",
      "2516 Training accuracy = 0.95 Loss = 0.166588\n",
      "2517 Training accuracy = 0.88 Loss = 0.358302\n",
      "2518 Training accuracy = 0.94 Loss = 0.215212\n",
      "2519 Training accuracy = 0.92 Loss = 0.275927\n",
      "2520 Training accuracy = 0.89 Loss = 0.357497\n",
      "2521 Training accuracy = 0.94 Loss = 0.321256\n",
      "2522 Training accuracy = 0.91 Loss = 0.257603\n",
      "2523 Training accuracy = 0.92 Loss = 0.226124\n",
      "2524 Training accuracy = 0.89 Loss = 0.369367\n",
      "2525 Training accuracy = 0.92 Loss = 0.290309\n",
      "2526 Training accuracy = 0.94 Loss = 0.193447\n",
      "2527 Training accuracy = 0.94 Loss = 0.322246\n",
      "2528 Training accuracy = 0.96 Loss = 0.307428\n",
      "2529 Training accuracy = 0.93 Loss = 0.240634\n",
      "2530 Training accuracy = 0.92 Loss = 0.22967\n",
      "2531 Training accuracy = 0.91 Loss = 0.249097\n",
      "2532 Training accuracy = 0.87 Loss = 0.420591\n",
      "2533 Training accuracy = 0.87 Loss = 0.32715\n",
      "2534 Training accuracy = 0.95 Loss = 0.199544\n",
      "2535 Training accuracy = 0.94 Loss = 0.234282\n",
      "2536 Training accuracy = 0.96 Loss = 0.206468\n",
      "2537 Training accuracy = 0.87 Loss = 0.356553\n",
      "2538 Training accuracy = 0.92 Loss = 0.272914\n",
      "2539 Training accuracy = 0.9 Loss = 0.25631\n",
      "2540 Training accuracy = 0.89 Loss = 0.320302\n",
      "2541 Training accuracy = 0.91 Loss = 0.320483\n",
      "2542 Training accuracy = 0.91 Loss = 0.312375\n",
      "2543 Training accuracy = 0.94 Loss = 0.198311\n",
      "2544 Training accuracy = 0.96 Loss = 0.191841\n",
      "2545 Training accuracy = 0.93 Loss = 0.269082\n",
      "2546 Training accuracy = 0.9 Loss = 0.362371\n",
      "2547 Training accuracy = 0.94 Loss = 0.287676\n",
      "2548 Training accuracy = 0.94 Loss = 0.255919\n",
      "2549 Training accuracy = 0.89 Loss = 0.271498\n",
      "2550 Training accuracy = 0.95 Loss = 0.208883\n",
      "2551 Training accuracy = 0.92 Loss = 0.206462\n",
      "2552 Training accuracy = 0.9 Loss = 0.336709\n",
      "2553 Training accuracy = 0.92 Loss = 0.32797\n",
      "2554 Training accuracy = 0.92 Loss = 0.361785\n",
      "2555 Training accuracy = 0.97 Loss = 0.16942\n",
      "2556 Training accuracy = 0.95 Loss = 0.197648\n",
      "2557 Training accuracy = 0.92 Loss = 0.243878\n",
      "2558 Training accuracy = 0.9 Loss = 0.299153\n",
      "2559 Training accuracy = 0.85 Loss = 0.469655\n",
      "2560 Training accuracy = 0.95 Loss = 0.276659\n",
      "2561 Training accuracy = 0.94 Loss = 0.18244\n",
      "2562 Training accuracy = 0.91 Loss = 0.381169\n",
      "2563 Training accuracy = 0.91 Loss = 0.242954\n",
      "2564 Training accuracy = 0.89 Loss = 0.585419\n",
      "2565 Training accuracy = 0.94 Loss = 0.231495\n",
      "2566 Training accuracy = 0.91 Loss = 0.292458\n",
      "2567 Training accuracy = 0.91 Loss = 0.338188\n",
      "2568 Training accuracy = 0.9 Loss = 0.302423\n",
      "2569 Training accuracy = 0.94 Loss = 0.243985\n",
      "2570 Training accuracy = 0.98 Loss = 0.117965\n",
      "2571 Training accuracy = 0.95 Loss = 0.203459\n",
      "2572 Training accuracy = 0.93 Loss = 0.240055\n",
      "2573 Training accuracy = 0.97 Loss = 0.215026\n",
      "2574 Training accuracy = 0.93 Loss = 0.271153\n",
      "2575 Training accuracy = 0.89 Loss = 0.363656\n",
      "2576 Training accuracy = 0.94 Loss = 0.168502\n",
      "2577 Training accuracy = 0.94 Loss = 0.24735\n",
      "2578 Training accuracy = 0.93 Loss = 0.295211\n",
      "2579 Training accuracy = 0.92 Loss = 0.241164\n",
      "2580 Training accuracy = 0.89 Loss = 0.336757\n",
      "2581 Training accuracy = 0.94 Loss = 0.188293\n",
      "2582 Training accuracy = 0.92 Loss = 0.265088\n",
      "2583 Training accuracy = 0.91 Loss = 0.385719\n",
      "2584 Training accuracy = 0.94 Loss = 0.230476\n",
      "2585 Training accuracy = 0.9 Loss = 0.405844\n",
      "2586 Training accuracy = 0.9 Loss = 0.374883\n",
      "2587 Training accuracy = 0.9 Loss = 0.314209\n",
      "2588 Training accuracy = 0.94 Loss = 0.295538\n",
      "2589 Training accuracy = 0.92 Loss = 0.229653\n",
      "2590 Training accuracy = 0.93 Loss = 0.206249\n",
      "2591 Training accuracy = 0.91 Loss = 0.302391\n",
      "2592 Training accuracy = 0.92 Loss = 0.281172\n",
      "2593 Training accuracy = 0.92 Loss = 0.241213\n",
      "2594 Training accuracy = 0.93 Loss = 0.192479\n",
      "2595 Training accuracy = 0.94 Loss = 0.183201\n",
      "2596 Training accuracy = 0.93 Loss = 0.344192\n",
      "2597 Training accuracy = 0.91 Loss = 0.31957\n",
      "2598 Training accuracy = 0.95 Loss = 0.224747\n",
      "2599 Training accuracy = 0.93 Loss = 0.290521\n",
      "2600 Training accuracy = 0.87 Loss = 0.350916\n",
      "2601 Training accuracy = 0.94 Loss = 0.19656\n",
      "2602 Training accuracy = 0.95 Loss = 0.329653\n",
      "2603 Training accuracy = 0.92 Loss = 0.275382\n",
      "2604 Training accuracy = 0.92 Loss = 0.26895\n",
      "2605 Training accuracy = 0.94 Loss = 0.285464\n",
      "2606 Training accuracy = 0.92 Loss = 0.307651\n",
      "2607 Training accuracy = 0.88 Loss = 0.344994\n",
      "2608 Training accuracy = 0.88 Loss = 0.375697\n",
      "2609 Training accuracy = 0.88 Loss = 0.395897\n",
      "2610 Training accuracy = 0.95 Loss = 0.199726\n",
      "2611 Training accuracy = 0.86 Loss = 0.476823\n",
      "2612 Training accuracy = 0.94 Loss = 0.268074\n",
      "2613 Training accuracy = 0.94 Loss = 0.253119\n",
      "2614 Training accuracy = 0.92 Loss = 0.303284\n",
      "2615 Training accuracy = 0.93 Loss = 0.352388\n",
      "2616 Training accuracy = 0.91 Loss = 0.304103\n",
      "2617 Training accuracy = 0.96 Loss = 0.185025\n",
      "2618 Training accuracy = 0.94 Loss = 0.290057\n",
      "2619 Training accuracy = 0.91 Loss = 0.303693\n",
      "2620 Training accuracy = 0.93 Loss = 0.195215\n",
      "2621 Training accuracy = 0.92 Loss = 0.279668\n",
      "2622 Training accuracy = 0.95 Loss = 0.247585\n",
      "2623 Training accuracy = 0.88 Loss = 0.267284\n",
      "2624 Training accuracy = 0.87 Loss = 0.403165\n",
      "2625 Training accuracy = 0.91 Loss = 0.219387\n",
      "2626 Training accuracy = 0.94 Loss = 0.334813\n",
      "2627 Training accuracy = 0.92 Loss = 0.304365\n",
      "2628 Training accuracy = 0.92 Loss = 0.205265\n",
      "2629 Training accuracy = 0.89 Loss = 0.280335\n",
      "2630 Training accuracy = 0.91 Loss = 0.268017\n",
      "2631 Training accuracy = 0.85 Loss = 0.398566\n",
      "2632 Training accuracy = 0.92 Loss = 0.254279\n",
      "2633 Training accuracy = 0.89 Loss = 0.390557\n",
      "2634 Training accuracy = 0.93 Loss = 0.246045\n",
      "2635 Training accuracy = 0.94 Loss = 0.209857\n",
      "2636 Training accuracy = 0.93 Loss = 0.237532\n",
      "2637 Training accuracy = 0.9 Loss = 0.282282\n",
      "2638 Training accuracy = 0.95 Loss = 0.20436\n",
      "2639 Training accuracy = 0.9 Loss = 0.317862\n",
      "2640 Training accuracy = 0.92 Loss = 0.280343\n",
      "2641 Training accuracy = 0.91 Loss = 0.242286\n",
      "2642 Training accuracy = 0.91 Loss = 0.327302\n",
      "2643 Training accuracy = 0.91 Loss = 0.325907\n",
      "2644 Training accuracy = 0.89 Loss = 0.327699\n",
      "2645 Training accuracy = 0.96 Loss = 0.21506\n",
      "2646 Training accuracy = 0.92 Loss = 0.276012\n",
      "2647 Training accuracy = 0.94 Loss = 0.268049\n",
      "2648 Training accuracy = 0.92 Loss = 0.34426\n",
      "2649 Training accuracy = 0.9 Loss = 0.372912\n",
      "2650 Training accuracy = 0.89 Loss = 0.30136\n",
      "2651 Training accuracy = 0.92 Loss = 0.233279\n",
      "2652 Training accuracy = 0.96 Loss = 0.230435\n",
      "2653 Training accuracy = 0.94 Loss = 0.197266\n",
      "2654 Training accuracy = 0.9 Loss = 0.308273\n",
      "2655 Training accuracy = 0.93 Loss = 0.283611\n",
      "2656 Training accuracy = 0.93 Loss = 0.255308\n",
      "2657 Training accuracy = 0.89 Loss = 0.502674\n",
      "2658 Training accuracy = 0.91 Loss = 0.334362\n",
      "2659 Training accuracy = 0.89 Loss = 0.349816\n",
      "2660 Training accuracy = 0.9 Loss = 0.302969\n",
      "2661 Training accuracy = 0.9 Loss = 0.291539\n",
      "2662 Training accuracy = 0.89 Loss = 0.526689\n",
      "2663 Training accuracy = 0.89 Loss = 0.459166\n",
      "2664 Training accuracy = 0.92 Loss = 0.229657\n",
      "2665 Training accuracy = 0.94 Loss = 0.201628\n",
      "2666 Training accuracy = 0.94 Loss = 0.224777\n",
      "2667 Training accuracy = 0.95 Loss = 0.205766\n",
      "2668 Training accuracy = 0.92 Loss = 0.2695\n",
      "2669 Training accuracy = 0.9 Loss = 0.243831\n",
      "2670 Training accuracy = 0.94 Loss = 0.230061\n",
      "2671 Training accuracy = 0.93 Loss = 0.178421\n",
      "2672 Training accuracy = 0.94 Loss = 0.272474\n",
      "2673 Training accuracy = 0.88 Loss = 0.40993\n",
      "2674 Training accuracy = 0.91 Loss = 0.235501\n",
      "2675 Training accuracy = 0.92 Loss = 0.296923\n",
      "2676 Training accuracy = 0.9 Loss = 0.378053\n",
      "2677 Training accuracy = 0.87 Loss = 0.440206\n",
      "2678 Training accuracy = 0.93 Loss = 0.244222\n",
      "2679 Training accuracy = 0.97 Loss = 0.131257\n",
      "2680 Training accuracy = 0.94 Loss = 0.176443\n",
      "2681 Training accuracy = 0.94 Loss = 0.341951\n",
      "2682 Training accuracy = 0.88 Loss = 0.399292\n",
      "2683 Training accuracy = 0.92 Loss = 0.215694\n",
      "2684 Training accuracy = 0.94 Loss = 0.228293\n",
      "2685 Training accuracy = 0.9 Loss = 0.356651\n",
      "2686 Training accuracy = 0.92 Loss = 0.314692\n",
      "2687 Training accuracy = 0.92 Loss = 0.273309\n",
      "2688 Training accuracy = 0.9 Loss = 0.392748\n",
      "2689 Training accuracy = 0.87 Loss = 0.546343\n",
      "2690 Training accuracy = 0.93 Loss = 0.20119\n",
      "2691 Training accuracy = 0.88 Loss = 0.382236\n",
      "2692 Training accuracy = 0.91 Loss = 0.289109\n",
      "2693 Training accuracy = 0.92 Loss = 0.232145\n",
      "2694 Training accuracy = 0.86 Loss = 0.432304\n",
      "2695 Training accuracy = 0.94 Loss = 0.284653\n",
      "2696 Training accuracy = 0.94 Loss = 0.168389\n",
      "2697 Training accuracy = 0.93 Loss = 0.321378\n",
      "2698 Training accuracy = 0.91 Loss = 0.43741\n",
      "2699 Training accuracy = 0.93 Loss = 0.279195\n",
      "2700 Training accuracy = 0.89 Loss = 0.326966\n",
      "2701 Training accuracy = 0.92 Loss = 0.317462\n",
      "2702 Training accuracy = 0.88 Loss = 0.281113\n",
      "2703 Training accuracy = 0.93 Loss = 0.244394\n",
      "2704 Training accuracy = 0.96 Loss = 0.192897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705 Training accuracy = 0.92 Loss = 0.272609\n",
      "2706 Training accuracy = 0.95 Loss = 0.267334\n",
      "2707 Training accuracy = 0.88 Loss = 0.457081\n",
      "2708 Training accuracy = 0.91 Loss = 0.290478\n",
      "2709 Training accuracy = 0.93 Loss = 0.305678\n",
      "2710 Training accuracy = 0.89 Loss = 0.355908\n",
      "2711 Training accuracy = 0.87 Loss = 0.474404\n",
      "2712 Training accuracy = 0.97 Loss = 0.155136\n",
      "2713 Training accuracy = 0.95 Loss = 0.26039\n",
      "2714 Training accuracy = 0.89 Loss = 0.414692\n",
      "2715 Training accuracy = 0.91 Loss = 0.293336\n",
      "2716 Training accuracy = 0.97 Loss = 0.180935\n",
      "2717 Training accuracy = 0.9 Loss = 0.26405\n",
      "2718 Training accuracy = 0.9 Loss = 0.26959\n",
      "2719 Training accuracy = 0.92 Loss = 0.321051\n",
      "2720 Training accuracy = 0.9 Loss = 0.345909\n",
      "2721 Training accuracy = 0.93 Loss = 0.234142\n",
      "2722 Training accuracy = 0.94 Loss = 0.258734\n",
      "2723 Training accuracy = 0.94 Loss = 0.243489\n",
      "2724 Training accuracy = 0.89 Loss = 0.324677\n",
      "2725 Training accuracy = 0.93 Loss = 0.273559\n",
      "2726 Training accuracy = 0.9 Loss = 0.369439\n",
      "2727 Training accuracy = 0.95 Loss = 0.276834\n",
      "2728 Training accuracy = 0.88 Loss = 0.3711\n",
      "2729 Training accuracy = 0.92 Loss = 0.297432\n",
      "2730 Training accuracy = 0.97 Loss = 0.157247\n",
      "2731 Training accuracy = 0.91 Loss = 0.38288\n",
      "2732 Training accuracy = 0.92 Loss = 0.3088\n",
      "2733 Training accuracy = 0.93 Loss = 0.271749\n",
      "2734 Training accuracy = 0.9 Loss = 0.326504\n",
      "2735 Training accuracy = 0.86 Loss = 0.499659\n",
      "2736 Training accuracy = 0.92 Loss = 0.332703\n",
      "2737 Training accuracy = 0.91 Loss = 0.410798\n",
      "2738 Training accuracy = 0.88 Loss = 0.25877\n",
      "2739 Training accuracy = 0.93 Loss = 0.355763\n",
      "2740 Training accuracy = 0.95 Loss = 0.22265\n",
      "2741 Training accuracy = 0.92 Loss = 0.30173\n",
      "2742 Training accuracy = 0.91 Loss = 0.444907\n",
      "2743 Training accuracy = 0.94 Loss = 0.235587\n",
      "2744 Training accuracy = 0.92 Loss = 0.365049\n",
      "2745 Training accuracy = 0.95 Loss = 0.222298\n",
      "2746 Training accuracy = 0.94 Loss = 0.246914\n",
      "2747 Training accuracy = 0.93 Loss = 0.350723\n",
      "2748 Training accuracy = 0.9 Loss = 0.387043\n",
      "2749 Training accuracy = 0.93 Loss = 0.302421\n",
      "2750 Training accuracy = 0.95 Loss = 0.211798\n",
      "2751 Training accuracy = 0.93 Loss = 0.259723\n",
      "2752 Training accuracy = 0.95 Loss = 0.183592\n",
      "2753 Training accuracy = 0.92 Loss = 0.248529\n",
      "2754 Training accuracy = 0.91 Loss = 0.246234\n",
      "2755 Training accuracy = 0.91 Loss = 0.274162\n",
      "2756 Training accuracy = 0.9 Loss = 0.320309\n",
      "2757 Training accuracy = 0.92 Loss = 0.321264\n",
      "2758 Training accuracy = 0.93 Loss = 0.288842\n",
      "2759 Training accuracy = 0.95 Loss = 0.204913\n",
      "2760 Training accuracy = 0.96 Loss = 0.176056\n",
      "2761 Training accuracy = 0.89 Loss = 0.292038\n",
      "2762 Training accuracy = 0.9 Loss = 0.394932\n",
      "2763 Training accuracy = 0.95 Loss = 0.244737\n",
      "2764 Training accuracy = 0.93 Loss = 0.263202\n",
      "2765 Training accuracy = 0.95 Loss = 0.211472\n",
      "2766 Training accuracy = 0.97 Loss = 0.153863\n",
      "2767 Training accuracy = 0.93 Loss = 0.25473\n",
      "2768 Training accuracy = 0.92 Loss = 0.284681\n",
      "2769 Training accuracy = 0.98 Loss = 0.138629\n",
      "2770 Training accuracy = 0.92 Loss = 0.293998\n",
      "2771 Training accuracy = 0.91 Loss = 0.229025\n",
      "2772 Training accuracy = 0.97 Loss = 0.188964\n",
      "2773 Training accuracy = 0.94 Loss = 0.233382\n",
      "2774 Training accuracy = 0.93 Loss = 0.293246\n",
      "2775 Training accuracy = 0.92 Loss = 0.315433\n",
      "2776 Training accuracy = 0.95 Loss = 0.322401\n",
      "2777 Training accuracy = 0.93 Loss = 0.228229\n",
      "2778 Training accuracy = 0.9 Loss = 0.35985\n",
      "2779 Training accuracy = 0.9 Loss = 0.317864\n",
      "2780 Training accuracy = 0.88 Loss = 0.379663\n",
      "2781 Training accuracy = 0.92 Loss = 0.270594\n",
      "2782 Training accuracy = 0.91 Loss = 0.198616\n",
      "2783 Training accuracy = 0.95 Loss = 0.270055\n",
      "2784 Training accuracy = 0.9 Loss = 0.28207\n",
      "2785 Training accuracy = 0.91 Loss = 0.351074\n",
      "2786 Training accuracy = 0.96 Loss = 0.167479\n",
      "2787 Training accuracy = 0.89 Loss = 0.31834\n",
      "2788 Training accuracy = 0.89 Loss = 0.418979\n",
      "2789 Training accuracy = 0.92 Loss = 0.239035\n",
      "2790 Training accuracy = 0.93 Loss = 0.228394\n",
      "2791 Training accuracy = 0.91 Loss = 0.371625\n",
      "2792 Training accuracy = 0.93 Loss = 0.206142\n",
      "2793 Training accuracy = 0.95 Loss = 0.164146\n",
      "2794 Training accuracy = 0.91 Loss = 0.344492\n",
      "2795 Training accuracy = 0.94 Loss = 0.223599\n",
      "2796 Training accuracy = 0.92 Loss = 0.308302\n",
      "2797 Training accuracy = 0.91 Loss = 0.240222\n",
      "2798 Training accuracy = 0.93 Loss = 0.185772\n",
      "2799 Training accuracy = 0.94 Loss = 0.178982\n",
      "2800 Training accuracy = 0.95 Loss = 0.158875\n",
      "2801 Training accuracy = 0.92 Loss = 0.290113\n",
      "2802 Training accuracy = 0.95 Loss = 0.20604\n",
      "2803 Training accuracy = 0.9 Loss = 0.384502\n",
      "2804 Training accuracy = 0.93 Loss = 0.306966\n",
      "2805 Training accuracy = 0.87 Loss = 0.393398\n",
      "2806 Training accuracy = 0.95 Loss = 0.194548\n",
      "2807 Training accuracy = 0.93 Loss = 0.227416\n",
      "2808 Training accuracy = 0.91 Loss = 0.281637\n",
      "2809 Training accuracy = 0.9 Loss = 0.344917\n",
      "2810 Training accuracy = 0.91 Loss = 0.422595\n",
      "2811 Training accuracy = 0.96 Loss = 0.175354\n",
      "2812 Training accuracy = 0.9 Loss = 0.419873\n",
      "2813 Training accuracy = 0.9 Loss = 0.345651\n",
      "2814 Training accuracy = 0.91 Loss = 0.254862\n",
      "2815 Training accuracy = 0.86 Loss = 0.428068\n",
      "2816 Training accuracy = 0.93 Loss = 0.227755\n",
      "2817 Training accuracy = 0.91 Loss = 0.365708\n",
      "2818 Training accuracy = 0.97 Loss = 0.184547\n",
      "2819 Training accuracy = 0.89 Loss = 0.396072\n",
      "2820 Training accuracy = 0.96 Loss = 0.190813\n",
      "2821 Training accuracy = 0.9 Loss = 0.259075\n",
      "2822 Training accuracy = 0.92 Loss = 0.20355\n",
      "2823 Training accuracy = 0.95 Loss = 0.166587\n",
      "2824 Training accuracy = 0.94 Loss = 0.303311\n",
      "2825 Training accuracy = 0.92 Loss = 0.335994\n",
      "2826 Training accuracy = 0.89 Loss = 0.378425\n",
      "2827 Training accuracy = 0.93 Loss = 0.223996\n",
      "2828 Training accuracy = 0.95 Loss = 0.198371\n",
      "2829 Training accuracy = 0.93 Loss = 0.246195\n",
      "2830 Training accuracy = 0.94 Loss = 0.210751\n",
      "2831 Training accuracy = 0.85 Loss = 0.490181\n",
      "2832 Training accuracy = 0.87 Loss = 0.363544\n",
      "2833 Training accuracy = 0.93 Loss = 0.247628\n",
      "2834 Training accuracy = 0.93 Loss = 0.241809\n",
      "2835 Training accuracy = 0.94 Loss = 0.182345\n",
      "2836 Training accuracy = 0.9 Loss = 0.333396\n",
      "2837 Training accuracy = 0.93 Loss = 0.29849\n",
      "2838 Training accuracy = 0.94 Loss = 0.208923\n",
      "2839 Training accuracy = 0.96 Loss = 0.198398\n",
      "2840 Training accuracy = 0.87 Loss = 0.428951\n",
      "2841 Training accuracy = 0.91 Loss = 0.31298\n",
      "2842 Training accuracy = 0.9 Loss = 0.333078\n",
      "2843 Training accuracy = 0.93 Loss = 0.446053\n",
      "2844 Training accuracy = 0.91 Loss = 0.252668\n",
      "2845 Training accuracy = 0.92 Loss = 0.227097\n",
      "2846 Training accuracy = 0.88 Loss = 0.343516\n",
      "2847 Training accuracy = 0.95 Loss = 0.251223\n",
      "2848 Training accuracy = 0.95 Loss = 0.192854\n",
      "2849 Training accuracy = 0.86 Loss = 0.523705\n",
      "2850 Training accuracy = 0.94 Loss = 0.202734\n",
      "2851 Training accuracy = 0.93 Loss = 0.205347\n",
      "2852 Training accuracy = 0.97 Loss = 0.149815\n",
      "2853 Training accuracy = 0.92 Loss = 0.269453\n",
      "2854 Training accuracy = 0.89 Loss = 0.35228\n",
      "2855 Training accuracy = 0.91 Loss = 0.360241\n",
      "2856 Training accuracy = 0.93 Loss = 0.211757\n",
      "2857 Training accuracy = 0.92 Loss = 0.313537\n",
      "2858 Training accuracy = 0.92 Loss = 0.259602\n",
      "2859 Training accuracy = 0.91 Loss = 0.341174\n",
      "2860 Training accuracy = 0.9 Loss = 0.283424\n",
      "2861 Training accuracy = 0.92 Loss = 0.237896\n",
      "2862 Training accuracy = 0.9 Loss = 0.35303\n",
      "2863 Training accuracy = 0.92 Loss = 0.349429\n",
      "2864 Training accuracy = 0.91 Loss = 0.300061\n",
      "2865 Training accuracy = 0.89 Loss = 0.274387\n",
      "2866 Training accuracy = 0.9 Loss = 0.256746\n",
      "2867 Training accuracy = 0.85 Loss = 0.37957\n",
      "2868 Training accuracy = 0.95 Loss = 0.185797\n",
      "2869 Training accuracy = 0.92 Loss = 0.326198\n",
      "2870 Training accuracy = 0.9 Loss = 0.333878\n",
      "2871 Training accuracy = 0.89 Loss = 0.267759\n",
      "2872 Training accuracy = 0.93 Loss = 0.279174\n",
      "2873 Training accuracy = 0.9 Loss = 0.353107\n",
      "2874 Training accuracy = 0.94 Loss = 0.254056\n",
      "2875 Training accuracy = 0.95 Loss = 0.216847\n",
      "2876 Training accuracy = 0.96 Loss = 0.224092\n",
      "2877 Training accuracy = 0.95 Loss = 0.232741\n",
      "2878 Training accuracy = 0.93 Loss = 0.32089\n",
      "2879 Training accuracy = 0.95 Loss = 0.192817\n",
      "2880 Training accuracy = 0.91 Loss = 0.247707\n",
      "2881 Training accuracy = 0.88 Loss = 0.251827\n",
      "2882 Training accuracy = 0.97 Loss = 0.179856\n",
      "2883 Training accuracy = 0.93 Loss = 0.26958\n",
      "2884 Training accuracy = 0.88 Loss = 0.415325\n",
      "2885 Training accuracy = 0.91 Loss = 0.253092\n",
      "2886 Training accuracy = 0.9 Loss = 0.288267\n",
      "2887 Training accuracy = 0.93 Loss = 0.321738\n",
      "2888 Training accuracy = 0.86 Loss = 0.335822\n",
      "2889 Training accuracy = 0.93 Loss = 0.217429\n",
      "2890 Training accuracy = 0.93 Loss = 0.291642\n",
      "2891 Training accuracy = 0.92 Loss = 0.279433\n",
      "2892 Training accuracy = 0.95 Loss = 0.188059\n",
      "2893 Training accuracy = 0.93 Loss = 0.248436\n",
      "2894 Training accuracy = 0.93 Loss = 0.234242\n",
      "2895 Training accuracy = 0.93 Loss = 0.300563\n",
      "2896 Training accuracy = 0.97 Loss = 0.146289\n",
      "2897 Training accuracy = 0.94 Loss = 0.31622\n",
      "2898 Training accuracy = 0.91 Loss = 0.309309\n",
      "2899 Training accuracy = 0.87 Loss = 0.336421\n",
      "2900 Training accuracy = 0.91 Loss = 0.263174\n",
      "2901 Training accuracy = 0.93 Loss = 0.322457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2902 Training accuracy = 0.92 Loss = 0.3035\n",
      "2903 Training accuracy = 0.9 Loss = 0.24865\n",
      "2904 Training accuracy = 0.91 Loss = 0.285265\n",
      "2905 Training accuracy = 0.97 Loss = 0.138501\n",
      "2906 Training accuracy = 0.93 Loss = 0.326572\n",
      "2907 Training accuracy = 0.93 Loss = 0.217333\n",
      "2908 Training accuracy = 0.88 Loss = 0.487038\n",
      "2909 Training accuracy = 0.91 Loss = 0.310737\n",
      "2910 Training accuracy = 0.93 Loss = 0.310858\n",
      "2911 Training accuracy = 0.93 Loss = 0.273726\n",
      "2912 Training accuracy = 0.88 Loss = 0.410091\n",
      "2913 Training accuracy = 0.94 Loss = 0.264231\n",
      "2914 Training accuracy = 0.93 Loss = 0.247844\n",
      "2915 Training accuracy = 0.92 Loss = 0.350354\n",
      "2916 Training accuracy = 0.87 Loss = 0.365666\n",
      "2917 Training accuracy = 0.91 Loss = 0.303025\n",
      "2918 Training accuracy = 0.94 Loss = 0.258856\n",
      "2919 Training accuracy = 0.93 Loss = 0.276282\n",
      "2920 Training accuracy = 0.96 Loss = 0.160284\n",
      "2921 Training accuracy = 0.95 Loss = 0.283345\n",
      "2922 Training accuracy = 0.85 Loss = 0.428908\n",
      "2923 Training accuracy = 0.93 Loss = 0.218858\n",
      "2924 Training accuracy = 0.93 Loss = 0.286751\n",
      "2925 Training accuracy = 0.95 Loss = 0.173193\n",
      "2926 Training accuracy = 0.91 Loss = 0.309731\n",
      "2927 Training accuracy = 0.9 Loss = 0.34029\n",
      "2928 Training accuracy = 0.92 Loss = 0.211431\n",
      "2929 Training accuracy = 0.94 Loss = 0.194575\n",
      "2930 Training accuracy = 0.94 Loss = 0.294559\n",
      "2931 Training accuracy = 0.95 Loss = 0.181216\n",
      "2932 Training accuracy = 0.92 Loss = 0.364498\n",
      "2933 Training accuracy = 0.95 Loss = 0.150606\n",
      "2934 Training accuracy = 0.96 Loss = 0.190048\n",
      "2935 Training accuracy = 0.89 Loss = 0.362406\n",
      "2936 Training accuracy = 0.95 Loss = 0.189138\n",
      "2937 Training accuracy = 0.92 Loss = 0.458055\n",
      "2938 Training accuracy = 0.94 Loss = 0.152973\n",
      "2939 Training accuracy = 0.91 Loss = 0.336753\n",
      "2940 Training accuracy = 0.95 Loss = 0.276646\n",
      "2941 Training accuracy = 0.91 Loss = 0.309532\n",
      "2942 Training accuracy = 0.93 Loss = 0.281869\n",
      "2943 Training accuracy = 0.9 Loss = 0.438773\n",
      "2944 Training accuracy = 0.94 Loss = 0.196496\n",
      "2945 Training accuracy = 0.91 Loss = 0.365287\n",
      "2946 Training accuracy = 0.94 Loss = 0.222826\n",
      "2947 Training accuracy = 0.96 Loss = 0.157747\n",
      "2948 Training accuracy = 0.91 Loss = 0.282618\n",
      "2949 Training accuracy = 0.86 Loss = 0.279559\n",
      "2950 Training accuracy = 0.92 Loss = 0.291273\n",
      "2951 Training accuracy = 0.95 Loss = 0.245138\n",
      "2952 Training accuracy = 0.97 Loss = 0.188607\n",
      "2953 Training accuracy = 0.84 Loss = 0.518846\n",
      "2954 Training accuracy = 0.83 Loss = 0.480823\n",
      "2955 Training accuracy = 0.91 Loss = 0.282772\n",
      "2956 Training accuracy = 0.95 Loss = 0.209694\n",
      "2957 Training accuracy = 0.95 Loss = 0.176083\n",
      "2958 Training accuracy = 0.95 Loss = 0.18453\n",
      "2959 Training accuracy = 0.88 Loss = 0.423764\n",
      "2960 Training accuracy = 0.9 Loss = 0.293231\n",
      "2961 Training accuracy = 0.91 Loss = 0.357894\n",
      "2962 Training accuracy = 0.89 Loss = 0.40932\n",
      "2963 Training accuracy = 0.91 Loss = 0.304356\n",
      "2964 Training accuracy = 0.9 Loss = 0.326206\n",
      "2965 Training accuracy = 0.93 Loss = 0.199625\n",
      "2966 Training accuracy = 0.92 Loss = 0.28987\n",
      "2967 Training accuracy = 0.87 Loss = 0.457784\n",
      "2968 Training accuracy = 0.93 Loss = 0.194325\n",
      "2969 Training accuracy = 0.92 Loss = 0.210498\n",
      "2970 Training accuracy = 0.88 Loss = 0.326975\n",
      "2971 Training accuracy = 0.88 Loss = 0.386455\n",
      "2972 Training accuracy = 0.93 Loss = 0.277103\n",
      "2973 Training accuracy = 0.91 Loss = 0.23385\n",
      "2974 Training accuracy = 0.94 Loss = 0.234821\n",
      "2975 Training accuracy = 0.88 Loss = 0.411372\n",
      "2976 Training accuracy = 0.9 Loss = 0.418255\n",
      "2977 Training accuracy = 0.9 Loss = 0.337287\n",
      "2978 Training accuracy = 0.88 Loss = 0.343155\n",
      "2979 Training accuracy = 0.93 Loss = 0.288766\n",
      "2980 Training accuracy = 0.94 Loss = 0.209697\n",
      "2981 Training accuracy = 0.94 Loss = 0.250762\n",
      "2982 Training accuracy = 0.92 Loss = 0.306949\n",
      "2983 Training accuracy = 0.93 Loss = 0.23076\n",
      "2984 Training accuracy = 0.93 Loss = 0.28976\n",
      "2985 Training accuracy = 0.95 Loss = 0.236667\n",
      "2986 Training accuracy = 0.94 Loss = 0.160817\n",
      "2987 Training accuracy = 0.9 Loss = 0.351018\n",
      "2988 Training accuracy = 0.93 Loss = 0.356377\n",
      "2989 Training accuracy = 0.95 Loss = 0.232391\n",
      "2990 Training accuracy = 0.94 Loss = 0.213005\n",
      "2991 Training accuracy = 0.86 Loss = 0.357997\n",
      "2992 Training accuracy = 0.9 Loss = 0.331848\n",
      "2993 Training accuracy = 0.95 Loss = 0.211349\n",
      "2994 Training accuracy = 1.0 Loss = 0.107003\n",
      "2995 Training accuracy = 0.89 Loss = 0.493061\n",
      "2996 Training accuracy = 0.9 Loss = 0.347173\n",
      "2997 Training accuracy = 0.96 Loss = 0.197098\n",
      "2998 Training accuracy = 0.92 Loss = 0.222817\n",
      "2999 Training accuracy = 0.93 Loss = 0.33845\n",
      "3000 Training accuracy = 0.93 Loss = 0.200075\n",
      "3001 Training accuracy = 0.94 Loss = 0.239093\n",
      "3002 Training accuracy = 0.96 Loss = 0.124985\n",
      "3003 Training accuracy = 0.91 Loss = 0.380191\n",
      "3004 Training accuracy = 0.92 Loss = 0.28159\n",
      "3005 Training accuracy = 0.93 Loss = 0.217263\n",
      "3006 Training accuracy = 0.9 Loss = 0.319603\n",
      "3007 Training accuracy = 0.91 Loss = 0.341148\n",
      "3008 Training accuracy = 0.96 Loss = 0.192778\n",
      "3009 Training accuracy = 0.9 Loss = 0.303031\n",
      "3010 Training accuracy = 0.93 Loss = 0.311827\n",
      "3011 Training accuracy = 0.94 Loss = 0.198008\n",
      "3012 Training accuracy = 0.89 Loss = 0.332299\n",
      "3013 Training accuracy = 0.88 Loss = 0.414963\n",
      "3014 Training accuracy = 0.89 Loss = 0.336836\n",
      "3015 Training accuracy = 0.85 Loss = 0.349377\n",
      "3016 Training accuracy = 0.94 Loss = 0.317193\n",
      "3017 Training accuracy = 0.95 Loss = 0.154771\n",
      "3018 Training accuracy = 0.92 Loss = 0.440945\n",
      "3019 Training accuracy = 0.89 Loss = 0.286616\n",
      "3020 Training accuracy = 0.91 Loss = 0.271522\n",
      "3021 Training accuracy = 0.92 Loss = 0.284152\n",
      "3022 Training accuracy = 0.95 Loss = 0.260835\n",
      "3023 Training accuracy = 0.93 Loss = 0.198841\n",
      "3024 Training accuracy = 0.94 Loss = 0.218065\n",
      "3025 Training accuracy = 0.91 Loss = 0.368509\n",
      "3026 Training accuracy = 0.9 Loss = 0.402873\n",
      "3027 Training accuracy = 0.94 Loss = 0.269277\n",
      "3028 Training accuracy = 0.91 Loss = 0.276737\n",
      "3029 Training accuracy = 0.92 Loss = 0.298571\n",
      "3030 Training accuracy = 0.93 Loss = 0.28526\n",
      "3031 Training accuracy = 0.96 Loss = 0.231987\n",
      "3032 Training accuracy = 0.95 Loss = 0.207369\n",
      "3033 Training accuracy = 0.95 Loss = 0.207083\n",
      "3034 Training accuracy = 0.92 Loss = 0.253058\n",
      "3035 Training accuracy = 0.9 Loss = 0.414554\n",
      "3036 Training accuracy = 0.94 Loss = 0.289514\n",
      "3037 Training accuracy = 0.92 Loss = 0.281997\n",
      "3038 Training accuracy = 0.95 Loss = 0.275797\n",
      "3039 Training accuracy = 0.97 Loss = 0.193635\n",
      "3040 Training accuracy = 0.92 Loss = 0.257348\n",
      "3041 Training accuracy = 0.96 Loss = 0.187459\n",
      "3042 Training accuracy = 0.89 Loss = 0.308394\n",
      "3043 Training accuracy = 0.9 Loss = 0.387214\n",
      "3044 Training accuracy = 0.96 Loss = 0.163693\n",
      "3045 Training accuracy = 0.97 Loss = 0.246186\n",
      "3046 Training accuracy = 0.94 Loss = 0.19065\n",
      "3047 Training accuracy = 0.97 Loss = 0.180796\n",
      "3048 Training accuracy = 0.92 Loss = 0.308244\n",
      "3049 Training accuracy = 0.94 Loss = 0.185233\n",
      "3050 Training accuracy = 0.97 Loss = 0.167922\n",
      "3051 Training accuracy = 0.91 Loss = 0.221072\n",
      "3052 Training accuracy = 0.96 Loss = 0.184053\n",
      "3053 Training accuracy = 0.95 Loss = 0.180404\n",
      "3054 Training accuracy = 0.92 Loss = 0.26477\n",
      "3055 Training accuracy = 0.9 Loss = 0.382347\n",
      "3056 Training accuracy = 0.94 Loss = 0.21671\n",
      "3057 Training accuracy = 0.88 Loss = 0.358529\n",
      "3058 Training accuracy = 0.87 Loss = 0.348908\n",
      "3059 Training accuracy = 0.96 Loss = 0.149283\n",
      "3060 Training accuracy = 0.93 Loss = 0.266349\n",
      "3061 Training accuracy = 0.95 Loss = 0.196114\n",
      "3062 Training accuracy = 0.93 Loss = 0.223702\n",
      "3063 Training accuracy = 0.93 Loss = 0.318353\n",
      "3064 Training accuracy = 0.96 Loss = 0.216106\n",
      "3065 Training accuracy = 0.91 Loss = 0.264513\n",
      "3066 Training accuracy = 0.94 Loss = 0.238734\n",
      "3067 Training accuracy = 0.96 Loss = 0.190262\n",
      "3068 Training accuracy = 0.97 Loss = 0.173314\n",
      "3069 Training accuracy = 0.91 Loss = 0.320142\n",
      "3070 Training accuracy = 0.97 Loss = 0.150733\n",
      "3071 Training accuracy = 0.93 Loss = 0.246815\n",
      "3072 Training accuracy = 0.92 Loss = 0.394581\n",
      "3073 Training accuracy = 0.9 Loss = 0.353092\n",
      "3074 Training accuracy = 0.9 Loss = 0.412148\n",
      "3075 Training accuracy = 0.88 Loss = 0.377368\n",
      "3076 Training accuracy = 0.89 Loss = 0.490625\n",
      "3077 Training accuracy = 0.95 Loss = 0.252952\n",
      "3078 Training accuracy = 0.91 Loss = 0.301198\n",
      "3079 Training accuracy = 0.91 Loss = 0.339222\n",
      "3080 Training accuracy = 0.94 Loss = 0.208533\n",
      "3081 Training accuracy = 0.92 Loss = 0.358776\n",
      "3082 Training accuracy = 0.9 Loss = 0.389263\n",
      "3083 Training accuracy = 0.91 Loss = 0.316615\n",
      "3084 Training accuracy = 0.93 Loss = 0.253951\n",
      "3085 Training accuracy = 0.9 Loss = 0.38812\n",
      "3086 Training accuracy = 0.9 Loss = 0.305545\n",
      "3087 Training accuracy = 0.91 Loss = 0.337212\n",
      "3088 Training accuracy = 0.9 Loss = 0.443405\n",
      "3089 Training accuracy = 0.94 Loss = 0.318735\n",
      "3090 Training accuracy = 0.87 Loss = 0.506666\n",
      "3091 Training accuracy = 0.88 Loss = 0.423371\n",
      "3092 Training accuracy = 0.94 Loss = 0.179096\n",
      "3093 Training accuracy = 0.86 Loss = 0.456243\n",
      "3094 Training accuracy = 0.93 Loss = 0.25175\n",
      "3095 Training accuracy = 0.87 Loss = 0.489711\n",
      "3096 Training accuracy = 0.94 Loss = 0.19608\n",
      "3097 Training accuracy = 0.97 Loss = 0.140117\n",
      "3098 Training accuracy = 0.91 Loss = 0.316473\n",
      "3099 Training accuracy = 0.93 Loss = 0.34926\n",
      "3100 Training accuracy = 0.96 Loss = 0.18636\n",
      "3101 Training accuracy = 0.97 Loss = 0.131246\n",
      "3102 Training accuracy = 0.94 Loss = 0.233863\n",
      "3103 Training accuracy = 0.93 Loss = 0.245143\n",
      "3104 Training accuracy = 0.94 Loss = 0.19666\n",
      "3105 Training accuracy = 0.92 Loss = 0.216248\n",
      "3106 Training accuracy = 0.9 Loss = 0.410267\n",
      "3107 Training accuracy = 0.96 Loss = 0.172897\n",
      "3108 Training accuracy = 0.94 Loss = 0.304743\n",
      "3109 Training accuracy = 0.95 Loss = 0.164768\n",
      "3110 Training accuracy = 0.91 Loss = 0.287831\n",
      "3111 Training accuracy = 0.87 Loss = 0.512915\n",
      "3112 Training accuracy = 0.94 Loss = 0.214107\n",
      "3113 Training accuracy = 0.91 Loss = 0.277197\n",
      "3114 Training accuracy = 0.9 Loss = 0.26139\n",
      "3115 Training accuracy = 0.93 Loss = 0.229958\n",
      "3116 Training accuracy = 0.91 Loss = 0.279592\n",
      "3117 Training accuracy = 0.93 Loss = 0.254533\n",
      "3118 Training accuracy = 0.93 Loss = 0.222463\n",
      "3119 Training accuracy = 0.92 Loss = 0.295035\n",
      "3120 Training accuracy = 0.86 Loss = 0.46873\n",
      "3121 Training accuracy = 0.9 Loss = 0.321623\n",
      "3122 Training accuracy = 0.92 Loss = 0.22345\n",
      "3123 Training accuracy = 0.92 Loss = 0.231771\n",
      "3124 Training accuracy = 0.94 Loss = 0.191128\n",
      "3125 Training accuracy = 0.94 Loss = 0.200527\n",
      "3126 Training accuracy = 0.92 Loss = 0.316468\n",
      "3127 Training accuracy = 0.91 Loss = 0.398022\n",
      "3128 Training accuracy = 0.89 Loss = 0.305309\n",
      "3129 Training accuracy = 0.93 Loss = 0.222218\n",
      "3130 Training accuracy = 0.9 Loss = 0.348719\n",
      "3131 Training accuracy = 0.94 Loss = 0.323883\n",
      "3132 Training accuracy = 0.95 Loss = 0.179005\n",
      "3133 Training accuracy = 0.91 Loss = 0.303888\n",
      "3134 Training accuracy = 0.9 Loss = 0.331666\n",
      "3135 Training accuracy = 0.92 Loss = 0.222732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3136 Training accuracy = 0.91 Loss = 0.266423\n",
      "3137 Training accuracy = 0.91 Loss = 0.299625\n",
      "3138 Training accuracy = 0.97 Loss = 0.215774\n",
      "3139 Training accuracy = 0.91 Loss = 0.343357\n",
      "3140 Training accuracy = 0.93 Loss = 0.287994\n",
      "3141 Training accuracy = 0.95 Loss = 0.171076\n",
      "3142 Training accuracy = 0.93 Loss = 0.2322\n",
      "3143 Training accuracy = 0.87 Loss = 0.465367\n",
      "3144 Training accuracy = 0.88 Loss = 0.304996\n",
      "3145 Training accuracy = 0.88 Loss = 0.526557\n",
      "3146 Training accuracy = 0.93 Loss = 0.364347\n",
      "3147 Training accuracy = 0.93 Loss = 0.251391\n",
      "3148 Training accuracy = 0.88 Loss = 0.376417\n",
      "3149 Training accuracy = 0.9 Loss = 0.329516\n",
      "3150 Training accuracy = 0.92 Loss = 0.308568\n",
      "3151 Training accuracy = 0.93 Loss = 0.31118\n",
      "3152 Training accuracy = 0.93 Loss = 0.223463\n",
      "3153 Training accuracy = 0.93 Loss = 0.289252\n",
      "3154 Training accuracy = 0.99 Loss = 0.128854\n",
      "3155 Training accuracy = 0.94 Loss = 0.252668\n",
      "3156 Training accuracy = 0.93 Loss = 0.27368\n",
      "3157 Training accuracy = 0.9 Loss = 0.406079\n",
      "3158 Training accuracy = 0.93 Loss = 0.272346\n",
      "3159 Training accuracy = 0.86 Loss = 0.349096\n",
      "3160 Training accuracy = 0.96 Loss = 0.186657\n",
      "3161 Training accuracy = 0.87 Loss = 0.397941\n",
      "3162 Training accuracy = 0.93 Loss = 0.312591\n",
      "3163 Training accuracy = 0.9 Loss = 0.358566\n",
      "3164 Training accuracy = 0.89 Loss = 0.404522\n",
      "3165 Training accuracy = 0.92 Loss = 0.327962\n",
      "3166 Training accuracy = 0.95 Loss = 0.157976\n",
      "3167 Training accuracy = 0.88 Loss = 0.249804\n",
      "3168 Training accuracy = 0.97 Loss = 0.162853\n",
      "3169 Training accuracy = 0.95 Loss = 0.2536\n",
      "3170 Training accuracy = 0.94 Loss = 0.211802\n",
      "3171 Training accuracy = 0.96 Loss = 0.157869\n",
      "3172 Training accuracy = 0.94 Loss = 0.286269\n",
      "3173 Training accuracy = 0.93 Loss = 0.276106\n",
      "3174 Training accuracy = 0.88 Loss = 0.399178\n",
      "3175 Training accuracy = 0.93 Loss = 0.302291\n",
      "3176 Training accuracy = 0.91 Loss = 0.29072\n",
      "3177 Training accuracy = 0.99 Loss = 0.158153\n",
      "3178 Training accuracy = 0.93 Loss = 0.177584\n",
      "3179 Training accuracy = 0.89 Loss = 0.339011\n",
      "3180 Training accuracy = 0.88 Loss = 0.350012\n",
      "3181 Training accuracy = 0.92 Loss = 0.34143\n",
      "3182 Training accuracy = 0.96 Loss = 0.164254\n",
      "3183 Training accuracy = 0.93 Loss = 0.287727\n",
      "3184 Training accuracy = 0.94 Loss = 0.163116\n",
      "3185 Training accuracy = 0.93 Loss = 0.169842\n",
      "3186 Training accuracy = 0.9 Loss = 0.262269\n",
      "3187 Training accuracy = 0.9 Loss = 0.383158\n",
      "3188 Training accuracy = 0.95 Loss = 0.156212\n",
      "3189 Training accuracy = 0.95 Loss = 0.186148\n",
      "3190 Training accuracy = 0.88 Loss = 0.311042\n",
      "3191 Training accuracy = 0.92 Loss = 0.248495\n",
      "3192 Training accuracy = 0.93 Loss = 0.266001\n",
      "3193 Training accuracy = 0.95 Loss = 0.178251\n",
      "3194 Training accuracy = 0.93 Loss = 0.283196\n",
      "3195 Training accuracy = 0.93 Loss = 0.26033\n",
      "3196 Training accuracy = 0.93 Loss = 0.260756\n",
      "3197 Training accuracy = 0.91 Loss = 0.27255\n",
      "3198 Training accuracy = 0.91 Loss = 0.250711\n",
      "3199 Training accuracy = 0.92 Loss = 0.338952\n",
      "3200 Training accuracy = 0.94 Loss = 0.20145\n",
      "3201 Training accuracy = 0.9 Loss = 0.377282\n",
      "3202 Training accuracy = 0.92 Loss = 0.327268\n",
      "3203 Training accuracy = 0.87 Loss = 0.374293\n",
      "3204 Training accuracy = 0.91 Loss = 0.268733\n",
      "3205 Training accuracy = 0.94 Loss = 0.244666\n",
      "3206 Training accuracy = 0.9 Loss = 0.41236\n",
      "3207 Training accuracy = 0.92 Loss = 0.217891\n",
      "3208 Training accuracy = 0.92 Loss = 0.206899\n",
      "3209 Training accuracy = 0.93 Loss = 0.283907\n",
      "3210 Training accuracy = 0.93 Loss = 0.281287\n",
      "3211 Training accuracy = 0.88 Loss = 0.463753\n",
      "3212 Training accuracy = 0.93 Loss = 0.237759\n",
      "3213 Training accuracy = 0.95 Loss = 0.230566\n",
      "3214 Training accuracy = 0.93 Loss = 0.214709\n",
      "3215 Training accuracy = 0.93 Loss = 0.252861\n",
      "3216 Training accuracy = 0.91 Loss = 0.272841\n",
      "3217 Training accuracy = 0.93 Loss = 0.336954\n",
      "3218 Training accuracy = 0.94 Loss = 0.290536\n",
      "3219 Training accuracy = 0.94 Loss = 0.264717\n",
      "3220 Training accuracy = 0.9 Loss = 0.433908\n",
      "3221 Training accuracy = 0.88 Loss = 0.35067\n",
      "3222 Training accuracy = 0.92 Loss = 0.369207\n",
      "3223 Training accuracy = 0.92 Loss = 0.301385\n",
      "3224 Training accuracy = 0.94 Loss = 0.245821\n",
      "3225 Training accuracy = 0.88 Loss = 0.378886\n",
      "3226 Training accuracy = 0.89 Loss = 0.281613\n",
      "3227 Training accuracy = 0.92 Loss = 0.263152\n",
      "3228 Training accuracy = 0.89 Loss = 0.419703\n",
      "3229 Training accuracy = 0.98 Loss = 0.177497\n",
      "3230 Training accuracy = 0.92 Loss = 0.244372\n",
      "3231 Training accuracy = 0.89 Loss = 0.421726\n",
      "3232 Training accuracy = 0.94 Loss = 0.289681\n",
      "3233 Training accuracy = 0.92 Loss = 0.273545\n",
      "3234 Training accuracy = 0.94 Loss = 0.271398\n",
      "3235 Training accuracy = 0.9 Loss = 0.275553\n",
      "3236 Training accuracy = 0.93 Loss = 0.31616\n",
      "3237 Training accuracy = 0.94 Loss = 0.312785\n",
      "3238 Training accuracy = 0.94 Loss = 0.203947\n",
      "3239 Training accuracy = 0.96 Loss = 0.203447\n",
      "3240 Training accuracy = 0.95 Loss = 0.2719\n",
      "3241 Training accuracy = 0.97 Loss = 0.211156\n",
      "3242 Training accuracy = 0.97 Loss = 0.132911\n",
      "3243 Training accuracy = 0.95 Loss = 0.25057\n",
      "3244 Training accuracy = 0.88 Loss = 0.33787\n",
      "3245 Training accuracy = 0.9 Loss = 0.286807\n",
      "3246 Training accuracy = 0.9 Loss = 0.327042\n",
      "3247 Training accuracy = 0.92 Loss = 0.369431\n",
      "3248 Training accuracy = 0.94 Loss = 0.276606\n",
      "3249 Training accuracy = 0.88 Loss = 0.311682\n",
      "3250 Training accuracy = 0.96 Loss = 0.224564\n",
      "3251 Training accuracy = 0.93 Loss = 0.286327\n",
      "3252 Training accuracy = 0.95 Loss = 0.199264\n",
      "3253 Training accuracy = 0.91 Loss = 0.286253\n",
      "3254 Training accuracy = 0.91 Loss = 0.270142\n",
      "3255 Training accuracy = 0.88 Loss = 0.335207\n",
      "3256 Training accuracy = 0.92 Loss = 0.327299\n",
      "3257 Training accuracy = 0.93 Loss = 0.251598\n",
      "3258 Training accuracy = 0.94 Loss = 0.248261\n",
      "3259 Training accuracy = 0.9 Loss = 0.318491\n",
      "3260 Training accuracy = 0.92 Loss = 0.290708\n",
      "3261 Training accuracy = 0.95 Loss = 0.152984\n",
      "3262 Training accuracy = 0.83 Loss = 0.362305\n",
      "3263 Training accuracy = 0.98 Loss = 0.140001\n",
      "3264 Training accuracy = 0.9 Loss = 0.444209\n",
      "3265 Training accuracy = 0.92 Loss = 0.197441\n",
      "3266 Training accuracy = 0.95 Loss = 0.239817\n",
      "3267 Training accuracy = 0.96 Loss = 0.182764\n",
      "3268 Training accuracy = 0.89 Loss = 0.283733\n",
      "3269 Training accuracy = 0.91 Loss = 0.361631\n",
      "3270 Training accuracy = 0.96 Loss = 0.184607\n",
      "3271 Training accuracy = 0.94 Loss = 0.177422\n",
      "3272 Training accuracy = 0.96 Loss = 0.163185\n",
      "3273 Training accuracy = 0.93 Loss = 0.245077\n",
      "3274 Training accuracy = 0.94 Loss = 0.232436\n",
      "3275 Training accuracy = 0.89 Loss = 0.310962\n",
      "3276 Training accuracy = 0.96 Loss = 0.142065\n",
      "3277 Training accuracy = 0.95 Loss = 0.17897\n",
      "3278 Training accuracy = 0.96 Loss = 0.195138\n",
      "3279 Training accuracy = 0.89 Loss = 0.288221\n",
      "3280 Training accuracy = 0.93 Loss = 0.214934\n",
      "3281 Training accuracy = 0.88 Loss = 0.442403\n",
      "3282 Training accuracy = 0.88 Loss = 0.36446\n",
      "3283 Training accuracy = 0.95 Loss = 0.193833\n",
      "3284 Training accuracy = 0.94 Loss = 0.25187\n",
      "3285 Training accuracy = 0.94 Loss = 0.141716\n",
      "3286 Training accuracy = 0.92 Loss = 0.255341\n",
      "3287 Training accuracy = 0.91 Loss = 0.265385\n",
      "3288 Training accuracy = 0.96 Loss = 0.163205\n",
      "3289 Training accuracy = 0.91 Loss = 0.380579\n",
      "3290 Training accuracy = 0.95 Loss = 0.296164\n",
      "3291 Training accuracy = 0.91 Loss = 0.220783\n",
      "3292 Training accuracy = 0.95 Loss = 0.160932\n",
      "3293 Training accuracy = 0.97 Loss = 0.309618\n",
      "3294 Training accuracy = 0.93 Loss = 0.221366\n",
      "3295 Training accuracy = 0.89 Loss = 0.249133\n",
      "3296 Training accuracy = 0.89 Loss = 0.397859\n",
      "3297 Training accuracy = 0.89 Loss = 0.299662\n",
      "3298 Training accuracy = 0.94 Loss = 0.356661\n",
      "3299 Training accuracy = 0.93 Loss = 0.254879\n",
      "3300 Training accuracy = 0.95 Loss = 0.230968\n",
      "3301 Training accuracy = 0.91 Loss = 0.280776\n",
      "3302 Training accuracy = 0.92 Loss = 0.304273\n",
      "3303 Training accuracy = 0.88 Loss = 0.441448\n",
      "3304 Training accuracy = 0.9 Loss = 0.279724\n",
      "3305 Training accuracy = 0.92 Loss = 0.435445\n",
      "3306 Training accuracy = 0.9 Loss = 0.491806\n",
      "3307 Training accuracy = 0.89 Loss = 0.332468\n",
      "3308 Training accuracy = 0.92 Loss = 0.229356\n",
      "3309 Training accuracy = 0.91 Loss = 0.207389\n",
      "3310 Training accuracy = 0.88 Loss = 0.375843\n",
      "3311 Training accuracy = 0.96 Loss = 0.121664\n",
      "3312 Training accuracy = 0.94 Loss = 0.250711\n",
      "3313 Training accuracy = 0.86 Loss = 0.510815\n",
      "3314 Training accuracy = 0.93 Loss = 0.227841\n",
      "3315 Training accuracy = 0.9 Loss = 0.273253\n",
      "3316 Training accuracy = 0.88 Loss = 0.349726\n",
      "3317 Training accuracy = 0.92 Loss = 0.307734\n",
      "3318 Training accuracy = 0.91 Loss = 0.28441\n",
      "3319 Training accuracy = 0.91 Loss = 0.270431\n",
      "3320 Training accuracy = 0.9 Loss = 0.428182\n",
      "3321 Training accuracy = 0.92 Loss = 0.258513\n",
      "3322 Training accuracy = 0.93 Loss = 0.351806\n",
      "3323 Training accuracy = 0.94 Loss = 0.209363\n",
      "3324 Training accuracy = 0.92 Loss = 0.221238\n",
      "3325 Training accuracy = 0.94 Loss = 0.2731\n",
      "3326 Training accuracy = 0.95 Loss = 0.179778\n",
      "3327 Training accuracy = 0.92 Loss = 0.262787\n",
      "3328 Training accuracy = 0.87 Loss = 0.434364\n",
      "3329 Training accuracy = 0.94 Loss = 0.190182\n",
      "3330 Training accuracy = 0.95 Loss = 0.257314\n",
      "3331 Training accuracy = 0.92 Loss = 0.289957\n",
      "3332 Training accuracy = 0.95 Loss = 0.179066\n",
      "3333 Training accuracy = 0.92 Loss = 0.451168\n",
      "3334 Training accuracy = 0.94 Loss = 0.193677\n",
      "3335 Training accuracy = 0.89 Loss = 0.370258\n",
      "3336 Training accuracy = 0.88 Loss = 0.4327\n",
      "3337 Training accuracy = 0.91 Loss = 0.309647\n",
      "3338 Training accuracy = 0.93 Loss = 0.226902\n",
      "3339 Training accuracy = 0.91 Loss = 0.252482\n",
      "3340 Training accuracy = 0.97 Loss = 0.145969\n",
      "3341 Training accuracy = 0.94 Loss = 0.246529\n",
      "3342 Training accuracy = 0.92 Loss = 0.248756\n",
      "3343 Training accuracy = 0.91 Loss = 0.346331\n",
      "3344 Training accuracy = 0.93 Loss = 0.25848\n",
      "3345 Training accuracy = 0.96 Loss = 0.244822\n",
      "3346 Training accuracy = 0.9 Loss = 0.376435\n",
      "3347 Training accuracy = 0.94 Loss = 0.188359\n",
      "3348 Training accuracy = 0.94 Loss = 0.222274\n",
      "3349 Training accuracy = 0.93 Loss = 0.275694\n",
      "3350 Training accuracy = 0.96 Loss = 0.214717\n",
      "3351 Training accuracy = 0.9 Loss = 0.59713\n",
      "3352 Training accuracy = 0.91 Loss = 0.299134\n",
      "3353 Training accuracy = 0.91 Loss = 0.31038\n",
      "3354 Training accuracy = 0.92 Loss = 0.260582\n",
      "3355 Training accuracy = 0.94 Loss = 0.232369\n",
      "3356 Training accuracy = 0.94 Loss = 0.246753\n",
      "3357 Training accuracy = 0.86 Loss = 0.370868\n",
      "3358 Training accuracy = 0.88 Loss = 0.397604\n",
      "3359 Training accuracy = 0.96 Loss = 0.214165\n",
      "3360 Training accuracy = 0.89 Loss = 0.311824\n",
      "3361 Training accuracy = 0.9 Loss = 0.297959\n",
      "3362 Training accuracy = 0.94 Loss = 0.260278\n",
      "3363 Training accuracy = 0.89 Loss = 0.336835\n",
      "3364 Training accuracy = 0.89 Loss = 0.361169\n",
      "3365 Training accuracy = 0.9 Loss = 0.211502\n",
      "3366 Training accuracy = 0.91 Loss = 0.464357\n",
      "3367 Training accuracy = 0.92 Loss = 0.227487\n",
      "3368 Training accuracy = 0.89 Loss = 0.429393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3369 Training accuracy = 0.92 Loss = 0.286874\n",
      "3370 Training accuracy = 0.91 Loss = 0.259465\n",
      "3371 Training accuracy = 0.94 Loss = 0.233601\n",
      "3372 Training accuracy = 0.9 Loss = 0.315877\n",
      "3373 Training accuracy = 0.94 Loss = 0.2697\n",
      "3374 Training accuracy = 0.94 Loss = 0.246239\n",
      "3375 Training accuracy = 0.94 Loss = 0.215761\n",
      "3376 Training accuracy = 0.94 Loss = 0.210977\n",
      "3377 Training accuracy = 0.89 Loss = 0.284388\n",
      "3378 Training accuracy = 0.89 Loss = 0.308911\n",
      "3379 Training accuracy = 0.94 Loss = 0.215226\n",
      "3380 Training accuracy = 0.95 Loss = 0.262818\n",
      "3381 Training accuracy = 0.9 Loss = 0.2422\n",
      "3382 Training accuracy = 0.94 Loss = 0.192833\n",
      "3383 Training accuracy = 0.91 Loss = 0.244529\n",
      "3384 Training accuracy = 0.95 Loss = 0.163938\n",
      "3385 Training accuracy = 0.93 Loss = 0.268602\n",
      "3386 Training accuracy = 0.96 Loss = 0.139939\n",
      "3387 Training accuracy = 0.94 Loss = 0.323976\n",
      "3388 Training accuracy = 0.92 Loss = 0.241476\n",
      "3389 Training accuracy = 0.92 Loss = 0.219306\n",
      "3390 Training accuracy = 0.92 Loss = 0.394913\n",
      "3391 Training accuracy = 0.92 Loss = 0.280501\n",
      "3392 Training accuracy = 0.92 Loss = 0.242425\n",
      "3393 Training accuracy = 0.9 Loss = 0.343949\n",
      "3394 Training accuracy = 0.89 Loss = 0.435719\n",
      "3395 Training accuracy = 0.89 Loss = 0.290205\n",
      "3396 Training accuracy = 0.9 Loss = 0.350262\n",
      "3397 Training accuracy = 0.94 Loss = 0.252118\n",
      "3398 Training accuracy = 0.95 Loss = 0.225634\n",
      "3399 Training accuracy = 0.9 Loss = 0.276697\n",
      "3400 Training accuracy = 0.89 Loss = 0.293662\n",
      "3401 Training accuracy = 0.9 Loss = 0.361807\n",
      "3402 Training accuracy = 0.93 Loss = 0.275482\n",
      "3403 Training accuracy = 0.93 Loss = 0.238446\n",
      "3404 Training accuracy = 0.99 Loss = 0.147193\n",
      "3405 Training accuracy = 0.94 Loss = 0.241007\n",
      "3406 Training accuracy = 0.94 Loss = 0.24907\n",
      "3407 Training accuracy = 0.95 Loss = 0.182157\n",
      "3408 Training accuracy = 0.89 Loss = 0.322386\n",
      "3409 Training accuracy = 0.9 Loss = 0.310824\n",
      "3410 Training accuracy = 0.91 Loss = 0.34935\n",
      "3411 Training accuracy = 0.92 Loss = 0.441849\n",
      "3412 Training accuracy = 0.91 Loss = 0.332499\n",
      "3413 Training accuracy = 0.95 Loss = 0.266199\n",
      "3414 Training accuracy = 0.94 Loss = 0.201384\n",
      "3415 Training accuracy = 0.94 Loss = 0.254208\n",
      "3416 Training accuracy = 0.91 Loss = 0.460768\n",
      "3417 Training accuracy = 0.9 Loss = 0.289501\n",
      "3418 Training accuracy = 0.95 Loss = 0.258493\n",
      "3419 Training accuracy = 0.91 Loss = 0.253653\n",
      "3420 Training accuracy = 0.9 Loss = 0.28532\n",
      "3421 Training accuracy = 0.94 Loss = 0.197051\n",
      "3422 Training accuracy = 0.96 Loss = 0.205935\n",
      "3423 Training accuracy = 0.89 Loss = 0.267971\n",
      "3424 Training accuracy = 0.97 Loss = 0.141681\n",
      "3425 Training accuracy = 0.93 Loss = 0.245383\n",
      "3426 Training accuracy = 0.93 Loss = 0.283558\n",
      "3427 Training accuracy = 0.9 Loss = 0.310165\n",
      "3428 Training accuracy = 0.9 Loss = 0.223196\n",
      "3429 Training accuracy = 0.88 Loss = 0.584988\n",
      "3430 Training accuracy = 0.95 Loss = 0.179987\n",
      "3431 Training accuracy = 0.91 Loss = 0.451032\n",
      "3432 Training accuracy = 0.89 Loss = 0.360563\n",
      "3433 Training accuracy = 0.95 Loss = 0.237748\n",
      "3434 Training accuracy = 0.91 Loss = 0.277521\n",
      "3435 Training accuracy = 0.91 Loss = 0.290651\n",
      "3436 Training accuracy = 0.92 Loss = 0.365875\n",
      "3437 Training accuracy = 0.93 Loss = 0.214154\n",
      "3438 Training accuracy = 0.93 Loss = 0.243628\n",
      "3439 Training accuracy = 0.92 Loss = 0.294481\n",
      "3440 Training accuracy = 0.94 Loss = 0.18167\n",
      "3441 Training accuracy = 0.91 Loss = 0.309965\n",
      "3442 Training accuracy = 0.93 Loss = 0.20102\n",
      "3443 Training accuracy = 0.9 Loss = 0.319963\n",
      "3444 Training accuracy = 0.89 Loss = 0.305685\n",
      "3445 Training accuracy = 0.94 Loss = 0.296912\n",
      "3446 Training accuracy = 0.9 Loss = 0.249954\n",
      "3447 Training accuracy = 0.92 Loss = 0.250699\n",
      "3448 Training accuracy = 0.9 Loss = 0.471528\n",
      "3449 Training accuracy = 0.95 Loss = 0.186394\n",
      "3450 Training accuracy = 0.94 Loss = 0.244478\n",
      "3451 Training accuracy = 0.91 Loss = 0.363134\n",
      "3452 Training accuracy = 0.95 Loss = 0.186314\n",
      "3453 Training accuracy = 0.9 Loss = 0.26992\n",
      "3454 Training accuracy = 0.92 Loss = 0.31384\n",
      "3455 Training accuracy = 0.93 Loss = 0.203429\n",
      "3456 Training accuracy = 0.93 Loss = 0.293501\n",
      "3457 Training accuracy = 0.93 Loss = 0.189969\n",
      "3458 Training accuracy = 0.94 Loss = 0.246208\n",
      "3459 Training accuracy = 0.9 Loss = 0.378558\n",
      "3460 Training accuracy = 0.93 Loss = 0.259056\n",
      "3461 Training accuracy = 0.96 Loss = 0.159459\n",
      "3462 Training accuracy = 0.91 Loss = 0.359936\n",
      "3463 Training accuracy = 0.93 Loss = 0.306018\n",
      "3464 Training accuracy = 0.91 Loss = 0.259737\n",
      "3465 Training accuracy = 0.93 Loss = 0.285125\n",
      "3466 Training accuracy = 0.93 Loss = 0.263228\n",
      "3467 Training accuracy = 0.93 Loss = 0.191822\n",
      "3468 Training accuracy = 0.91 Loss = 0.213326\n",
      "3469 Training accuracy = 0.93 Loss = 0.22455\n",
      "3470 Training accuracy = 0.93 Loss = 0.274969\n",
      "3471 Training accuracy = 0.94 Loss = 0.229388\n",
      "3472 Training accuracy = 0.95 Loss = 0.209368\n",
      "3473 Training accuracy = 0.98 Loss = 0.114047\n",
      "3474 Training accuracy = 0.96 Loss = 0.168407\n",
      "3475 Training accuracy = 0.89 Loss = 0.328929\n",
      "3476 Training accuracy = 0.95 Loss = 0.201232\n",
      "3477 Training accuracy = 0.94 Loss = 0.236863\n",
      "3478 Training accuracy = 0.9 Loss = 0.37097\n",
      "3479 Training accuracy = 0.9 Loss = 0.388687\n",
      "3480 Training accuracy = 0.88 Loss = 0.464695\n",
      "3481 Training accuracy = 0.92 Loss = 0.272186\n",
      "3482 Training accuracy = 0.94 Loss = 0.227271\n",
      "3483 Training accuracy = 0.92 Loss = 0.250547\n",
      "3484 Training accuracy = 0.93 Loss = 0.248059\n",
      "3485 Training accuracy = 0.88 Loss = 0.416621\n",
      "3486 Training accuracy = 0.93 Loss = 0.17629\n",
      "3487 Training accuracy = 0.92 Loss = 0.26822\n",
      "3488 Training accuracy = 0.96 Loss = 0.144\n",
      "3489 Training accuracy = 0.88 Loss = 0.308075\n",
      "3490 Training accuracy = 0.93 Loss = 0.260593\n",
      "3491 Training accuracy = 0.94 Loss = 0.240745\n",
      "3492 Training accuracy = 0.91 Loss = 0.305289\n",
      "3493 Training accuracy = 0.93 Loss = 0.248569\n",
      "3494 Training accuracy = 0.97 Loss = 0.141728\n",
      "3495 Training accuracy = 0.97 Loss = 0.157013\n",
      "3496 Training accuracy = 0.91 Loss = 0.376958\n",
      "3497 Training accuracy = 0.92 Loss = 0.377602\n",
      "3498 Training accuracy = 0.9 Loss = 0.266752\n",
      "3499 Training accuracy = 0.95 Loss = 0.224381\n",
      "3500 Training accuracy = 0.9 Loss = 0.335576\n",
      "3501 Training accuracy = 0.91 Loss = 0.436341\n",
      "3502 Training accuracy = 0.92 Loss = 0.280121\n",
      "3503 Training accuracy = 0.91 Loss = 0.275721\n",
      "3504 Training accuracy = 0.94 Loss = 0.277946\n",
      "3505 Training accuracy = 0.9 Loss = 0.374207\n",
      "3506 Training accuracy = 0.92 Loss = 0.286602\n",
      "3507 Training accuracy = 0.94 Loss = 0.196833\n",
      "3508 Training accuracy = 0.9 Loss = 0.3829\n",
      "3509 Training accuracy = 0.92 Loss = 0.392204\n",
      "3510 Training accuracy = 0.94 Loss = 0.243622\n",
      "3511 Training accuracy = 0.94 Loss = 0.255333\n",
      "3512 Training accuracy = 0.93 Loss = 0.320175\n",
      "3513 Training accuracy = 0.97 Loss = 0.200901\n",
      "3514 Training accuracy = 0.89 Loss = 0.422611\n",
      "3515 Training accuracy = 0.96 Loss = 0.171981\n",
      "3516 Training accuracy = 0.93 Loss = 0.238494\n",
      "3517 Training accuracy = 0.92 Loss = 0.24238\n",
      "3518 Training accuracy = 0.92 Loss = 0.33734\n",
      "3519 Training accuracy = 0.96 Loss = 0.269665\n",
      "3520 Training accuracy = 0.96 Loss = 0.168685\n",
      "3521 Training accuracy = 0.94 Loss = 0.267764\n",
      "3522 Training accuracy = 0.89 Loss = 0.328141\n",
      "3523 Training accuracy = 0.96 Loss = 0.354175\n",
      "3524 Training accuracy = 0.95 Loss = 0.197313\n",
      "3525 Training accuracy = 0.94 Loss = 0.172705\n",
      "3526 Training accuracy = 0.9 Loss = 0.330775\n",
      "3527 Training accuracy = 0.95 Loss = 0.313564\n",
      "3528 Training accuracy = 0.97 Loss = 0.199401\n",
      "3529 Training accuracy = 0.9 Loss = 0.384077\n",
      "3530 Training accuracy = 0.97 Loss = 0.152553\n",
      "3531 Training accuracy = 0.97 Loss = 0.212638\n",
      "3532 Training accuracy = 0.93 Loss = 0.250727\n",
      "3533 Training accuracy = 0.92 Loss = 0.343374\n",
      "3534 Training accuracy = 0.94 Loss = 0.2886\n",
      "3535 Training accuracy = 0.91 Loss = 0.293072\n",
      "3536 Training accuracy = 0.93 Loss = 0.217676\n",
      "3537 Training accuracy = 0.91 Loss = 0.292079\n",
      "3538 Training accuracy = 0.95 Loss = 0.190266\n",
      "3539 Training accuracy = 0.93 Loss = 0.27454\n",
      "3540 Training accuracy = 0.91 Loss = 0.322591\n",
      "3541 Training accuracy = 0.9 Loss = 0.255198\n",
      "3542 Training accuracy = 0.95 Loss = 0.20787\n",
      "3543 Training accuracy = 0.96 Loss = 0.189586\n",
      "3544 Training accuracy = 0.95 Loss = 0.290496\n",
      "3545 Training accuracy = 0.88 Loss = 0.324638\n",
      "3546 Training accuracy = 0.95 Loss = 0.229062\n",
      "3547 Training accuracy = 0.94 Loss = 0.233569\n",
      "3548 Training accuracy = 0.93 Loss = 0.268048\n",
      "3549 Training accuracy = 0.97 Loss = 0.131648\n",
      "3550 Training accuracy = 0.97 Loss = 0.132141\n",
      "3551 Training accuracy = 0.9 Loss = 0.201018\n",
      "3552 Training accuracy = 0.96 Loss = 0.1469\n",
      "3553 Training accuracy = 0.93 Loss = 0.185604\n",
      "3554 Training accuracy = 0.94 Loss = 0.20812\n",
      "3555 Training accuracy = 0.89 Loss = 0.363727\n",
      "3556 Training accuracy = 0.92 Loss = 0.233961\n",
      "3557 Training accuracy = 0.9 Loss = 0.372674\n",
      "3558 Training accuracy = 0.93 Loss = 0.225867\n",
      "3559 Training accuracy = 0.94 Loss = 0.248887\n",
      "3560 Training accuracy = 0.93 Loss = 0.288642\n",
      "3561 Training accuracy = 0.92 Loss = 0.184786\n",
      "3562 Training accuracy = 0.94 Loss = 0.19422\n",
      "3563 Training accuracy = 0.94 Loss = 0.215045\n",
      "3564 Training accuracy = 0.93 Loss = 0.256322\n",
      "3565 Training accuracy = 0.89 Loss = 0.333535\n",
      "3566 Training accuracy = 0.92 Loss = 0.392813\n",
      "3567 Training accuracy = 0.93 Loss = 0.25304\n",
      "3568 Training accuracy = 0.94 Loss = 0.238314\n",
      "3569 Training accuracy = 0.93 Loss = 0.24019\n",
      "3570 Training accuracy = 0.97 Loss = 0.141802\n",
      "3571 Training accuracy = 0.94 Loss = 0.232983\n",
      "3572 Training accuracy = 0.92 Loss = 0.335995\n",
      "3573 Training accuracy = 0.92 Loss = 0.364275\n",
      "3574 Training accuracy = 0.93 Loss = 0.232471\n",
      "3575 Training accuracy = 0.93 Loss = 0.204175\n",
      "3576 Training accuracy = 0.89 Loss = 0.370146\n",
      "3577 Training accuracy = 0.88 Loss = 0.478666\n",
      "3578 Training accuracy = 0.91 Loss = 0.32309\n",
      "3579 Training accuracy = 0.92 Loss = 0.23663\n",
      "3580 Training accuracy = 0.94 Loss = 0.21645\n",
      "3581 Training accuracy = 0.92 Loss = 0.259968\n",
      "3582 Training accuracy = 0.92 Loss = 0.352371\n",
      "3583 Training accuracy = 0.91 Loss = 0.346634\n",
      "3584 Training accuracy = 0.93 Loss = 0.22248\n",
      "3585 Training accuracy = 0.94 Loss = 0.198786\n",
      "3586 Training accuracy = 0.95 Loss = 0.238729\n",
      "3587 Training accuracy = 0.93 Loss = 0.317713\n",
      "3588 Training accuracy = 0.88 Loss = 0.315513\n",
      "3589 Training accuracy = 0.95 Loss = 0.206418\n",
      "3590 Training accuracy = 0.92 Loss = 0.261266\n",
      "3591 Training accuracy = 0.89 Loss = 0.366123\n",
      "3592 Training accuracy = 0.93 Loss = 0.175496\n",
      "3593 Training accuracy = 0.92 Loss = 0.296303\n",
      "3594 Training accuracy = 0.93 Loss = 0.230485\n",
      "3595 Training accuracy = 0.91 Loss = 0.29091\n",
      "3596 Training accuracy = 0.9 Loss = 0.296356\n",
      "3597 Training accuracy = 0.94 Loss = 0.226641\n",
      "3598 Training accuracy = 0.9 Loss = 0.376676\n",
      "3599 Training accuracy = 0.92 Loss = 0.344694\n",
      "3600 Training accuracy = 0.95 Loss = 0.212787\n",
      "3601 Training accuracy = 0.96 Loss = 0.135259\n",
      "3602 Training accuracy = 0.97 Loss = 0.157929\n",
      "3603 Training accuracy = 0.91 Loss = 0.259069\n",
      "3604 Training accuracy = 0.93 Loss = 0.198171\n",
      "3605 Training accuracy = 0.89 Loss = 0.320981\n",
      "3606 Training accuracy = 0.92 Loss = 0.200734\n",
      "3607 Training accuracy = 0.92 Loss = 0.304333\n",
      "3608 Training accuracy = 0.92 Loss = 0.226697\n",
      "3609 Training accuracy = 0.92 Loss = 0.314588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3610 Training accuracy = 0.92 Loss = 0.430495\n",
      "3611 Training accuracy = 0.84 Loss = 0.438882\n",
      "3612 Training accuracy = 0.95 Loss = 0.204917\n",
      "3613 Training accuracy = 0.91 Loss = 0.280814\n",
      "3614 Training accuracy = 0.92 Loss = 0.211314\n",
      "3615 Training accuracy = 0.96 Loss = 0.214875\n",
      "3616 Training accuracy = 0.98 Loss = 0.113706\n",
      "3617 Training accuracy = 0.93 Loss = 0.27689\n",
      "3618 Training accuracy = 0.96 Loss = 0.203082\n",
      "3619 Training accuracy = 0.92 Loss = 0.258623\n",
      "3620 Training accuracy = 0.89 Loss = 0.326514\n",
      "3621 Training accuracy = 0.92 Loss = 0.241163\n",
      "3622 Training accuracy = 0.92 Loss = 0.229863\n",
      "3623 Training accuracy = 0.93 Loss = 0.269242\n",
      "3624 Training accuracy = 0.92 Loss = 0.286982\n",
      "3625 Training accuracy = 0.92 Loss = 0.225369\n",
      "3626 Training accuracy = 0.9 Loss = 0.351925\n",
      "3627 Training accuracy = 0.92 Loss = 0.43717\n",
      "3628 Training accuracy = 0.94 Loss = 0.268343\n",
      "3629 Training accuracy = 0.91 Loss = 0.458109\n",
      "3630 Training accuracy = 0.88 Loss = 0.367391\n",
      "3631 Training accuracy = 0.95 Loss = 0.212519\n",
      "3632 Training accuracy = 0.95 Loss = 0.238589\n",
      "3633 Training accuracy = 0.88 Loss = 0.307867\n",
      "3634 Training accuracy = 0.96 Loss = 0.175782\n",
      "3635 Training accuracy = 0.97 Loss = 0.129373\n",
      "3636 Training accuracy = 0.94 Loss = 0.209535\n",
      "3637 Training accuracy = 0.94 Loss = 0.154555\n",
      "3638 Training accuracy = 0.96 Loss = 0.207699\n",
      "3639 Training accuracy = 0.91 Loss = 0.293374\n",
      "3640 Training accuracy = 0.89 Loss = 0.33123\n",
      "3641 Training accuracy = 0.91 Loss = 0.321934\n",
      "3642 Training accuracy = 0.95 Loss = 0.197644\n",
      "3643 Training accuracy = 0.96 Loss = 0.135374\n",
      "3644 Training accuracy = 0.89 Loss = 0.360997\n",
      "3645 Training accuracy = 0.93 Loss = 0.257834\n",
      "3646 Training accuracy = 0.92 Loss = 0.264155\n",
      "3647 Training accuracy = 0.95 Loss = 0.248137\n",
      "3648 Training accuracy = 0.91 Loss = 0.253789\n",
      "3649 Training accuracy = 0.89 Loss = 0.479872\n",
      "3650 Training accuracy = 0.95 Loss = 0.245288\n",
      "3651 Training accuracy = 0.93 Loss = 0.355583\n",
      "3652 Training accuracy = 0.97 Loss = 0.191299\n",
      "3653 Training accuracy = 0.92 Loss = 0.236686\n",
      "3654 Training accuracy = 0.91 Loss = 0.321415\n",
      "3655 Training accuracy = 0.94 Loss = 0.192677\n",
      "3656 Training accuracy = 0.91 Loss = 0.272298\n",
      "3657 Training accuracy = 0.95 Loss = 0.266199\n",
      "3658 Training accuracy = 0.88 Loss = 0.38977\n",
      "3659 Training accuracy = 0.93 Loss = 0.232606\n",
      "3660 Training accuracy = 0.95 Loss = 0.165942\n",
      "3661 Training accuracy = 0.91 Loss = 0.42958\n",
      "3662 Training accuracy = 0.92 Loss = 0.275547\n",
      "3663 Training accuracy = 0.9 Loss = 0.298524\n",
      "3664 Training accuracy = 0.93 Loss = 0.267995\n",
      "3665 Training accuracy = 0.91 Loss = 0.269805\n",
      "3666 Training accuracy = 0.88 Loss = 0.352671\n",
      "3667 Training accuracy = 0.92 Loss = 0.278794\n",
      "3668 Training accuracy = 0.9 Loss = 0.329369\n",
      "3669 Training accuracy = 0.95 Loss = 0.275655\n",
      "3670 Training accuracy = 0.92 Loss = 0.273844\n",
      "3671 Training accuracy = 0.96 Loss = 0.171226\n",
      "3672 Training accuracy = 0.93 Loss = 0.186655\n",
      "3673 Training accuracy = 0.9 Loss = 0.324816\n",
      "3674 Training accuracy = 0.92 Loss = 0.220719\n",
      "3675 Training accuracy = 0.93 Loss = 0.293578\n",
      "3676 Training accuracy = 0.94 Loss = 0.230741\n",
      "3677 Training accuracy = 0.9 Loss = 0.376865\n",
      "3678 Training accuracy = 0.93 Loss = 0.232448\n",
      "3679 Training accuracy = 0.94 Loss = 0.243796\n",
      "3680 Training accuracy = 0.96 Loss = 0.142526\n",
      "3681 Training accuracy = 0.95 Loss = 0.174869\n",
      "3682 Training accuracy = 0.91 Loss = 0.264099\n",
      "3683 Training accuracy = 0.9 Loss = 0.33141\n",
      "3684 Training accuracy = 0.86 Loss = 0.374259\n",
      "3685 Training accuracy = 0.91 Loss = 0.315964\n",
      "3686 Training accuracy = 0.92 Loss = 0.264584\n",
      "3687 Training accuracy = 0.91 Loss = 0.288953\n",
      "3688 Training accuracy = 0.94 Loss = 0.261515\n",
      "3689 Training accuracy = 0.92 Loss = 0.249947\n",
      "3690 Training accuracy = 0.93 Loss = 0.259808\n",
      "3691 Training accuracy = 0.92 Loss = 0.30243\n",
      "3692 Training accuracy = 0.94 Loss = 0.214932\n",
      "3693 Training accuracy = 0.92 Loss = 0.257352\n",
      "3694 Training accuracy = 0.93 Loss = 0.228261\n",
      "3695 Training accuracy = 0.9 Loss = 0.287273\n",
      "3696 Training accuracy = 0.89 Loss = 0.363901\n",
      "3697 Training accuracy = 0.94 Loss = 0.25878\n",
      "3698 Training accuracy = 0.93 Loss = 0.229753\n",
      "3699 Training accuracy = 0.96 Loss = 0.249101\n",
      "3700 Training accuracy = 0.92 Loss = 0.285398\n",
      "3701 Training accuracy = 0.93 Loss = 0.220765\n",
      "3702 Training accuracy = 0.88 Loss = 0.400928\n",
      "3703 Training accuracy = 0.93 Loss = 0.236901\n",
      "3704 Training accuracy = 0.89 Loss = 0.243852\n",
      "3705 Training accuracy = 0.91 Loss = 0.290433\n",
      "3706 Training accuracy = 0.93 Loss = 0.365939\n",
      "3707 Training accuracy = 0.94 Loss = 0.225143\n",
      "3708 Training accuracy = 0.96 Loss = 0.177079\n",
      "3709 Training accuracy = 0.92 Loss = 0.214799\n",
      "3710 Training accuracy = 0.92 Loss = 0.229806\n",
      "3711 Training accuracy = 0.9 Loss = 0.325855\n",
      "3712 Training accuracy = 0.96 Loss = 0.177046\n",
      "3713 Training accuracy = 0.94 Loss = 0.302537\n",
      "3714 Training accuracy = 0.89 Loss = 0.352262\n",
      "3715 Training accuracy = 0.96 Loss = 0.158382\n",
      "3716 Training accuracy = 0.89 Loss = 0.332017\n",
      "3717 Training accuracy = 0.92 Loss = 0.313432\n",
      "3718 Training accuracy = 0.95 Loss = 0.23955\n",
      "3719 Training accuracy = 0.85 Loss = 0.439265\n",
      "3720 Training accuracy = 0.95 Loss = 0.140397\n",
      "3721 Training accuracy = 0.92 Loss = 0.217058\n",
      "3722 Training accuracy = 0.98 Loss = 0.117147\n",
      "3723 Training accuracy = 0.92 Loss = 0.252109\n",
      "3724 Training accuracy = 0.95 Loss = 0.215701\n",
      "3725 Training accuracy = 0.95 Loss = 0.147119\n",
      "3726 Training accuracy = 0.89 Loss = 0.445949\n",
      "3727 Training accuracy = 0.9 Loss = 0.287279\n",
      "3728 Training accuracy = 0.91 Loss = 0.298546\n",
      "3729 Training accuracy = 0.91 Loss = 0.326631\n",
      "3730 Training accuracy = 0.9 Loss = 0.29352\n",
      "3731 Training accuracy = 0.9 Loss = 0.350491\n",
      "3732 Training accuracy = 0.94 Loss = 0.225142\n",
      "3733 Training accuracy = 0.93 Loss = 0.240149\n",
      "3734 Training accuracy = 0.93 Loss = 0.334076\n",
      "3735 Training accuracy = 0.98 Loss = 0.143589\n",
      "3736 Training accuracy = 0.92 Loss = 0.386341\n",
      "3737 Training accuracy = 0.9 Loss = 0.26426\n",
      "3738 Training accuracy = 0.98 Loss = 0.111333\n",
      "3739 Training accuracy = 0.94 Loss = 0.275446\n",
      "3740 Training accuracy = 0.89 Loss = 0.368892\n",
      "3741 Training accuracy = 0.94 Loss = 0.230176\n",
      "3742 Training accuracy = 0.91 Loss = 0.320645\n",
      "3743 Training accuracy = 0.94 Loss = 0.267049\n",
      "3744 Training accuracy = 0.94 Loss = 0.287567\n",
      "3745 Training accuracy = 0.93 Loss = 0.248833\n",
      "3746 Training accuracy = 0.98 Loss = 0.143099\n",
      "3747 Training accuracy = 0.92 Loss = 0.233274\n",
      "3748 Training accuracy = 0.92 Loss = 0.299905\n",
      "3749 Training accuracy = 0.91 Loss = 0.295542\n",
      "3750 Training accuracy = 0.96 Loss = 0.234915\n",
      "3751 Training accuracy = 0.91 Loss = 0.267665\n",
      "3752 Training accuracy = 0.89 Loss = 0.373399\n",
      "3753 Training accuracy = 0.86 Loss = 0.425029\n",
      "3754 Training accuracy = 0.93 Loss = 0.305564\n",
      "3755 Training accuracy = 0.96 Loss = 0.167251\n",
      "3756 Training accuracy = 0.92 Loss = 0.307548\n",
      "3757 Training accuracy = 0.94 Loss = 0.221852\n",
      "3758 Training accuracy = 0.86 Loss = 0.382392\n",
      "3759 Training accuracy = 0.89 Loss = 0.274531\n",
      "3760 Training accuracy = 0.9 Loss = 0.380014\n",
      "3761 Training accuracy = 0.94 Loss = 0.206325\n",
      "3762 Training accuracy = 0.88 Loss = 0.38167\n",
      "3763 Training accuracy = 0.87 Loss = 0.420355\n",
      "3764 Training accuracy = 0.91 Loss = 0.25023\n",
      "3765 Training accuracy = 0.9 Loss = 0.284609\n",
      "3766 Training accuracy = 0.93 Loss = 0.295509\n",
      "3767 Training accuracy = 0.89 Loss = 0.384571\n",
      "3768 Training accuracy = 0.93 Loss = 0.237619\n",
      "3769 Training accuracy = 0.95 Loss = 0.287642\n",
      "3770 Training accuracy = 0.9 Loss = 0.282581\n",
      "3771 Training accuracy = 0.9 Loss = 0.348589\n",
      "3772 Training accuracy = 0.91 Loss = 0.346858\n",
      "3773 Training accuracy = 0.97 Loss = 0.13416\n",
      "3774 Training accuracy = 0.91 Loss = 0.198763\n",
      "3775 Training accuracy = 0.96 Loss = 0.160899\n",
      "3776 Training accuracy = 0.95 Loss = 0.196379\n",
      "3777 Training accuracy = 0.91 Loss = 0.308404\n",
      "3778 Training accuracy = 0.91 Loss = 0.379375\n",
      "3779 Training accuracy = 0.93 Loss = 0.314303\n",
      "3780 Training accuracy = 0.94 Loss = 0.193841\n",
      "3781 Training accuracy = 0.9 Loss = 0.295761\n",
      "3782 Training accuracy = 0.88 Loss = 0.36505\n",
      "3783 Training accuracy = 0.94 Loss = 0.218974\n",
      "3784 Training accuracy = 0.94 Loss = 0.348206\n",
      "3785 Training accuracy = 0.92 Loss = 0.227302\n",
      "3786 Training accuracy = 0.91 Loss = 0.298376\n",
      "3787 Training accuracy = 0.95 Loss = 0.166169\n",
      "3788 Training accuracy = 0.87 Loss = 0.4221\n",
      "3789 Training accuracy = 0.94 Loss = 0.208806\n",
      "3790 Training accuracy = 0.91 Loss = 0.274315\n",
      "3791 Training accuracy = 0.96 Loss = 0.149819\n",
      "3792 Training accuracy = 0.9 Loss = 0.300964\n",
      "3793 Training accuracy = 0.95 Loss = 0.156148\n",
      "3794 Training accuracy = 0.94 Loss = 0.212159\n",
      "3795 Training accuracy = 0.95 Loss = 0.197624\n",
      "3796 Training accuracy = 0.89 Loss = 0.288873\n",
      "3797 Training accuracy = 0.97 Loss = 0.22284\n",
      "3798 Training accuracy = 0.94 Loss = 0.172731\n",
      "3799 Training accuracy = 0.95 Loss = 0.154567\n",
      "3800 Training accuracy = 0.93 Loss = 0.235942\n",
      "3801 Training accuracy = 0.9 Loss = 0.35677\n",
      "3802 Training accuracy = 0.89 Loss = 0.350597\n",
      "3803 Training accuracy = 0.91 Loss = 0.270018\n",
      "3804 Training accuracy = 0.95 Loss = 0.24758\n",
      "3805 Training accuracy = 0.94 Loss = 0.256863\n",
      "3806 Training accuracy = 0.9 Loss = 0.335915\n",
      "3807 Training accuracy = 0.9 Loss = 0.324503\n",
      "3808 Training accuracy = 0.91 Loss = 0.297535\n",
      "3809 Training accuracy = 0.9 Loss = 0.354689\n",
      "3810 Training accuracy = 0.92 Loss = 0.235383\n",
      "3811 Training accuracy = 0.95 Loss = 0.20064\n",
      "3812 Training accuracy = 0.91 Loss = 0.382731\n",
      "3813 Training accuracy = 0.95 Loss = 0.24365\n",
      "3814 Training accuracy = 0.94 Loss = 0.206319\n",
      "3815 Training accuracy = 0.95 Loss = 0.188525\n",
      "3816 Training accuracy = 0.91 Loss = 0.328995\n",
      "3817 Training accuracy = 0.92 Loss = 0.283785\n",
      "3818 Training accuracy = 0.95 Loss = 0.224298\n",
      "3819 Training accuracy = 0.91 Loss = 0.301234\n",
      "3820 Training accuracy = 0.92 Loss = 0.279469\n",
      "3821 Training accuracy = 0.93 Loss = 0.304339\n",
      "3822 Training accuracy = 0.94 Loss = 0.196533\n",
      "3823 Training accuracy = 0.9 Loss = 0.310113\n",
      "3824 Training accuracy = 0.9 Loss = 0.308253\n",
      "3825 Training accuracy = 0.93 Loss = 0.184055\n",
      "3826 Training accuracy = 0.96 Loss = 0.193002\n",
      "3827 Training accuracy = 0.87 Loss = 0.39501\n",
      "3828 Training accuracy = 0.93 Loss = 0.400257\n",
      "3829 Training accuracy = 0.95 Loss = 0.239089\n",
      "3830 Training accuracy = 0.85 Loss = 0.450101\n",
      "3831 Training accuracy = 0.91 Loss = 0.415579\n",
      "3832 Training accuracy = 0.92 Loss = 0.265554\n",
      "3833 Training accuracy = 0.95 Loss = 0.141316\n",
      "3834 Training accuracy = 0.94 Loss = 0.174102\n",
      "3835 Training accuracy = 0.91 Loss = 0.221642\n",
      "3836 Training accuracy = 0.93 Loss = 0.241482\n",
      "3837 Training accuracy = 0.92 Loss = 0.274627\n",
      "3838 Training accuracy = 0.93 Loss = 0.236009\n",
      "3839 Training accuracy = 0.92 Loss = 0.305834\n",
      "3840 Training accuracy = 0.97 Loss = 0.139076\n",
      "3841 Training accuracy = 0.93 Loss = 0.277696\n",
      "3842 Training accuracy = 0.95 Loss = 0.263482\n",
      "3843 Training accuracy = 0.95 Loss = 0.206207\n",
      "3844 Training accuracy = 0.93 Loss = 0.157624\n",
      "3845 Training accuracy = 0.95 Loss = 0.158641\n",
      "3846 Training accuracy = 0.94 Loss = 0.248673\n",
      "3847 Training accuracy = 0.95 Loss = 0.183814\n",
      "3848 Training accuracy = 0.91 Loss = 0.312545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849 Training accuracy = 0.95 Loss = 0.213782\n",
      "3850 Training accuracy = 0.89 Loss = 0.335436\n",
      "3851 Training accuracy = 0.91 Loss = 0.285629\n",
      "3852 Training accuracy = 0.9 Loss = 0.352116\n",
      "3853 Training accuracy = 0.94 Loss = 0.247496\n",
      "3854 Training accuracy = 0.85 Loss = 0.427759\n",
      "3855 Training accuracy = 0.96 Loss = 0.157833\n",
      "3856 Training accuracy = 0.95 Loss = 0.250264\n",
      "3857 Training accuracy = 0.94 Loss = 0.248956\n",
      "3858 Training accuracy = 0.96 Loss = 0.15589\n",
      "3859 Training accuracy = 0.93 Loss = 0.258933\n",
      "3860 Training accuracy = 0.92 Loss = 0.295905\n",
      "3861 Training accuracy = 0.96 Loss = 0.227699\n",
      "3862 Training accuracy = 0.9 Loss = 0.335884\n",
      "3863 Training accuracy = 0.92 Loss = 0.257712\n",
      "3864 Training accuracy = 0.93 Loss = 0.240166\n",
      "3865 Training accuracy = 0.93 Loss = 0.236053\n",
      "3866 Training accuracy = 0.93 Loss = 0.184125\n",
      "3867 Training accuracy = 0.96 Loss = 0.218557\n",
      "3868 Training accuracy = 0.91 Loss = 0.218781\n",
      "3869 Training accuracy = 0.93 Loss = 0.252474\n",
      "3870 Training accuracy = 0.93 Loss = 0.16346\n",
      "3871 Training accuracy = 0.94 Loss = 0.221259\n",
      "3872 Training accuracy = 0.95 Loss = 0.167845\n",
      "3873 Training accuracy = 0.98 Loss = 0.124661\n",
      "3874 Training accuracy = 0.86 Loss = 0.490229\n",
      "3875 Training accuracy = 0.93 Loss = 0.194594\n",
      "3876 Training accuracy = 0.92 Loss = 0.371861\n",
      "3877 Training accuracy = 0.93 Loss = 0.301779\n",
      "3878 Training accuracy = 0.95 Loss = 0.190761\n",
      "3879 Training accuracy = 0.94 Loss = 0.22386\n",
      "3880 Training accuracy = 0.89 Loss = 0.34063\n",
      "3881 Training accuracy = 0.95 Loss = 0.231535\n",
      "3882 Training accuracy = 0.94 Loss = 0.276133\n",
      "3883 Training accuracy = 0.94 Loss = 0.217337\n",
      "3884 Training accuracy = 0.91 Loss = 0.347004\n",
      "3885 Training accuracy = 0.89 Loss = 0.32316\n",
      "3886 Training accuracy = 0.96 Loss = 0.168839\n",
      "3887 Training accuracy = 0.92 Loss = 0.302033\n",
      "3888 Training accuracy = 0.96 Loss = 0.232955\n",
      "3889 Training accuracy = 0.94 Loss = 0.288531\n",
      "3890 Training accuracy = 0.95 Loss = 0.291065\n",
      "3891 Training accuracy = 0.96 Loss = 0.240007\n",
      "3892 Training accuracy = 0.91 Loss = 0.274535\n",
      "3893 Training accuracy = 0.93 Loss = 0.292395\n",
      "3894 Training accuracy = 0.93 Loss = 0.257142\n",
      "3895 Training accuracy = 0.92 Loss = 0.204967\n",
      "3896 Training accuracy = 0.92 Loss = 0.294198\n",
      "3897 Training accuracy = 0.89 Loss = 0.494612\n",
      "3898 Training accuracy = 0.94 Loss = 0.268506\n",
      "3899 Training accuracy = 0.94 Loss = 0.220472\n",
      "3900 Training accuracy = 0.93 Loss = 0.213639\n",
      "3901 Training accuracy = 0.91 Loss = 0.376387\n",
      "3902 Training accuracy = 0.93 Loss = 0.273877\n",
      "3903 Training accuracy = 0.93 Loss = 0.310673\n",
      "3904 Training accuracy = 0.93 Loss = 0.21095\n",
      "3905 Training accuracy = 0.93 Loss = 0.225738\n",
      "3906 Training accuracy = 0.9 Loss = 0.429779\n",
      "3907 Training accuracy = 0.94 Loss = 0.302358\n",
      "3908 Training accuracy = 0.92 Loss = 0.300508\n",
      "3909 Training accuracy = 0.91 Loss = 0.444928\n",
      "3910 Training accuracy = 0.9 Loss = 0.389783\n",
      "3911 Training accuracy = 0.97 Loss = 0.194021\n",
      "3912 Training accuracy = 0.92 Loss = 0.23434\n",
      "3913 Training accuracy = 0.93 Loss = 0.224234\n",
      "3914 Training accuracy = 0.93 Loss = 0.278714\n",
      "3915 Training accuracy = 0.94 Loss = 0.244951\n",
      "3916 Training accuracy = 0.93 Loss = 0.22295\n",
      "3917 Training accuracy = 0.93 Loss = 0.206669\n",
      "3918 Training accuracy = 0.92 Loss = 0.190477\n",
      "3919 Training accuracy = 0.89 Loss = 0.321382\n",
      "3920 Training accuracy = 0.93 Loss = 0.231186\n",
      "3921 Training accuracy = 0.91 Loss = 0.231663\n",
      "3922 Training accuracy = 0.92 Loss = 0.228048\n",
      "3923 Training accuracy = 0.91 Loss = 0.270583\n",
      "3924 Training accuracy = 0.93 Loss = 0.270408\n",
      "3925 Training accuracy = 0.92 Loss = 0.398424\n",
      "3926 Training accuracy = 0.94 Loss = 0.227263\n",
      "3927 Training accuracy = 0.95 Loss = 0.198089\n",
      "3928 Training accuracy = 0.92 Loss = 0.216269\n",
      "3929 Training accuracy = 0.97 Loss = 0.120222\n",
      "3930 Training accuracy = 0.93 Loss = 0.231028\n",
      "3931 Training accuracy = 0.92 Loss = 0.26806\n",
      "3932 Training accuracy = 0.91 Loss = 0.344228\n",
      "3933 Training accuracy = 0.96 Loss = 0.150598\n",
      "3934 Training accuracy = 0.89 Loss = 0.381874\n",
      "3935 Training accuracy = 0.91 Loss = 0.200427\n",
      "3936 Training accuracy = 0.92 Loss = 0.290621\n",
      "3937 Training accuracy = 0.92 Loss = 0.368732\n",
      "3938 Training accuracy = 0.97 Loss = 0.175224\n",
      "3939 Training accuracy = 0.9 Loss = 0.31676\n",
      "3940 Training accuracy = 0.9 Loss = 0.268234\n",
      "3941 Training accuracy = 0.92 Loss = 0.381711\n",
      "3942 Training accuracy = 0.92 Loss = 0.202024\n",
      "3943 Training accuracy = 0.94 Loss = 0.352688\n",
      "3944 Training accuracy = 0.95 Loss = 0.178094\n",
      "3945 Training accuracy = 0.89 Loss = 0.302303\n",
      "3946 Training accuracy = 0.9 Loss = 0.343418\n",
      "3947 Training accuracy = 0.93 Loss = 0.231733\n",
      "3948 Training accuracy = 0.92 Loss = 0.219117\n",
      "3949 Training accuracy = 0.93 Loss = 0.311057\n",
      "3950 Training accuracy = 0.94 Loss = 0.211805\n",
      "3951 Training accuracy = 0.94 Loss = 0.303217\n",
      "3952 Training accuracy = 0.89 Loss = 0.431858\n",
      "3953 Training accuracy = 0.89 Loss = 0.350008\n",
      "3954 Training accuracy = 0.9 Loss = 0.399557\n",
      "3955 Training accuracy = 0.89 Loss = 0.438798\n",
      "3956 Training accuracy = 0.89 Loss = 0.384829\n",
      "3957 Training accuracy = 0.97 Loss = 0.153709\n",
      "3958 Training accuracy = 0.89 Loss = 0.304651\n",
      "3959 Training accuracy = 0.95 Loss = 0.194157\n",
      "3960 Training accuracy = 0.97 Loss = 0.106344\n",
      "3961 Training accuracy = 0.92 Loss = 0.275482\n",
      "3962 Training accuracy = 0.9 Loss = 0.227987\n",
      "3963 Training accuracy = 0.97 Loss = 0.149632\n",
      "3964 Training accuracy = 0.95 Loss = 0.156505\n",
      "3965 Training accuracy = 0.9 Loss = 0.27994\n",
      "3966 Training accuracy = 0.9 Loss = 0.318754\n",
      "3967 Training accuracy = 0.94 Loss = 0.166679\n",
      "3968 Training accuracy = 0.95 Loss = 0.301224\n",
      "3969 Training accuracy = 0.92 Loss = 0.239911\n",
      "3970 Training accuracy = 0.92 Loss = 0.202608\n",
      "3971 Training accuracy = 0.89 Loss = 0.351003\n",
      "3972 Training accuracy = 0.92 Loss = 0.197411\n",
      "3973 Training accuracy = 0.93 Loss = 0.355936\n",
      "3974 Training accuracy = 0.9 Loss = 0.463836\n",
      "3975 Training accuracy = 0.95 Loss = 0.185321\n",
      "3976 Training accuracy = 0.98 Loss = 0.177133\n",
      "3977 Training accuracy = 0.91 Loss = 0.318348\n",
      "3978 Training accuracy = 0.95 Loss = 0.188403\n",
      "3979 Training accuracy = 0.93 Loss = 0.291955\n",
      "3980 Training accuracy = 0.96 Loss = 0.198068\n",
      "3981 Training accuracy = 0.9 Loss = 0.477893\n",
      "3982 Training accuracy = 0.97 Loss = 0.116886\n",
      "3983 Training accuracy = 0.94 Loss = 0.364802\n",
      "3984 Training accuracy = 0.96 Loss = 0.178843\n",
      "3985 Training accuracy = 0.94 Loss = 0.316978\n",
      "3986 Training accuracy = 0.94 Loss = 0.367621\n",
      "3987 Training accuracy = 0.91 Loss = 0.252556\n",
      "3988 Training accuracy = 0.94 Loss = 0.167578\n",
      "3989 Training accuracy = 0.92 Loss = 0.266096\n",
      "3990 Training accuracy = 0.94 Loss = 0.150649\n",
      "3991 Training accuracy = 0.91 Loss = 0.23753\n",
      "3992 Training accuracy = 0.92 Loss = 0.305813\n",
      "3993 Training accuracy = 0.95 Loss = 0.170905\n",
      "3994 Training accuracy = 0.91 Loss = 0.329285\n",
      "3995 Training accuracy = 0.97 Loss = 0.100014\n",
      "3996 Training accuracy = 0.87 Loss = 0.388363\n",
      "3997 Training accuracy = 0.91 Loss = 0.37982\n",
      "3998 Training accuracy = 0.95 Loss = 0.285399\n",
      "3999 Training accuracy = 0.9 Loss = 0.341493\n",
      "4000 Training accuracy = 0.93 Loss = 0.236529\n",
      "4001 Training accuracy = 0.93 Loss = 0.24665\n",
      "4002 Training accuracy = 0.96 Loss = 0.152142\n",
      "4003 Training accuracy = 0.95 Loss = 0.161521\n",
      "4004 Training accuracy = 0.89 Loss = 0.441353\n",
      "4005 Training accuracy = 0.93 Loss = 0.159663\n",
      "4006 Training accuracy = 0.91 Loss = 0.28077\n",
      "4007 Training accuracy = 0.92 Loss = 0.331826\n",
      "4008 Training accuracy = 0.88 Loss = 0.242786\n",
      "4009 Training accuracy = 0.94 Loss = 0.216495\n",
      "4010 Training accuracy = 0.97 Loss = 0.153051\n",
      "4011 Training accuracy = 0.91 Loss = 0.242397\n",
      "4012 Training accuracy = 0.91 Loss = 0.281212\n",
      "4013 Training accuracy = 0.93 Loss = 0.235243\n",
      "4014 Training accuracy = 0.89 Loss = 0.404979\n",
      "4015 Training accuracy = 0.92 Loss = 0.210981\n",
      "4016 Training accuracy = 0.92 Loss = 0.252828\n",
      "4017 Training accuracy = 0.91 Loss = 0.20488\n",
      "4018 Training accuracy = 0.95 Loss = 0.171679\n",
      "4019 Training accuracy = 0.95 Loss = 0.145491\n",
      "4020 Training accuracy = 0.91 Loss = 0.238718\n",
      "4021 Training accuracy = 0.96 Loss = 0.182077\n",
      "4022 Training accuracy = 0.97 Loss = 0.153025\n",
      "4023 Training accuracy = 0.95 Loss = 0.166941\n",
      "4024 Training accuracy = 0.94 Loss = 0.201371\n",
      "4025 Training accuracy = 0.94 Loss = 0.200276\n",
      "4026 Training accuracy = 0.91 Loss = 0.232446\n",
      "4027 Training accuracy = 0.89 Loss = 0.305076\n",
      "4028 Training accuracy = 0.9 Loss = 0.310818\n",
      "4029 Training accuracy = 0.91 Loss = 0.249498\n",
      "4030 Training accuracy = 0.91 Loss = 0.252089\n",
      "4031 Training accuracy = 0.92 Loss = 0.324348\n",
      "4032 Training accuracy = 0.95 Loss = 0.165748\n",
      "4033 Training accuracy = 0.94 Loss = 0.259006\n",
      "4034 Training accuracy = 0.95 Loss = 0.264929\n",
      "4035 Training accuracy = 0.91 Loss = 0.302223\n",
      "4036 Training accuracy = 0.93 Loss = 0.319343\n",
      "4037 Training accuracy = 0.96 Loss = 0.172681\n",
      "4038 Training accuracy = 0.95 Loss = 0.20379\n",
      "4039 Training accuracy = 0.94 Loss = 0.285054\n",
      "4040 Training accuracy = 0.92 Loss = 0.286213\n",
      "4041 Training accuracy = 0.89 Loss = 0.356361\n",
      "4042 Training accuracy = 0.94 Loss = 0.175706\n",
      "4043 Training accuracy = 0.9 Loss = 0.399648\n",
      "4044 Training accuracy = 0.9 Loss = 0.351088\n",
      "4045 Training accuracy = 0.86 Loss = 0.474473\n",
      "4046 Training accuracy = 0.93 Loss = 0.198365\n",
      "4047 Training accuracy = 0.95 Loss = 0.169578\n",
      "4048 Training accuracy = 0.94 Loss = 0.245293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4049 Training accuracy = 0.93 Loss = 0.284444\n",
      "4050 Training accuracy = 0.9 Loss = 0.414999\n",
      "4051 Training accuracy = 0.92 Loss = 0.248189\n",
      "4052 Training accuracy = 0.88 Loss = 0.254117\n",
      "4053 Training accuracy = 0.97 Loss = 0.153868\n",
      "4054 Training accuracy = 0.92 Loss = 0.439224\n",
      "4055 Training accuracy = 0.94 Loss = 0.22065\n",
      "4056 Training accuracy = 0.9 Loss = 0.396494\n",
      "4057 Training accuracy = 0.92 Loss = 0.241512\n",
      "4058 Training accuracy = 0.95 Loss = 0.170542\n",
      "4059 Training accuracy = 0.93 Loss = 0.1994\n",
      "4060 Training accuracy = 0.94 Loss = 0.312918\n",
      "4061 Training accuracy = 0.93 Loss = 0.186644\n",
      "4062 Training accuracy = 0.95 Loss = 0.20905\n",
      "4063 Training accuracy = 0.94 Loss = 0.217288\n",
      "4064 Training accuracy = 0.9 Loss = 0.369302\n",
      "4065 Training accuracy = 0.95 Loss = 0.23792\n",
      "4066 Training accuracy = 0.94 Loss = 0.263947\n",
      "4067 Training accuracy = 0.88 Loss = 0.421159\n",
      "4068 Training accuracy = 0.94 Loss = 0.163644\n",
      "4069 Training accuracy = 0.91 Loss = 0.275039\n",
      "4070 Training accuracy = 0.9 Loss = 0.384518\n",
      "4071 Training accuracy = 0.97 Loss = 0.175776\n",
      "4072 Training accuracy = 0.94 Loss = 0.4484\n",
      "4073 Training accuracy = 0.92 Loss = 0.297082\n",
      "4074 Training accuracy = 0.94 Loss = 0.219909\n",
      "4075 Training accuracy = 0.89 Loss = 0.312291\n",
      "4076 Training accuracy = 0.9 Loss = 0.34413\n",
      "4077 Training accuracy = 0.93 Loss = 0.270932\n",
      "4078 Training accuracy = 0.87 Loss = 0.546994\n",
      "4079 Training accuracy = 0.87 Loss = 0.383043\n",
      "4080 Training accuracy = 0.93 Loss = 0.345028\n",
      "4081 Training accuracy = 0.91 Loss = 0.287838\n",
      "4082 Training accuracy = 0.93 Loss = 0.406072\n",
      "4083 Training accuracy = 0.95 Loss = 0.214541\n",
      "4084 Training accuracy = 0.92 Loss = 0.267542\n",
      "4085 Training accuracy = 0.94 Loss = 0.248219\n",
      "4086 Training accuracy = 0.89 Loss = 0.331879\n",
      "4087 Training accuracy = 0.94 Loss = 0.274013\n",
      "4088 Training accuracy = 0.91 Loss = 0.383035\n",
      "4089 Training accuracy = 0.9 Loss = 0.359455\n",
      "4090 Training accuracy = 0.91 Loss = 0.304907\n",
      "4091 Training accuracy = 0.91 Loss = 0.279027\n",
      "4092 Training accuracy = 0.9 Loss = 0.242396\n",
      "4093 Training accuracy = 0.85 Loss = 0.397827\n",
      "4094 Training accuracy = 0.95 Loss = 0.301622\n",
      "4095 Training accuracy = 0.93 Loss = 0.198522\n",
      "4096 Training accuracy = 0.94 Loss = 0.164732\n",
      "4097 Training accuracy = 0.95 Loss = 0.233868\n",
      "4098 Training accuracy = 0.92 Loss = 0.222173\n",
      "4099 Training accuracy = 0.94 Loss = 0.258362\n",
      "4100 Training accuracy = 0.96 Loss = 0.217612\n",
      "4101 Training accuracy = 0.94 Loss = 0.301335\n",
      "4102 Training accuracy = 0.91 Loss = 0.2361\n",
      "4103 Training accuracy = 0.92 Loss = 0.22199\n",
      "4104 Training accuracy = 0.91 Loss = 0.40131\n",
      "4105 Training accuracy = 0.94 Loss = 0.16156\n",
      "4106 Training accuracy = 0.95 Loss = 0.192759\n",
      "4107 Training accuracy = 0.94 Loss = 0.241253\n",
      "4108 Training accuracy = 0.93 Loss = 0.231278\n",
      "4109 Training accuracy = 0.97 Loss = 0.193821\n",
      "4110 Training accuracy = 0.93 Loss = 0.248415\n",
      "4111 Training accuracy = 0.9 Loss = 0.383517\n",
      "4112 Training accuracy = 0.93 Loss = 0.30134\n",
      "4113 Training accuracy = 0.88 Loss = 0.48379\n",
      "4114 Training accuracy = 0.87 Loss = 0.456651\n",
      "4115 Training accuracy = 0.9 Loss = 0.252883\n",
      "4116 Training accuracy = 0.92 Loss = 0.28102\n",
      "4117 Training accuracy = 0.93 Loss = 0.197342\n",
      "4118 Training accuracy = 0.97 Loss = 0.178046\n",
      "4119 Training accuracy = 0.87 Loss = 0.359309\n",
      "4120 Training accuracy = 0.93 Loss = 0.238979\n",
      "4121 Training accuracy = 0.93 Loss = 0.233794\n",
      "4122 Training accuracy = 0.91 Loss = 0.328565\n",
      "4123 Training accuracy = 0.88 Loss = 0.376285\n",
      "4124 Training accuracy = 0.9 Loss = 0.32147\n",
      "4125 Training accuracy = 0.93 Loss = 0.290614\n",
      "4126 Training accuracy = 0.94 Loss = 0.3028\n",
      "4127 Training accuracy = 0.89 Loss = 0.351018\n",
      "4128 Training accuracy = 0.97 Loss = 0.227546\n",
      "4129 Training accuracy = 0.92 Loss = 0.455728\n",
      "4130 Training accuracy = 0.9 Loss = 0.363209\n",
      "4131 Training accuracy = 0.97 Loss = 0.140996\n",
      "4132 Training accuracy = 0.92 Loss = 0.137058\n",
      "4133 Training accuracy = 0.96 Loss = 0.171996\n",
      "4134 Training accuracy = 0.91 Loss = 0.290479\n",
      "4135 Training accuracy = 0.93 Loss = 0.281887\n",
      "4136 Training accuracy = 0.94 Loss = 0.257577\n",
      "4137 Training accuracy = 0.95 Loss = 0.271895\n",
      "4138 Training accuracy = 0.95 Loss = 0.254189\n",
      "4139 Training accuracy = 0.96 Loss = 0.197082\n",
      "4140 Training accuracy = 0.95 Loss = 0.234038\n",
      "4141 Training accuracy = 0.96 Loss = 0.127495\n",
      "4142 Training accuracy = 0.93 Loss = 0.329711\n",
      "4143 Training accuracy = 0.9 Loss = 0.362605\n",
      "4144 Training accuracy = 0.88 Loss = 0.338231\n",
      "4145 Training accuracy = 0.93 Loss = 0.215721\n",
      "4146 Training accuracy = 0.91 Loss = 0.251151\n",
      "4147 Training accuracy = 0.9 Loss = 0.307782\n",
      "4148 Training accuracy = 0.87 Loss = 0.46578\n",
      "4149 Training accuracy = 0.94 Loss = 0.357808\n",
      "4150 Training accuracy = 0.95 Loss = 0.251126\n",
      "4151 Training accuracy = 0.94 Loss = 0.18255\n",
      "4152 Training accuracy = 0.98 Loss = 0.115056\n",
      "4153 Training accuracy = 0.92 Loss = 0.218262\n",
      "4154 Training accuracy = 0.94 Loss = 0.26927\n",
      "4155 Training accuracy = 0.91 Loss = 0.258673\n",
      "4156 Training accuracy = 0.93 Loss = 0.240503\n",
      "4157 Training accuracy = 0.92 Loss = 0.233259\n",
      "4158 Training accuracy = 0.96 Loss = 0.157101\n",
      "4159 Training accuracy = 0.95 Loss = 0.243087\n",
      "4160 Training accuracy = 0.95 Loss = 0.217616\n",
      "4161 Training accuracy = 0.93 Loss = 0.196269\n",
      "4162 Training accuracy = 0.93 Loss = 0.200282\n",
      "4163 Training accuracy = 0.9 Loss = 0.242686\n",
      "4164 Training accuracy = 0.96 Loss = 0.146055\n",
      "4165 Training accuracy = 0.93 Loss = 0.262295\n",
      "4166 Training accuracy = 0.93 Loss = 0.265237\n",
      "4167 Training accuracy = 0.97 Loss = 0.131064\n",
      "4168 Training accuracy = 0.93 Loss = 0.344597\n",
      "4169 Training accuracy = 0.87 Loss = 0.405244\n",
      "4170 Training accuracy = 0.91 Loss = 0.258237\n",
      "4171 Training accuracy = 0.93 Loss = 0.345334\n",
      "4172 Training accuracy = 0.93 Loss = 0.203138\n",
      "4173 Training accuracy = 0.92 Loss = 0.293081\n",
      "4174 Training accuracy = 0.94 Loss = 0.260471\n",
      "4175 Training accuracy = 0.97 Loss = 0.149981\n",
      "4176 Training accuracy = 0.94 Loss = 0.219514\n",
      "4177 Training accuracy = 0.97 Loss = 0.176369\n",
      "4178 Training accuracy = 0.93 Loss = 0.246428\n",
      "4179 Training accuracy = 0.9 Loss = 0.269446\n",
      "4180 Training accuracy = 0.94 Loss = 0.213316\n",
      "4181 Training accuracy = 0.92 Loss = 0.259088\n",
      "4182 Training accuracy = 0.94 Loss = 0.195386\n",
      "4183 Training accuracy = 0.88 Loss = 0.259046\n",
      "4184 Training accuracy = 0.92 Loss = 0.223053\n",
      "4185 Training accuracy = 0.92 Loss = 0.258956\n",
      "4186 Training accuracy = 0.93 Loss = 0.190981\n",
      "4187 Training accuracy = 0.95 Loss = 0.17977\n",
      "4188 Training accuracy = 0.9 Loss = 0.545675\n",
      "4189 Training accuracy = 0.93 Loss = 0.332377\n",
      "4190 Training accuracy = 0.95 Loss = 0.193806\n",
      "4191 Training accuracy = 0.92 Loss = 0.316025\n",
      "4192 Training accuracy = 0.9 Loss = 0.304748\n",
      "4193 Training accuracy = 0.94 Loss = 0.336678\n",
      "4194 Training accuracy = 0.91 Loss = 0.241464\n",
      "4195 Training accuracy = 0.97 Loss = 0.16264\n",
      "4196 Training accuracy = 0.89 Loss = 0.302916\n",
      "4197 Training accuracy = 0.94 Loss = 0.244894\n",
      "4198 Training accuracy = 0.93 Loss = 0.223327\n",
      "4199 Training accuracy = 0.95 Loss = 0.182853\n",
      "4200 Training accuracy = 0.88 Loss = 0.39007\n",
      "4201 Training accuracy = 0.94 Loss = 0.175519\n",
      "4202 Training accuracy = 0.97 Loss = 0.13489\n",
      "4203 Training accuracy = 0.92 Loss = 0.396852\n",
      "4204 Training accuracy = 0.94 Loss = 0.208141\n",
      "4205 Training accuracy = 0.88 Loss = 0.315655\n",
      "4206 Training accuracy = 0.93 Loss = 0.166348\n",
      "4207 Training accuracy = 0.91 Loss = 0.319635\n",
      "4208 Training accuracy = 0.9 Loss = 0.337541\n",
      "4209 Training accuracy = 0.94 Loss = 0.211348\n",
      "4210 Training accuracy = 0.93 Loss = 0.280749\n",
      "4211 Training accuracy = 0.91 Loss = 0.286935\n",
      "4212 Training accuracy = 0.91 Loss = 0.319857\n",
      "4213 Training accuracy = 0.94 Loss = 0.205887\n",
      "4214 Training accuracy = 0.89 Loss = 0.334952\n",
      "4215 Training accuracy = 0.91 Loss = 0.238879\n",
      "4216 Training accuracy = 1.0 Loss = 0.0884379\n",
      "4217 Training accuracy = 0.88 Loss = 0.291227\n",
      "4218 Training accuracy = 0.91 Loss = 0.340403\n",
      "4219 Training accuracy = 0.93 Loss = 0.233427\n",
      "4220 Training accuracy = 0.88 Loss = 0.333062\n",
      "4221 Training accuracy = 0.94 Loss = 0.226464\n",
      "4222 Training accuracy = 0.89 Loss = 0.394473\n",
      "4223 Training accuracy = 0.93 Loss = 0.24435\n",
      "4224 Training accuracy = 0.95 Loss = 0.18364\n",
      "4225 Training accuracy = 0.94 Loss = 0.246266\n",
      "4226 Training accuracy = 0.94 Loss = 0.254723\n",
      "4227 Training accuracy = 0.91 Loss = 0.314906\n",
      "4228 Training accuracy = 0.95 Loss = 0.201483\n",
      "4229 Training accuracy = 0.94 Loss = 0.24161\n",
      "4230 Training accuracy = 0.93 Loss = 0.217778\n",
      "4231 Training accuracy = 0.92 Loss = 0.231363\n",
      "4232 Training accuracy = 0.91 Loss = 0.328227\n",
      "4233 Training accuracy = 0.91 Loss = 0.221187\n",
      "4234 Training accuracy = 0.92 Loss = 0.242425\n",
      "4235 Training accuracy = 0.87 Loss = 0.376463\n",
      "4236 Training accuracy = 0.94 Loss = 0.158758\n",
      "4237 Training accuracy = 0.9 Loss = 0.39672\n",
      "4238 Training accuracy = 0.92 Loss = 0.318028\n",
      "4239 Training accuracy = 0.91 Loss = 0.247767\n",
      "4240 Training accuracy = 0.92 Loss = 0.213768\n",
      "4241 Training accuracy = 0.94 Loss = 0.30788\n",
      "4242 Training accuracy = 0.89 Loss = 0.318458\n",
      "4243 Training accuracy = 0.91 Loss = 0.317962\n",
      "4244 Training accuracy = 0.88 Loss = 0.411816\n",
      "4245 Training accuracy = 0.96 Loss = 0.229414\n",
      "4246 Training accuracy = 0.96 Loss = 0.17583\n",
      "4247 Training accuracy = 0.92 Loss = 0.175357\n",
      "4248 Training accuracy = 0.97 Loss = 0.172352\n",
      "4249 Training accuracy = 0.94 Loss = 0.312051\n",
      "4250 Training accuracy = 0.9 Loss = 0.287325\n",
      "4251 Training accuracy = 0.91 Loss = 0.310655\n",
      "4252 Training accuracy = 0.92 Loss = 0.266338\n",
      "4253 Training accuracy = 0.89 Loss = 0.495141\n",
      "4254 Training accuracy = 0.92 Loss = 0.313389\n",
      "4255 Training accuracy = 0.9 Loss = 0.268454\n",
      "4256 Training accuracy = 0.95 Loss = 0.165268\n",
      "4257 Training accuracy = 0.9 Loss = 0.281456\n",
      "4258 Training accuracy = 0.94 Loss = 0.31545\n",
      "4259 Training accuracy = 0.94 Loss = 0.193488\n",
      "4260 Training accuracy = 0.96 Loss = 0.148791\n",
      "4261 Training accuracy = 0.93 Loss = 0.234255\n",
      "4262 Training accuracy = 0.96 Loss = 0.162787\n",
      "4263 Training accuracy = 0.91 Loss = 0.37364\n",
      "4264 Training accuracy = 0.94 Loss = 0.196075\n",
      "4265 Training accuracy = 0.91 Loss = 0.225587\n",
      "4266 Training accuracy = 0.9 Loss = 0.490515\n",
      "4267 Training accuracy = 0.95 Loss = 0.196562\n",
      "4268 Training accuracy = 0.96 Loss = 0.119218\n",
      "4269 Training accuracy = 0.91 Loss = 0.280192\n",
      "4270 Training accuracy = 0.93 Loss = 0.252679\n",
      "4271 Training accuracy = 0.92 Loss = 0.291805\n",
      "4272 Training accuracy = 0.93 Loss = 0.272015\n",
      "4273 Training accuracy = 0.91 Loss = 0.195726\n",
      "4274 Training accuracy = 0.92 Loss = 0.367386\n",
      "4275 Training accuracy = 0.95 Loss = 0.179491\n",
      "4276 Training accuracy = 0.92 Loss = 0.468306\n",
      "4277 Training accuracy = 0.92 Loss = 0.328\n",
      "4278 Training accuracy = 0.95 Loss = 0.190274\n",
      "4279 Training accuracy = 0.93 Loss = 0.238058\n",
      "4280 Training accuracy = 0.93 Loss = 0.215708\n",
      "4281 Training accuracy = 0.83 Loss = 0.474458\n",
      "4282 Training accuracy = 0.91 Loss = 0.195445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4283 Training accuracy = 0.92 Loss = 0.269555\n",
      "4284 Training accuracy = 0.9 Loss = 0.345817\n",
      "4285 Training accuracy = 0.96 Loss = 0.205407\n",
      "4286 Training accuracy = 0.95 Loss = 0.159328\n",
      "4287 Training accuracy = 0.96 Loss = 0.303155\n",
      "4288 Training accuracy = 0.93 Loss = 0.259341\n",
      "4289 Training accuracy = 0.91 Loss = 0.405203\n",
      "4290 Training accuracy = 0.91 Loss = 0.28261\n",
      "4291 Training accuracy = 0.92 Loss = 0.343178\n",
      "4292 Training accuracy = 0.98 Loss = 0.112996\n",
      "4293 Training accuracy = 0.93 Loss = 0.195519\n",
      "4294 Training accuracy = 0.9 Loss = 0.348056\n",
      "4295 Training accuracy = 0.87 Loss = 0.363223\n",
      "4296 Training accuracy = 0.94 Loss = 0.276953\n",
      "4297 Training accuracy = 0.93 Loss = 0.258986\n",
      "4298 Training accuracy = 0.95 Loss = 0.176497\n",
      "4299 Training accuracy = 0.93 Loss = 0.303444\n",
      "4300 Training accuracy = 0.93 Loss = 0.183066\n",
      "4301 Training accuracy = 0.93 Loss = 0.242424\n",
      "4302 Training accuracy = 0.93 Loss = 0.174395\n",
      "4303 Training accuracy = 0.87 Loss = 0.422969\n",
      "4304 Training accuracy = 0.9 Loss = 0.343459\n",
      "4305 Training accuracy = 0.92 Loss = 0.285246\n",
      "4306 Training accuracy = 0.96 Loss = 0.199106\n",
      "4307 Training accuracy = 0.94 Loss = 0.195323\n",
      "4308 Training accuracy = 0.94 Loss = 0.175191\n",
      "4309 Training accuracy = 0.93 Loss = 0.267074\n",
      "4310 Training accuracy = 0.89 Loss = 0.409009\n",
      "4311 Training accuracy = 0.94 Loss = 0.200526\n",
      "4312 Training accuracy = 0.92 Loss = 0.34479\n",
      "4313 Training accuracy = 0.93 Loss = 0.253996\n",
      "4314 Training accuracy = 0.97 Loss = 0.12528\n",
      "4315 Training accuracy = 0.89 Loss = 0.321334\n",
      "4316 Training accuracy = 0.92 Loss = 0.299781\n",
      "4317 Training accuracy = 0.95 Loss = 0.216852\n",
      "4318 Training accuracy = 0.93 Loss = 0.271195\n",
      "4319 Training accuracy = 0.88 Loss = 0.45827\n",
      "4320 Training accuracy = 0.93 Loss = 0.209018\n",
      "4321 Training accuracy = 0.93 Loss = 0.278379\n",
      "4322 Training accuracy = 0.91 Loss = 0.343361\n",
      "4323 Training accuracy = 0.93 Loss = 0.188566\n",
      "4324 Training accuracy = 0.89 Loss = 0.295274\n",
      "4325 Training accuracy = 0.9 Loss = 0.242177\n",
      "4326 Training accuracy = 0.94 Loss = 0.243843\n",
      "4327 Training accuracy = 0.93 Loss = 0.300415\n",
      "4328 Training accuracy = 0.91 Loss = 0.254119\n",
      "4329 Training accuracy = 0.97 Loss = 0.153041\n",
      "4330 Training accuracy = 0.96 Loss = 0.23088\n",
      "4331 Training accuracy = 0.91 Loss = 0.30856\n",
      "4332 Training accuracy = 0.95 Loss = 0.25047\n",
      "4333 Training accuracy = 0.96 Loss = 0.221996\n",
      "4334 Training accuracy = 0.88 Loss = 0.419758\n",
      "4335 Training accuracy = 0.9 Loss = 0.321147\n",
      "4336 Training accuracy = 0.93 Loss = 0.17309\n",
      "4337 Training accuracy = 0.9 Loss = 0.243241\n",
      "4338 Training accuracy = 0.95 Loss = 0.246563\n",
      "4339 Training accuracy = 0.92 Loss = 0.384212\n",
      "4340 Training accuracy = 0.91 Loss = 0.284459\n",
      "4341 Training accuracy = 0.87 Loss = 0.325211\n",
      "4342 Training accuracy = 0.93 Loss = 0.364075\n",
      "4343 Training accuracy = 0.94 Loss = 0.177204\n",
      "4344 Training accuracy = 0.88 Loss = 0.349468\n",
      "4345 Training accuracy = 0.91 Loss = 0.411382\n",
      "4346 Training accuracy = 0.93 Loss = 0.235492\n",
      "4347 Training accuracy = 0.95 Loss = 0.239621\n",
      "4348 Training accuracy = 0.9 Loss = 0.387654\n",
      "4349 Training accuracy = 0.91 Loss = 0.269925\n",
      "4350 Training accuracy = 0.91 Loss = 0.265833\n",
      "4351 Training accuracy = 0.93 Loss = 0.228915\n",
      "4352 Training accuracy = 0.94 Loss = 0.25177\n",
      "4353 Training accuracy = 0.9 Loss = 0.334198\n",
      "4354 Training accuracy = 0.95 Loss = 0.161191\n",
      "4355 Training accuracy = 0.94 Loss = 0.213021\n",
      "4356 Training accuracy = 0.96 Loss = 0.208412\n",
      "4357 Training accuracy = 0.91 Loss = 0.290699\n",
      "4358 Training accuracy = 0.94 Loss = 0.181326\n",
      "4359 Training accuracy = 0.94 Loss = 0.184763\n",
      "4360 Training accuracy = 0.9 Loss = 0.362042\n",
      "4361 Training accuracy = 0.93 Loss = 0.236742\n",
      "4362 Training accuracy = 0.88 Loss = 0.397112\n",
      "4363 Training accuracy = 0.92 Loss = 0.301525\n",
      "4364 Training accuracy = 0.91 Loss = 0.186384\n",
      "4365 Training accuracy = 0.94 Loss = 0.293839\n",
      "4366 Training accuracy = 0.96 Loss = 0.185263\n",
      "4367 Training accuracy = 0.91 Loss = 0.230538\n",
      "4368 Training accuracy = 0.95 Loss = 0.212124\n",
      "4369 Training accuracy = 0.89 Loss = 0.280274\n",
      "4370 Training accuracy = 0.97 Loss = 0.176651\n",
      "4371 Training accuracy = 0.87 Loss = 0.386969\n",
      "4372 Training accuracy = 0.92 Loss = 0.236145\n",
      "4373 Training accuracy = 0.94 Loss = 0.235235\n",
      "4374 Training accuracy = 0.91 Loss = 0.40786\n",
      "4375 Training accuracy = 0.94 Loss = 0.18341\n",
      "4376 Training accuracy = 0.95 Loss = 0.273881\n",
      "4377 Training accuracy = 0.97 Loss = 0.160179\n",
      "4378 Training accuracy = 0.86 Loss = 0.545744\n",
      "4379 Training accuracy = 0.97 Loss = 0.153574\n",
      "4380 Training accuracy = 0.93 Loss = 0.255911\n",
      "4381 Training accuracy = 0.9 Loss = 0.354915\n",
      "4382 Training accuracy = 0.9 Loss = 0.363782\n",
      "4383 Training accuracy = 0.88 Loss = 0.456771\n",
      "4384 Training accuracy = 0.95 Loss = 0.264375\n",
      "4385 Training accuracy = 0.89 Loss = 0.33307\n",
      "4386 Training accuracy = 0.93 Loss = 0.252582\n",
      "4387 Training accuracy = 0.98 Loss = 0.121562\n",
      "4388 Training accuracy = 0.93 Loss = 0.298953\n",
      "4389 Training accuracy = 0.91 Loss = 0.286006\n",
      "4390 Training accuracy = 0.91 Loss = 0.318554\n",
      "4391 Training accuracy = 0.93 Loss = 0.277691\n",
      "4392 Training accuracy = 0.91 Loss = 0.302042\n",
      "4393 Training accuracy = 0.93 Loss = 0.234438\n",
      "4394 Training accuracy = 0.94 Loss = 0.232509\n",
      "4395 Training accuracy = 0.9 Loss = 0.330777\n",
      "4396 Training accuracy = 0.95 Loss = 0.213236\n",
      "4397 Training accuracy = 0.9 Loss = 0.380335\n",
      "4398 Training accuracy = 0.9 Loss = 0.250267\n",
      "4399 Training accuracy = 0.91 Loss = 0.334466\n",
      "4400 Training accuracy = 0.93 Loss = 0.271314\n",
      "4401 Training accuracy = 0.92 Loss = 0.343008\n",
      "4402 Training accuracy = 0.93 Loss = 0.454499\n",
      "4403 Training accuracy = 0.92 Loss = 0.310478\n",
      "4404 Training accuracy = 0.91 Loss = 0.406318\n",
      "4405 Training accuracy = 0.98 Loss = 0.120241\n",
      "4406 Training accuracy = 0.95 Loss = 0.167824\n",
      "4407 Training accuracy = 0.95 Loss = 0.221648\n",
      "4408 Training accuracy = 0.9 Loss = 0.32622\n",
      "4409 Training accuracy = 0.91 Loss = 0.2881\n",
      "4410 Training accuracy = 0.94 Loss = 0.292254\n",
      "4411 Training accuracy = 0.94 Loss = 0.194485\n",
      "4412 Training accuracy = 0.9 Loss = 0.527421\n",
      "4413 Training accuracy = 0.92 Loss = 0.240179\n",
      "4414 Training accuracy = 0.93 Loss = 0.301347\n",
      "4415 Training accuracy = 0.95 Loss = 0.186932\n",
      "4416 Training accuracy = 0.97 Loss = 0.151823\n",
      "4417 Training accuracy = 0.89 Loss = 0.263953\n",
      "4418 Training accuracy = 0.92 Loss = 0.366362\n",
      "4419 Training accuracy = 0.91 Loss = 0.298857\n",
      "4420 Training accuracy = 0.9 Loss = 0.283538\n",
      "4421 Training accuracy = 0.92 Loss = 0.234089\n",
      "4422 Training accuracy = 0.94 Loss = 0.21851\n",
      "4423 Training accuracy = 0.97 Loss = 0.119842\n",
      "4424 Training accuracy = 0.93 Loss = 0.245327\n",
      "4425 Training accuracy = 0.95 Loss = 0.149435\n",
      "4426 Training accuracy = 0.88 Loss = 0.429574\n",
      "4427 Training accuracy = 0.9 Loss = 0.370665\n",
      "4428 Training accuracy = 0.9 Loss = 0.433883\n",
      "4429 Training accuracy = 0.95 Loss = 0.214467\n",
      "4430 Training accuracy = 0.93 Loss = 0.327329\n",
      "4431 Training accuracy = 0.9 Loss = 0.432155\n",
      "4432 Training accuracy = 0.89 Loss = 0.310751\n",
      "4433 Training accuracy = 0.94 Loss = 0.248448\n",
      "4434 Training accuracy = 0.99 Loss = 0.0743508\n",
      "4435 Training accuracy = 0.92 Loss = 0.369083\n",
      "4436 Training accuracy = 0.96 Loss = 0.17383\n",
      "4437 Training accuracy = 0.93 Loss = 0.209149\n",
      "4438 Training accuracy = 0.91 Loss = 0.285021\n",
      "4439 Training accuracy = 0.9 Loss = 0.30748\n",
      "4440 Training accuracy = 0.9 Loss = 0.286739\n",
      "4441 Training accuracy = 0.84 Loss = 0.397818\n",
      "4442 Training accuracy = 0.95 Loss = 0.167065\n",
      "4443 Training accuracy = 0.91 Loss = 0.510268\n",
      "4444 Training accuracy = 0.97 Loss = 0.152326\n",
      "4445 Training accuracy = 0.91 Loss = 0.333541\n",
      "4446 Training accuracy = 0.95 Loss = 0.218905\n",
      "4447 Training accuracy = 0.9 Loss = 0.276136\n",
      "4448 Training accuracy = 0.91 Loss = 0.283854\n",
      "4449 Training accuracy = 0.94 Loss = 0.197383\n",
      "4450 Training accuracy = 0.89 Loss = 0.438505\n",
      "4451 Training accuracy = 0.89 Loss = 0.338913\n",
      "4452 Training accuracy = 0.95 Loss = 0.159771\n",
      "4453 Training accuracy = 0.94 Loss = 0.193938\n",
      "4454 Training accuracy = 0.9 Loss = 0.356801\n",
      "4455 Training accuracy = 0.95 Loss = 0.213356\n",
      "4456 Training accuracy = 0.95 Loss = 0.165471\n",
      "4457 Training accuracy = 0.96 Loss = 0.147658\n",
      "4458 Training accuracy = 0.9 Loss = 0.302975\n",
      "4459 Training accuracy = 0.95 Loss = 0.213161\n",
      "4460 Training accuracy = 0.95 Loss = 0.150811\n",
      "4461 Training accuracy = 0.93 Loss = 0.179783\n",
      "4462 Training accuracy = 0.93 Loss = 0.277114\n",
      "4463 Training accuracy = 0.92 Loss = 0.420175\n",
      "4464 Training accuracy = 0.92 Loss = 0.278531\n",
      "4465 Training accuracy = 0.92 Loss = 0.239842\n",
      "4466 Training accuracy = 0.87 Loss = 0.46557\n",
      "4467 Training accuracy = 0.93 Loss = 0.33208\n",
      "4468 Training accuracy = 0.89 Loss = 0.361721\n",
      "4469 Training accuracy = 0.96 Loss = 0.193108\n",
      "4470 Training accuracy = 0.93 Loss = 0.217137\n",
      "4471 Training accuracy = 0.94 Loss = 0.215088\n",
      "4472 Training accuracy = 0.93 Loss = 0.280197\n",
      "4473 Training accuracy = 0.9 Loss = 0.287373\n",
      "4474 Training accuracy = 0.92 Loss = 0.343504\n",
      "4475 Training accuracy = 0.94 Loss = 0.205922\n",
      "4476 Training accuracy = 0.93 Loss = 0.304046\n",
      "4477 Training accuracy = 0.95 Loss = 0.234201\n",
      "4478 Training accuracy = 0.93 Loss = 0.256413\n",
      "4479 Training accuracy = 0.9 Loss = 0.322456\n",
      "4480 Training accuracy = 0.9 Loss = 0.278954\n",
      "4481 Training accuracy = 0.92 Loss = 0.26717\n",
      "4482 Training accuracy = 0.93 Loss = 0.334362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4483 Training accuracy = 0.93 Loss = 0.227216\n",
      "4484 Training accuracy = 0.98 Loss = 0.112968\n",
      "4485 Training accuracy = 0.92 Loss = 0.242283\n",
      "4486 Training accuracy = 0.95 Loss = 0.272617\n",
      "4487 Training accuracy = 0.93 Loss = 0.26395\n",
      "4488 Training accuracy = 0.91 Loss = 0.190839\n",
      "4489 Training accuracy = 0.93 Loss = 0.238046\n",
      "4490 Training accuracy = 0.89 Loss = 0.352458\n",
      "4491 Training accuracy = 0.94 Loss = 0.285773\n",
      "4492 Training accuracy = 0.95 Loss = 0.303955\n",
      "4493 Training accuracy = 0.86 Loss = 0.336773\n",
      "4494 Training accuracy = 0.94 Loss = 0.239577\n",
      "4495 Training accuracy = 0.98 Loss = 0.163242\n",
      "4496 Training accuracy = 0.91 Loss = 0.358405\n",
      "4497 Training accuracy = 0.94 Loss = 0.205658\n",
      "4498 Training accuracy = 0.93 Loss = 0.288644\n",
      "4499 Training accuracy = 0.95 Loss = 0.188014\n",
      "4500 Training accuracy = 0.92 Loss = 0.273263\n",
      "4501 Training accuracy = 0.89 Loss = 0.336041\n",
      "4502 Training accuracy = 0.95 Loss = 0.402468\n",
      "4503 Training accuracy = 0.91 Loss = 0.281929\n",
      "4504 Training accuracy = 0.91 Loss = 0.352592\n",
      "4505 Training accuracy = 0.91 Loss = 0.360296\n",
      "4506 Training accuracy = 0.93 Loss = 0.260349\n",
      "4507 Training accuracy = 0.92 Loss = 0.229279\n",
      "4508 Training accuracy = 0.91 Loss = 0.285421\n",
      "4509 Training accuracy = 0.93 Loss = 0.260207\n",
      "4510 Training accuracy = 0.94 Loss = 0.252763\n",
      "4511 Training accuracy = 0.93 Loss = 0.251729\n",
      "4512 Training accuracy = 0.91 Loss = 0.286898\n",
      "4513 Training accuracy = 0.98 Loss = 0.150609\n",
      "4514 Training accuracy = 0.94 Loss = 0.212778\n",
      "4515 Training accuracy = 0.93 Loss = 0.251912\n",
      "4516 Training accuracy = 0.91 Loss = 0.303812\n",
      "4517 Training accuracy = 0.95 Loss = 0.168274\n",
      "4518 Training accuracy = 0.93 Loss = 0.238656\n",
      "4519 Training accuracy = 0.92 Loss = 0.343319\n",
      "4520 Training accuracy = 0.91 Loss = 0.212769\n",
      "4521 Training accuracy = 0.93 Loss = 0.298207\n",
      "4522 Training accuracy = 0.89 Loss = 0.255436\n",
      "4523 Training accuracy = 0.91 Loss = 0.266155\n",
      "4524 Training accuracy = 0.94 Loss = 0.21591\n",
      "4525 Training accuracy = 0.94 Loss = 0.275995\n",
      "4526 Training accuracy = 0.9 Loss = 0.344764\n",
      "4527 Training accuracy = 0.92 Loss = 0.20402\n",
      "4528 Training accuracy = 0.96 Loss = 0.148692\n",
      "4529 Training accuracy = 0.93 Loss = 0.261193\n",
      "4530 Training accuracy = 0.91 Loss = 0.325831\n",
      "4531 Training accuracy = 0.94 Loss = 0.272929\n",
      "4532 Training accuracy = 0.93 Loss = 0.182919\n",
      "4533 Training accuracy = 0.91 Loss = 0.340056\n",
      "4534 Training accuracy = 0.96 Loss = 0.152544\n",
      "4535 Training accuracy = 0.92 Loss = 0.289579\n",
      "4536 Training accuracy = 0.95 Loss = 0.295127\n",
      "4537 Training accuracy = 0.9 Loss = 0.478874\n",
      "4538 Training accuracy = 0.92 Loss = 0.19011\n",
      "4539 Training accuracy = 0.93 Loss = 0.264384\n",
      "4540 Training accuracy = 0.94 Loss = 0.179515\n",
      "4541 Training accuracy = 0.93 Loss = 0.291209\n",
      "4542 Training accuracy = 0.94 Loss = 0.335846\n",
      "4543 Training accuracy = 0.84 Loss = 0.360893\n",
      "4544 Training accuracy = 0.93 Loss = 0.281787\n",
      "4545 Training accuracy = 0.95 Loss = 0.342757\n",
      "4546 Training accuracy = 0.9 Loss = 0.303831\n",
      "4547 Training accuracy = 0.93 Loss = 0.205745\n",
      "4548 Training accuracy = 0.95 Loss = 0.258087\n",
      "4549 Training accuracy = 0.96 Loss = 0.210065\n",
      "4550 Training accuracy = 0.93 Loss = 0.268659\n",
      "4551 Training accuracy = 0.95 Loss = 0.192777\n",
      "4552 Training accuracy = 0.86 Loss = 0.396149\n",
      "4553 Training accuracy = 0.97 Loss = 0.184863\n",
      "4554 Training accuracy = 0.91 Loss = 0.29431\n",
      "4555 Training accuracy = 0.92 Loss = 0.26281\n",
      "4556 Training accuracy = 0.92 Loss = 0.303221\n",
      "4557 Training accuracy = 0.93 Loss = 0.252375\n",
      "4558 Training accuracy = 0.94 Loss = 0.204181\n",
      "4559 Training accuracy = 0.93 Loss = 0.252384\n",
      "4560 Training accuracy = 0.9 Loss = 0.246022\n",
      "4561 Training accuracy = 0.94 Loss = 0.301915\n",
      "4562 Training accuracy = 0.96 Loss = 0.121661\n",
      "4563 Training accuracy = 0.9 Loss = 0.304128\n",
      "4564 Training accuracy = 0.91 Loss = 0.372589\n",
      "4565 Training accuracy = 0.91 Loss = 0.366413\n",
      "4566 Training accuracy = 0.95 Loss = 0.172292\n",
      "4567 Training accuracy = 0.94 Loss = 0.254088\n",
      "4568 Training accuracy = 0.91 Loss = 0.349039\n",
      "4569 Training accuracy = 0.9 Loss = 0.466395\n",
      "4570 Training accuracy = 0.93 Loss = 0.23239\n",
      "4571 Training accuracy = 0.96 Loss = 0.233091\n",
      "4572 Training accuracy = 0.96 Loss = 0.195216\n",
      "4573 Training accuracy = 0.92 Loss = 0.282732\n",
      "4574 Training accuracy = 0.91 Loss = 0.226451\n",
      "4575 Training accuracy = 0.93 Loss = 0.258984\n",
      "4576 Training accuracy = 0.95 Loss = 0.312434\n",
      "4577 Training accuracy = 0.94 Loss = 0.194894\n",
      "4578 Training accuracy = 0.94 Loss = 0.247694\n",
      "4579 Training accuracy = 0.94 Loss = 0.203544\n",
      "4580 Training accuracy = 0.89 Loss = 0.267007\n",
      "4581 Training accuracy = 0.92 Loss = 0.329071\n",
      "4582 Training accuracy = 0.93 Loss = 0.184496\n",
      "4583 Training accuracy = 0.87 Loss = 0.317899\n",
      "4584 Training accuracy = 0.95 Loss = 0.241663\n",
      "4585 Training accuracy = 0.9 Loss = 0.340756\n",
      "4586 Training accuracy = 0.94 Loss = 0.358103\n",
      "4587 Training accuracy = 0.93 Loss = 0.160853\n",
      "4588 Training accuracy = 0.96 Loss = 0.155712\n",
      "4589 Training accuracy = 0.91 Loss = 0.336623\n",
      "4590 Training accuracy = 0.9 Loss = 0.326995\n",
      "4591 Training accuracy = 0.96 Loss = 0.172897\n",
      "4592 Training accuracy = 0.93 Loss = 0.175413\n",
      "4593 Training accuracy = 0.92 Loss = 0.261146\n",
      "4594 Training accuracy = 0.95 Loss = 0.127591\n",
      "4595 Training accuracy = 0.92 Loss = 0.256282\n",
      "4596 Training accuracy = 0.87 Loss = 0.47328\n",
      "4597 Training accuracy = 0.93 Loss = 0.264059\n",
      "4598 Training accuracy = 0.93 Loss = 0.167952\n",
      "4599 Training accuracy = 0.91 Loss = 0.418584\n",
      "4600 Training accuracy = 0.9 Loss = 0.282062\n",
      "4601 Training accuracy = 0.88 Loss = 0.32763\n",
      "4602 Training accuracy = 0.88 Loss = 0.359062\n",
      "4603 Training accuracy = 0.94 Loss = 0.237826\n",
      "4604 Training accuracy = 0.93 Loss = 0.15987\n",
      "4605 Training accuracy = 0.93 Loss = 0.186021\n",
      "4606 Training accuracy = 0.91 Loss = 0.264718\n",
      "4607 Training accuracy = 0.97 Loss = 0.157576\n",
      "4608 Training accuracy = 0.94 Loss = 0.36695\n",
      "4609 Training accuracy = 0.95 Loss = 0.264184\n",
      "4610 Training accuracy = 0.94 Loss = 0.261577\n",
      "4611 Training accuracy = 0.9 Loss = 0.296044\n",
      "4612 Training accuracy = 0.9 Loss = 0.270868\n",
      "4613 Training accuracy = 0.97 Loss = 0.171536\n",
      "4614 Training accuracy = 0.93 Loss = 0.171229\n",
      "4615 Training accuracy = 0.93 Loss = 0.219734\n",
      "4616 Training accuracy = 0.96 Loss = 0.152476\n",
      "4617 Training accuracy = 0.94 Loss = 0.208317\n",
      "4618 Training accuracy = 0.94 Loss = 0.245261\n",
      "4619 Training accuracy = 0.95 Loss = 0.239861\n",
      "4620 Training accuracy = 0.94 Loss = 0.349301\n",
      "4621 Training accuracy = 0.89 Loss = 0.453456\n",
      "4622 Training accuracy = 0.92 Loss = 0.272116\n",
      "4623 Training accuracy = 0.96 Loss = 0.148365\n",
      "4624 Training accuracy = 0.94 Loss = 0.241172\n",
      "4625 Training accuracy = 0.94 Loss = 0.240918\n",
      "4626 Training accuracy = 0.94 Loss = 0.2303\n",
      "4627 Training accuracy = 0.95 Loss = 0.267205\n",
      "4628 Training accuracy = 0.92 Loss = 0.18635\n",
      "4629 Training accuracy = 0.94 Loss = 0.181577\n",
      "4630 Training accuracy = 0.97 Loss = 0.202199\n",
      "4631 Training accuracy = 0.93 Loss = 0.291879\n",
      "4632 Training accuracy = 0.92 Loss = 0.352773\n",
      "4633 Training accuracy = 0.92 Loss = 0.361502\n",
      "4634 Training accuracy = 0.96 Loss = 0.186517\n",
      "4635 Training accuracy = 0.92 Loss = 0.245653\n",
      "4636 Training accuracy = 0.92 Loss = 0.277893\n",
      "4637 Training accuracy = 0.93 Loss = 0.187224\n",
      "4638 Training accuracy = 0.93 Loss = 0.303311\n",
      "4639 Training accuracy = 0.93 Loss = 0.213804\n",
      "4640 Training accuracy = 0.98 Loss = 0.144993\n",
      "4641 Training accuracy = 0.95 Loss = 0.209217\n",
      "4642 Training accuracy = 0.94 Loss = 0.219038\n",
      "4643 Training accuracy = 0.87 Loss = 0.373197\n",
      "4644 Training accuracy = 0.92 Loss = 0.326554\n",
      "4645 Training accuracy = 0.88 Loss = 0.411943\n",
      "4646 Training accuracy = 0.94 Loss = 0.236602\n",
      "4647 Training accuracy = 0.93 Loss = 0.243949\n",
      "4648 Training accuracy = 0.93 Loss = 0.234077\n",
      "4649 Training accuracy = 0.96 Loss = 0.178344\n",
      "4650 Training accuracy = 0.92 Loss = 0.208708\n",
      "4651 Training accuracy = 0.91 Loss = 0.255422\n",
      "4652 Training accuracy = 0.88 Loss = 0.409855\n",
      "4653 Training accuracy = 0.91 Loss = 0.275638\n",
      "4654 Training accuracy = 0.89 Loss = 0.337999\n",
      "4655 Training accuracy = 0.97 Loss = 0.157084\n",
      "4656 Training accuracy = 0.95 Loss = 0.24793\n",
      "4657 Training accuracy = 0.9 Loss = 0.412559\n",
      "4658 Training accuracy = 0.94 Loss = 0.241459\n",
      "4659 Training accuracy = 0.92 Loss = 0.293187\n",
      "4660 Training accuracy = 0.93 Loss = 0.165153\n",
      "4661 Training accuracy = 0.96 Loss = 0.219079\n",
      "4662 Training accuracy = 0.9 Loss = 0.292346\n",
      "4663 Training accuracy = 0.92 Loss = 0.219959\n",
      "4664 Training accuracy = 0.93 Loss = 0.177198\n",
      "4665 Training accuracy = 0.93 Loss = 0.268111\n",
      "4666 Training accuracy = 0.91 Loss = 0.310894\n",
      "4667 Training accuracy = 0.95 Loss = 0.177997\n",
      "4668 Training accuracy = 0.88 Loss = 0.305667\n",
      "4669 Training accuracy = 0.96 Loss = 0.152242\n",
      "4670 Training accuracy = 0.95 Loss = 0.184209\n",
      "4671 Training accuracy = 0.93 Loss = 0.325839\n",
      "4672 Training accuracy = 0.93 Loss = 0.220833\n",
      "4673 Training accuracy = 0.92 Loss = 0.280293\n",
      "4674 Training accuracy = 0.93 Loss = 0.186598\n",
      "4675 Training accuracy = 0.98 Loss = 0.14326\n",
      "4676 Training accuracy = 0.94 Loss = 0.300208\n",
      "4677 Training accuracy = 0.93 Loss = 0.371718\n",
      "4678 Training accuracy = 0.89 Loss = 0.395369\n",
      "4679 Training accuracy = 0.91 Loss = 0.396526\n",
      "4680 Training accuracy = 0.95 Loss = 0.156963\n",
      "4681 Training accuracy = 0.92 Loss = 0.241936\n",
      "4682 Training accuracy = 0.93 Loss = 0.193615\n",
      "4683 Training accuracy = 0.91 Loss = 0.279246\n",
      "4684 Training accuracy = 0.93 Loss = 0.260547\n",
      "4685 Training accuracy = 0.94 Loss = 0.310864\n",
      "4686 Training accuracy = 0.93 Loss = 0.219603\n",
      "4687 Training accuracy = 0.92 Loss = 0.390957\n",
      "4688 Training accuracy = 0.96 Loss = 0.194799\n",
      "4689 Training accuracy = 0.95 Loss = 0.219143\n",
      "4690 Training accuracy = 0.89 Loss = 0.330812\n",
      "4691 Training accuracy = 0.91 Loss = 0.247315\n",
      "4692 Training accuracy = 0.95 Loss = 0.175489\n",
      "4693 Training accuracy = 0.97 Loss = 0.12387\n",
      "4694 Training accuracy = 0.94 Loss = 0.248248\n",
      "4695 Training accuracy = 0.93 Loss = 0.234933\n",
      "4696 Training accuracy = 0.96 Loss = 0.246214\n",
      "4697 Training accuracy = 0.95 Loss = 0.374649\n",
      "4698 Training accuracy = 0.94 Loss = 0.225856\n",
      "4699 Training accuracy = 0.89 Loss = 0.424779\n",
      "4700 Training accuracy = 0.95 Loss = 0.253896\n",
      "4701 Training accuracy = 0.87 Loss = 0.574649\n",
      "4702 Training accuracy = 0.92 Loss = 0.282121\n",
      "4703 Training accuracy = 0.93 Loss = 0.294842\n",
      "4704 Training accuracy = 0.95 Loss = 0.196776\n",
      "4705 Training accuracy = 0.89 Loss = 0.294451\n",
      "4706 Training accuracy = 0.95 Loss = 0.173992\n",
      "4707 Training accuracy = 0.97 Loss = 0.121468\n",
      "4708 Training accuracy = 0.93 Loss = 0.254437\n",
      "4709 Training accuracy = 0.93 Loss = 0.263589\n",
      "4710 Training accuracy = 0.88 Loss = 0.404515\n",
      "4711 Training accuracy = 0.9 Loss = 0.24157\n",
      "4712 Training accuracy = 0.94 Loss = 0.256655\n",
      "4713 Training accuracy = 0.92 Loss = 0.31276\n",
      "4714 Training accuracy = 0.9 Loss = 0.27331\n",
      "4715 Training accuracy = 0.94 Loss = 0.230979\n",
      "4716 Training accuracy = 0.95 Loss = 0.170689\n",
      "4717 Training accuracy = 0.9 Loss = 0.267111\n",
      "4718 Training accuracy = 0.98 Loss = 0.147453\n",
      "4719 Training accuracy = 0.89 Loss = 0.280798\n",
      "4720 Training accuracy = 0.95 Loss = 0.198153\n",
      "4721 Training accuracy = 0.93 Loss = 0.23996\n",
      "4722 Training accuracy = 0.96 Loss = 0.194334\n",
      "4723 Training accuracy = 0.94 Loss = 0.211237\n",
      "4724 Training accuracy = 0.96 Loss = 0.148886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4725 Training accuracy = 0.91 Loss = 0.328969\n",
      "4726 Training accuracy = 0.91 Loss = 0.351798\n",
      "4727 Training accuracy = 0.87 Loss = 0.374258\n",
      "4728 Training accuracy = 0.93 Loss = 0.231377\n",
      "4729 Training accuracy = 0.95 Loss = 0.167736\n",
      "4730 Training accuracy = 0.86 Loss = 0.320109\n",
      "4731 Training accuracy = 0.91 Loss = 0.249001\n",
      "4732 Training accuracy = 0.97 Loss = 0.166039\n",
      "4733 Training accuracy = 0.92 Loss = 0.228846\n",
      "4734 Training accuracy = 0.96 Loss = 0.213269\n",
      "4735 Training accuracy = 0.93 Loss = 0.181909\n",
      "4736 Training accuracy = 0.91 Loss = 0.256913\n",
      "4737 Training accuracy = 0.95 Loss = 0.235052\n",
      "4738 Training accuracy = 0.94 Loss = 0.223236\n",
      "4739 Training accuracy = 0.93 Loss = 0.191688\n",
      "4740 Training accuracy = 0.94 Loss = 0.318904\n",
      "4741 Training accuracy = 0.92 Loss = 0.334212\n",
      "4742 Training accuracy = 0.91 Loss = 0.30325\n",
      "4743 Training accuracy = 0.95 Loss = 0.204188\n",
      "4744 Training accuracy = 0.94 Loss = 0.196017\n",
      "4745 Training accuracy = 0.93 Loss = 0.231697\n",
      "4746 Training accuracy = 0.92 Loss = 0.216795\n",
      "4747 Training accuracy = 0.88 Loss = 0.588904\n",
      "4748 Training accuracy = 0.92 Loss = 0.221637\n",
      "4749 Training accuracy = 0.94 Loss = 0.194137\n",
      "4750 Training accuracy = 0.89 Loss = 0.301475\n",
      "4751 Training accuracy = 0.92 Loss = 0.305056\n",
      "4752 Training accuracy = 0.91 Loss = 0.315782\n",
      "4753 Training accuracy = 0.91 Loss = 0.316891\n",
      "4754 Training accuracy = 0.94 Loss = 0.229821\n",
      "4755 Training accuracy = 0.93 Loss = 0.289098\n",
      "4756 Training accuracy = 0.9 Loss = 0.239968\n",
      "4757 Training accuracy = 0.89 Loss = 0.262754\n",
      "4758 Training accuracy = 0.96 Loss = 0.246903\n",
      "4759 Training accuracy = 0.96 Loss = 0.160092\n",
      "4760 Training accuracy = 0.92 Loss = 0.269862\n",
      "4761 Training accuracy = 0.95 Loss = 0.181837\n",
      "4762 Training accuracy = 0.94 Loss = 0.189503\n",
      "4763 Training accuracy = 0.94 Loss = 0.185685\n",
      "4764 Training accuracy = 0.95 Loss = 0.153218\n",
      "4765 Training accuracy = 0.94 Loss = 0.275766\n",
      "4766 Training accuracy = 0.91 Loss = 0.293645\n",
      "4767 Training accuracy = 0.9 Loss = 0.333921\n",
      "4768 Training accuracy = 0.94 Loss = 0.269433\n",
      "4769 Training accuracy = 0.93 Loss = 0.304088\n",
      "4770 Training accuracy = 0.94 Loss = 0.225149\n",
      "4771 Training accuracy = 0.9 Loss = 0.312094\n",
      "4772 Training accuracy = 0.94 Loss = 0.215212\n",
      "4773 Training accuracy = 0.93 Loss = 0.340313\n",
      "4774 Training accuracy = 0.91 Loss = 0.229083\n",
      "4775 Training accuracy = 0.92 Loss = 0.245607\n",
      "4776 Training accuracy = 0.93 Loss = 0.223144\n",
      "4777 Training accuracy = 0.89 Loss = 0.339457\n",
      "4778 Training accuracy = 0.94 Loss = 0.236555\n",
      "4779 Training accuracy = 0.96 Loss = 0.228369\n",
      "4780 Training accuracy = 0.89 Loss = 0.456162\n",
      "4781 Training accuracy = 0.96 Loss = 0.196905\n",
      "4782 Training accuracy = 0.93 Loss = 0.205029\n",
      "4783 Training accuracy = 0.92 Loss = 0.247259\n",
      "4784 Training accuracy = 0.87 Loss = 0.338746\n",
      "4785 Training accuracy = 0.91 Loss = 0.32218\n",
      "4786 Training accuracy = 0.93 Loss = 0.237716\n",
      "4787 Training accuracy = 0.94 Loss = 0.197406\n",
      "4788 Training accuracy = 0.95 Loss = 0.181514\n",
      "4789 Training accuracy = 0.93 Loss = 0.350649\n",
      "4790 Training accuracy = 0.91 Loss = 0.360498\n",
      "4791 Training accuracy = 0.85 Loss = 0.461217\n",
      "4792 Training accuracy = 0.9 Loss = 0.330094\n",
      "4793 Training accuracy = 0.94 Loss = 0.284254\n",
      "4794 Training accuracy = 0.95 Loss = 0.181428\n",
      "4795 Training accuracy = 0.94 Loss = 0.282446\n",
      "4796 Training accuracy = 0.93 Loss = 0.339711\n",
      "4797 Training accuracy = 0.89 Loss = 0.307085\n",
      "4798 Training accuracy = 0.95 Loss = 0.252807\n",
      "4799 Training accuracy = 0.91 Loss = 0.303888\n",
      "4800 Training accuracy = 0.9 Loss = 0.380425\n",
      "4801 Training accuracy = 0.94 Loss = 0.344422\n",
      "4802 Training accuracy = 0.98 Loss = 0.174417\n",
      "4803 Training accuracy = 0.96 Loss = 0.193738\n",
      "4804 Training accuracy = 0.94 Loss = 0.222763\n",
      "4805 Training accuracy = 0.98 Loss = 0.179272\n",
      "4806 Training accuracy = 0.94 Loss = 0.238815\n",
      "4807 Training accuracy = 0.96 Loss = 0.25104\n",
      "4808 Training accuracy = 0.94 Loss = 0.219534\n",
      "4809 Training accuracy = 0.94 Loss = 0.334931\n",
      "4810 Training accuracy = 0.98 Loss = 0.148832\n",
      "4811 Training accuracy = 0.89 Loss = 0.392117\n",
      "4812 Training accuracy = 0.95 Loss = 0.152344\n",
      "4813 Training accuracy = 0.89 Loss = 0.288901\n",
      "4814 Training accuracy = 0.93 Loss = 0.21816\n",
      "4815 Training accuracy = 0.95 Loss = 0.21913\n",
      "4816 Training accuracy = 0.89 Loss = 0.346261\n",
      "4817 Training accuracy = 0.99 Loss = 0.0794154\n",
      "4818 Training accuracy = 0.89 Loss = 0.383005\n",
      "4819 Training accuracy = 0.9 Loss = 0.344662\n",
      "4820 Training accuracy = 0.89 Loss = 0.327194\n",
      "4821 Training accuracy = 0.92 Loss = 0.347741\n",
      "4822 Training accuracy = 0.93 Loss = 0.171469\n",
      "4823 Training accuracy = 0.94 Loss = 0.190834\n",
      "4824 Training accuracy = 0.9 Loss = 0.281317\n",
      "4825 Training accuracy = 0.95 Loss = 0.188174\n",
      "4826 Training accuracy = 0.9 Loss = 0.242584\n",
      "4827 Training accuracy = 0.95 Loss = 0.172126\n",
      "4828 Training accuracy = 0.96 Loss = 0.145039\n",
      "4829 Training accuracy = 0.94 Loss = 0.310218\n",
      "4830 Training accuracy = 0.92 Loss = 0.197214\n",
      "4831 Training accuracy = 0.9 Loss = 0.318843\n",
      "4832 Training accuracy = 0.93 Loss = 0.22019\n",
      "4833 Training accuracy = 0.9 Loss = 0.282654\n",
      "4834 Training accuracy = 0.93 Loss = 0.213173\n",
      "4835 Training accuracy = 0.91 Loss = 0.460426\n",
      "4836 Training accuracy = 0.9 Loss = 0.267235\n",
      "4837 Training accuracy = 0.97 Loss = 0.147695\n",
      "4838 Training accuracy = 0.92 Loss = 0.324555\n",
      "4839 Training accuracy = 0.88 Loss = 0.510585\n",
      "4840 Training accuracy = 0.95 Loss = 0.150491\n",
      "4841 Training accuracy = 0.94 Loss = 0.242128\n",
      "4842 Training accuracy = 0.92 Loss = 0.230975\n",
      "4843 Training accuracy = 0.96 Loss = 0.21236\n",
      "4844 Training accuracy = 0.89 Loss = 0.305583\n",
      "4845 Training accuracy = 0.91 Loss = 0.44066\n",
      "4846 Training accuracy = 0.9 Loss = 0.281357\n",
      "4847 Training accuracy = 0.91 Loss = 0.215877\n",
      "4848 Training accuracy = 0.92 Loss = 0.484129\n",
      "4849 Training accuracy = 0.92 Loss = 0.23665\n",
      "4850 Training accuracy = 0.96 Loss = 0.156687\n",
      "4851 Training accuracy = 0.94 Loss = 0.171121\n",
      "4852 Training accuracy = 0.92 Loss = 0.230167\n",
      "4853 Training accuracy = 0.95 Loss = 0.234452\n",
      "4854 Training accuracy = 0.94 Loss = 0.168284\n",
      "4855 Training accuracy = 0.89 Loss = 0.269744\n",
      "4856 Training accuracy = 0.94 Loss = 0.358665\n",
      "4857 Training accuracy = 0.94 Loss = 0.268001\n",
      "4858 Training accuracy = 0.98 Loss = 0.108727\n",
      "4859 Training accuracy = 0.98 Loss = 0.124898\n",
      "4860 Training accuracy = 0.92 Loss = 0.312473\n",
      "4861 Training accuracy = 0.91 Loss = 0.359858\n",
      "4862 Training accuracy = 0.89 Loss = 0.371035\n",
      "4863 Training accuracy = 0.89 Loss = 0.429591\n",
      "4864 Training accuracy = 0.94 Loss = 0.274599\n",
      "4865 Training accuracy = 0.89 Loss = 0.248513\n",
      "4866 Training accuracy = 0.89 Loss = 0.308507\n",
      "4867 Training accuracy = 0.93 Loss = 0.19962\n",
      "4868 Training accuracy = 0.86 Loss = 0.384873\n",
      "4869 Training accuracy = 0.91 Loss = 0.239583\n",
      "4870 Training accuracy = 0.87 Loss = 0.341191\n",
      "4871 Training accuracy = 0.89 Loss = 0.398708\n",
      "4872 Training accuracy = 0.93 Loss = 0.162543\n",
      "4873 Training accuracy = 0.9 Loss = 0.377468\n",
      "4874 Training accuracy = 0.92 Loss = 0.27696\n",
      "4875 Training accuracy = 0.84 Loss = 0.490893\n",
      "4876 Training accuracy = 0.91 Loss = 0.368583\n",
      "4877 Training accuracy = 0.9 Loss = 0.249696\n",
      "4878 Training accuracy = 0.95 Loss = 0.186568\n",
      "4879 Training accuracy = 0.91 Loss = 0.272373\n",
      "4880 Training accuracy = 0.95 Loss = 0.169883\n",
      "4881 Training accuracy = 0.93 Loss = 0.272839\n",
      "4882 Training accuracy = 0.97 Loss = 0.172738\n",
      "4883 Training accuracy = 0.94 Loss = 0.335605\n",
      "4884 Training accuracy = 0.92 Loss = 0.216446\n",
      "4885 Training accuracy = 0.89 Loss = 0.292936\n",
      "4886 Training accuracy = 0.9 Loss = 0.304423\n",
      "4887 Training accuracy = 0.93 Loss = 0.258262\n",
      "4888 Training accuracy = 0.92 Loss = 0.345874\n",
      "4889 Training accuracy = 0.95 Loss = 0.177317\n",
      "4890 Training accuracy = 0.92 Loss = 0.336917\n",
      "4891 Training accuracy = 0.95 Loss = 0.167459\n",
      "4892 Training accuracy = 0.94 Loss = 0.144844\n",
      "4893 Training accuracy = 0.95 Loss = 0.21283\n",
      "4894 Training accuracy = 0.96 Loss = 0.186025\n",
      "4895 Training accuracy = 0.95 Loss = 0.235553\n",
      "4896 Training accuracy = 0.94 Loss = 0.164387\n",
      "4897 Training accuracy = 0.89 Loss = 0.283992\n",
      "4898 Training accuracy = 0.96 Loss = 0.18318\n",
      "4899 Training accuracy = 0.97 Loss = 0.155305\n",
      "4900 Training accuracy = 0.93 Loss = 0.321215\n",
      "4901 Training accuracy = 0.96 Loss = 0.312512\n",
      "4902 Training accuracy = 0.94 Loss = 0.224713\n",
      "4903 Training accuracy = 0.94 Loss = 0.167374\n",
      "4904 Training accuracy = 0.92 Loss = 0.346344\n",
      "4905 Training accuracy = 0.93 Loss = 0.342376\n",
      "4906 Training accuracy = 0.96 Loss = 0.205978\n",
      "4907 Training accuracy = 0.96 Loss = 0.342057\n",
      "4908 Training accuracy = 0.91 Loss = 0.243997\n",
      "4909 Training accuracy = 0.94 Loss = 0.159228\n",
      "4910 Training accuracy = 0.92 Loss = 0.250147\n",
      "4911 Training accuracy = 0.91 Loss = 0.351718\n",
      "4912 Training accuracy = 0.92 Loss = 0.248506\n",
      "4913 Training accuracy = 0.93 Loss = 0.238893\n",
      "4914 Training accuracy = 0.94 Loss = 0.14362\n",
      "4915 Training accuracy = 0.95 Loss = 0.370433\n",
      "4916 Training accuracy = 0.97 Loss = 0.138179\n",
      "4917 Training accuracy = 0.92 Loss = 0.273015\n",
      "4918 Training accuracy = 0.97 Loss = 0.14785\n",
      "4919 Training accuracy = 0.9 Loss = 0.324757\n",
      "4920 Training accuracy = 0.92 Loss = 0.331055\n",
      "4921 Training accuracy = 0.96 Loss = 0.299067\n",
      "4922 Training accuracy = 0.93 Loss = 0.216722\n",
      "4923 Training accuracy = 0.87 Loss = 0.509599\n",
      "4924 Training accuracy = 0.93 Loss = 0.335386\n",
      "4925 Training accuracy = 0.89 Loss = 0.281511\n",
      "4926 Training accuracy = 0.93 Loss = 0.296803\n",
      "4927 Training accuracy = 0.87 Loss = 0.361505\n",
      "4928 Training accuracy = 0.92 Loss = 0.232345\n",
      "4929 Training accuracy = 0.93 Loss = 0.291269\n",
      "4930 Training accuracy = 0.94 Loss = 0.178574\n",
      "4931 Training accuracy = 0.98 Loss = 0.128679\n",
      "4932 Training accuracy = 0.96 Loss = 0.186522\n",
      "4933 Training accuracy = 0.92 Loss = 0.218767\n",
      "4934 Training accuracy = 0.93 Loss = 0.285024\n",
      "4935 Training accuracy = 0.96 Loss = 0.22341\n",
      "4936 Training accuracy = 0.96 Loss = 0.186606\n",
      "4937 Training accuracy = 0.96 Loss = 0.132229\n",
      "4938 Training accuracy = 0.97 Loss = 0.201435\n",
      "4939 Training accuracy = 0.9 Loss = 0.288506\n",
      "4940 Training accuracy = 0.89 Loss = 0.324658\n",
      "4941 Training accuracy = 0.9 Loss = 0.370617\n",
      "4942 Training accuracy = 0.96 Loss = 0.189381\n",
      "4943 Training accuracy = 0.88 Loss = 0.307744\n",
      "4944 Training accuracy = 0.89 Loss = 0.364196\n",
      "4945 Training accuracy = 0.97 Loss = 0.152999\n",
      "4946 Training accuracy = 0.93 Loss = 0.287223\n",
      "4947 Training accuracy = 0.92 Loss = 0.223768\n",
      "4948 Training accuracy = 0.91 Loss = 0.330204\n",
      "4949 Training accuracy = 0.9 Loss = 0.353721\n",
      "4950 Training accuracy = 0.92 Loss = 0.246973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4951 Training accuracy = 0.93 Loss = 0.2235\n",
      "4952 Training accuracy = 0.87 Loss = 0.341844\n",
      "4953 Training accuracy = 0.92 Loss = 0.303735\n",
      "4954 Training accuracy = 0.93 Loss = 0.288268\n",
      "4955 Training accuracy = 0.97 Loss = 0.154861\n",
      "4956 Training accuracy = 0.94 Loss = 0.276525\n",
      "4957 Training accuracy = 0.91 Loss = 0.350061\n",
      "4958 Training accuracy = 0.92 Loss = 0.235798\n",
      "4959 Training accuracy = 0.88 Loss = 0.523229\n",
      "4960 Training accuracy = 0.93 Loss = 0.250475\n",
      "4961 Training accuracy = 0.92 Loss = 0.340418\n",
      "4962 Training accuracy = 0.95 Loss = 0.203087\n",
      "4963 Training accuracy = 0.95 Loss = 0.271896\n",
      "4964 Training accuracy = 0.93 Loss = 0.356121\n",
      "4965 Training accuracy = 0.96 Loss = 0.173905\n",
      "4966 Training accuracy = 0.97 Loss = 0.153215\n",
      "4967 Training accuracy = 0.96 Loss = 0.147282\n",
      "4968 Training accuracy = 0.92 Loss = 0.240326\n",
      "4969 Training accuracy = 0.91 Loss = 0.326726\n",
      "4970 Training accuracy = 0.93 Loss = 0.262448\n",
      "4971 Training accuracy = 0.97 Loss = 0.158556\n",
      "4972 Training accuracy = 0.94 Loss = 0.188071\n",
      "4973 Training accuracy = 0.96 Loss = 0.181132\n",
      "4974 Training accuracy = 0.94 Loss = 0.218182\n",
      "4975 Training accuracy = 0.95 Loss = 0.196319\n",
      "4976 Training accuracy = 0.91 Loss = 0.320967\n",
      "4977 Training accuracy = 0.93 Loss = 0.242709\n",
      "4978 Training accuracy = 0.91 Loss = 0.272408\n",
      "4979 Training accuracy = 0.97 Loss = 0.108136\n",
      "4980 Training accuracy = 0.95 Loss = 0.192757\n",
      "4981 Training accuracy = 0.88 Loss = 0.294394\n",
      "4982 Training accuracy = 0.93 Loss = 0.244603\n",
      "4983 Training accuracy = 0.95 Loss = 0.164523\n",
      "4984 Training accuracy = 0.92 Loss = 0.168769\n",
      "4985 Training accuracy = 0.93 Loss = 0.28036\n",
      "4986 Training accuracy = 0.94 Loss = 0.328741\n",
      "4987 Training accuracy = 0.9 Loss = 0.28796\n",
      "4988 Training accuracy = 0.95 Loss = 0.156215\n",
      "4989 Training accuracy = 0.97 Loss = 0.183317\n",
      "4990 Training accuracy = 0.97 Loss = 0.150794\n",
      "4991 Training accuracy = 0.93 Loss = 0.212086\n",
      "4992 Training accuracy = 0.92 Loss = 0.320518\n",
      "4993 Training accuracy = 0.93 Loss = 0.20739\n",
      "4994 Training accuracy = 0.94 Loss = 0.29185\n",
      "4995 Training accuracy = 0.92 Loss = 0.282097\n",
      "4996 Training accuracy = 0.94 Loss = 0.443428\n",
      "4997 Training accuracy = 0.94 Loss = 0.164685\n",
      "4998 Training accuracy = 0.97 Loss = 0.272849\n",
      "4999 Training accuracy = 0.86 Loss = 0.485377\n",
      "5000 Training accuracy = 0.89 Loss = 0.285033\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Training Loop\n",
    "for i in range(5000):\n",
    "    batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "    train_data = {X: batch_X, y: batch_y}\n",
    "    sess.run(train, feed_dict=train_data)\n",
    "\n",
    "    print(i+1, \"Training accuracy =\",sess.run(accuracy, feed_dict=train_data),\n",
    "\"Loss =\",sess.run(loss, feed_dict=train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy =  0.9253\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Evaluation\n",
    "test_data = {X:mnist.test.images,y:mnist.test.labels}\n",
    "print(\"Testing accuracy = \",sess.run(accuracy, feed_dict=test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
